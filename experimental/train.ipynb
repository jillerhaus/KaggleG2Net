{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:41.713595Z",
     "iopub.status.busy": "2021-09-17T17:22:41.713335Z",
     "iopub.status.idle": "2021-09-17T17:22:41.725791Z",
     "shell.execute_reply": "2021-09-17T17:22:41.725140Z",
     "shell.execute_reply.started": "2021-09-17T17:22:41.713521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KAGGLE: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "KAGGLE = True\n",
    "if os.name == \"nt\":\n",
    "    KAGGLE = False\n",
    "print(f\"KAGGLE: {KAGGLE}\")\n",
    "if not KAGGLE:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:41.727703Z",
     "iopub.status.busy": "2021-09-17T17:22:41.726861Z",
     "iopub.status.idle": "2021-09-17T17:22:43.782183Z",
     "shell.execute_reply": "2021-09-17T17:22:43.781042Z",
     "shell.execute_reply.started": "2021-09-17T17:22:41.727659Z"
    }
   },
   "outputs": [],
   "source": [
    "# general\n",
    "import gc\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "from glob import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from IPython.core.display import display, HTML\n",
    "if KAGGLE:\n",
    "    from kaggle_datasets import KaggleDatasets\n",
    "\n",
    "# ML\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# DL\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:43.784172Z",
     "iopub.status.busy": "2021-09-17T17:22:43.783660Z",
     "iopub.status.idle": "2021-09-17T17:22:43.799591Z",
     "shell.execute_reply": "2021-09-17T17:22:43.798788Z",
     "shell.execute_reply.started": "2021-09-17T17:22:43.784124Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tf.compat.v1.disable_eager_execution()\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:43.803149Z",
     "iopub.status.busy": "2021-09-17T17:22:43.802737Z",
     "iopub.status.idle": "2021-09-17T17:22:43.811423Z",
     "shell.execute_reply": "2021-09-17T17:22:43.810381Z",
     "shell.execute_reply.started": "2021-09-17T17:22:43.803115Z"
    }
   },
   "outputs": [],
   "source": [
    "def auto_select_accelerator():\n",
    "    TPU_DETECTED = False\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "        tf.keras.mixed_precision.set_global_policy(\"mixed_bfloat16\")\n",
    "        print(f\"Running on TPU: {tpu.master()}\")\n",
    "        TPU_DETECTED = True\n",
    "    except ValueError:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "        tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    num_replicas = strategy.num_replicas_in_sync\n",
    "    print(f\"Running on {num_replicas} replica{'s' if num_replicas > 1 else ''}\")\n",
    "    return strategy, TPU_DETECTED, num_replicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:43.813082Z",
     "iopub.status.busy": "2021-09-17T17:22:43.812779Z",
     "iopub.status.idle": "2021-09-17T17:22:49.385095Z",
     "shell.execute_reply": "2021-09-17T17:22:49.384154Z",
     "shell.execute_reply.started": "2021-09-17T17:22:43.813043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "Running on 1 replica\n"
     ]
    }
   ],
   "source": [
    "strategy, TPU_Detected, REPLICAS = auto_select_accelerator()\n",
    "INPUT_DIR = \"../input/g2net-gravitational-wave-detection\"\n",
    "MDLS_PATH = \".\" if KAGGLE else \"../models\"\n",
    "# TRAIN_FILES_PATH = \"../input/filtered*_tfrec\"\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "tfrec_folders = [\"whitened-tfrec\"]#[\"whitened-longer-tfrec\", \"whitened-tfrec\", \"filtered-whitened-tfrec\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:10:49.388016Z",
     "iopub.status.busy": "2021-09-17T17:10:49.387794Z",
     "iopub.status.idle": "2021-09-17T17:10:49.406903Z",
     "shell.execute_reply": "2021-09-17T17:10:49.405619Z",
     "shell.execute_reply.started": "2021-09-17T17:10:49.387988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>version</th>\n",
       "      <th>train_mode</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>bavg_epoch</th>\n",
       "      <th>bavg_loss</th>\n",
       "      <th>bavg_auc</th>\n",
       "      <th>changelog</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.00020</td>\n",
       "      <td>124</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>23</td>\n",
       "      <td>0.448196</td>\n",
       "      <td>0.827187</td>\n",
       "      <td>10x lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>134</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>12</td>\n",
       "      <td>0.467185</td>\n",
       "      <td>0.826250</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>135</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>9</td>\n",
       "      <td>0.444164</td>\n",
       "      <td>0.848162</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>136</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>21</td>\n",
       "      <td>0.420500</td>\n",
       "      <td>0.854332</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>137</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>31</td>\n",
       "      <td>0.364450</td>\n",
       "      <td>0.876402</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>139</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.436394</td>\n",
       "      <td>0.845982</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>139</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.436394</td>\n",
       "      <td>0.845982</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>140</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>0.423539</td>\n",
       "      <td>0.844709</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>142</td>\n",
       "      <td>full</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.447673</td>\n",
       "      <td>0.836974</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>164</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>9</td>\n",
       "      <td>0.438891</td>\n",
       "      <td>0.844589</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>200</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>36</td>\n",
       "      <td>0.464103</td>\n",
       "      <td>0.839057</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>210</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>24</td>\n",
       "      <td>0.444001</td>\n",
       "      <td>0.847384</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>221</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "      <td>0.435199</td>\n",
       "      <td>0.850387</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>222</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>26</td>\n",
       "      <td>0.437144</td>\n",
       "      <td>0.849532</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>224</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>30</td>\n",
       "      <td>0.434321</td>\n",
       "      <td>0.851999</td>\n",
       "      <td>Input layers: 1/2 kernel size. more kernels</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>231</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>29</td>\n",
       "      <td>0.439757</td>\n",
       "      <td>0.848020</td>\n",
       "      <td>only whitened ds</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>242</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>0.447073</td>\n",
       "      <td>0.848576</td>\n",
       "      <td>only whitened ds with sgd</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>244</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>0.465401</td>\n",
       "      <td>0.834070</td>\n",
       "      <td>only whitened ds with sgd</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>245</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.842836</td>\n",
       "      <td>all ds</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>247</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>0.444699</td>\n",
       "      <td>0.842759</td>\n",
       "      <td>second run with adam</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lr  version train_mode  batch_size  epochs  bavg_epoch  bavg_loss  \\\n",
       "33  0.00020      124       full         256     100          23   0.448196   \n",
       "34  0.00010      134       full         256     100          12   0.467185   \n",
       "35  0.00010      135       full         256     100           9   0.444164   \n",
       "36  0.00010      136       full         256     100          21   0.420500   \n",
       "37  0.00010      137       full         256     100          31   0.364450   \n",
       "38  0.00010      139       full         256     100          10   0.436394   \n",
       "39  0.00010      139       full         256     100          10   0.436394   \n",
       "40  0.00010      140       full         256      15          12   0.423539   \n",
       "41  0.00010      142       full         128       5           4   0.447673   \n",
       "42  0.00010      164       full         256     100           9   0.438891   \n",
       "43  0.00001      200       full         256     100          36   0.464103   \n",
       "44  0.00100      210       full         256     100          24   0.444001   \n",
       "45  0.00010      221       full         256     100          40   0.435199   \n",
       "46  0.00010      222       full         256     100          26   0.437144   \n",
       "47  0.00010      224       full         256     100          30   0.434321   \n",
       "48  0.00010      231       full         256     100          29   0.439757   \n",
       "49  0.00010      242       full         256      25          23   0.447073   \n",
       "50  0.00010      244       full         256      25          21   0.465401   \n",
       "51  0.00010      245       full         256      25          23   0.446809   \n",
       "52  0.00010      247       full         256      35           9   0.444699   \n",
       "\n",
       "    bavg_auc                                    changelog  seed  \n",
       "33  0.827187                                       10x lr  42.0  \n",
       "34  0.826250                                       1/4 lr  42.0  \n",
       "35  0.848162                                       1/4 lr  42.0  \n",
       "36  0.854332                                       1/4 lr  42.0  \n",
       "37  0.876402                                       1/4 lr  42.0  \n",
       "38  0.845982                                       1/4 lr  42.0  \n",
       "39  0.845982                                       1/4 lr  42.0  \n",
       "40  0.844709                                       1/4 lr  42.0  \n",
       "41  0.836974                                       1/4 lr  42.0  \n",
       "42  0.844589                                       1/4 lr  42.0  \n",
       "43  0.839057                                       1/4 lr  42.0  \n",
       "44  0.847384                                       1/4 lr  42.0  \n",
       "45  0.850387                                       1/4 lr  69.0  \n",
       "46  0.849532                                       1/4 lr  69.0  \n",
       "47  0.851999  Input layers: 1/2 kernel size. more kernels  69.0  \n",
       "48  0.848020                             only whitened ds  69.0  \n",
       "49  0.848576                    only whitened ds with sgd  42.0  \n",
       "50  0.834070                    only whitened ds with sgd  42.0  \n",
       "51  0.842836                                       all ds  42.0  \n",
       "52  0.842759                         second run with adam  42.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = \"no file\"\n",
    "if not KAGGLE:\n",
    "    results_df = pd.read_csv(\"../models/results.csv\", index_col=[0]).tail(20)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:49.386775Z",
     "iopub.status.busy": "2021-09-17T17:22:49.386544Z",
     "iopub.status.idle": "2021-09-17T17:22:49.599221Z",
     "shell.execute_reply": "2021-09-17T17:22:49.598282Z",
     "shell.execute_reply.started": "2021-09-17T17:22:49.386749Z"
    }
   },
   "outputs": [],
   "source": [
    "if KAGGLE:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    user_credential = user_secrets.get_gcloud_credential()\n",
    "    user_secrets.set_tensorflow_credential(user_credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:49.601675Z",
     "iopub.status.busy": "2021-09-17T17:22:49.601417Z",
     "iopub.status.idle": "2021-09-17T17:22:49.608611Z",
     "shell.execute_reply": "2021-09-17T17:22:49.607652Z",
     "shell.execute_reply.started": "2021-09-17T17:22:49.601646Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{MDLS_PATH}/results.csv\"):\n",
    "    VER = 1\n",
    "else:\n",
    "    results = pd.read_csv(f\"{MDLS_PATH}/results.csv\", index_col=[0])\n",
    "    VER = int(results.version.max())\n",
    "Params ={\n",
    "    \"lr\": 1e-4 * REPLICAS,\n",
    "    \"version\": VER,\n",
    "    \"train_mode\": \"full\", #test, full\n",
    "    \"batch_size\": 256 * REPLICAS,\n",
    "    \"epochs\":35,\n",
    "    \"seed\": 42,\n",
    "    \"changelog\": \"second run with adam\",\n",
    "}\n",
    "seed_everything(Params[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:49.609927Z",
     "iopub.status.busy": "2021-09-17T17:22:49.609716Z",
     "iopub.status.idle": "2021-09-17T17:22:49.626003Z",
     "shell.execute_reply": "2021-09-17T17:22:49.624837Z",
     "shell.execute_reply.started": "2021-09-17T17:22:49.609902Z"
    }
   },
   "outputs": [],
   "source": [
    "VER = Params[\"version\"]\n",
    "MDL_PATH = f\"{MDLS_PATH}/models_v{VER:03}\"\n",
    "while os.path.exists(MDL_PATH):\n",
    "    VER += 1\n",
    "    MDL_PATH = f\"{MDLS_PATH}/models_v{VER:03}\"\n",
    "Params[\"version\"]=VER\n",
    "os.mkdir(MDL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_cut(x, y):\n",
    "    a = np.zeros(x.shape, dtype=np.float32)\n",
    "    dt = np.random.randint(2,512)\n",
    "    t0 = np.random.randint(1,dt)\n",
    "    t1 = np.random.randint(0,t0)\n",
    "    a[:,t1:t1+(4096-dt)] = x[:,t0:t0+(4096-dt)]\n",
    "    a[:] *= np.array([-1 if random.random() > 0.5 else 1,-1 if random.random() > 0.5 else 1, -1 if random.random() > 0.5 else 1])\n",
    "    return a,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:49.628201Z",
     "iopub.status.busy": "2021-09-17T17:22:49.627767Z",
     "iopub.status.idle": "2021-09-17T17:22:49.642197Z",
     "shell.execute_reply": "2021-09-17T17:22:49.641358Z",
     "shell.execute_reply.started": "2021-09-17T17:22:49.628155Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "input_context = tf.distribute.InputContext(\n",
    "    input_pipeline_id=1,\n",
    "    num_input_pipelines = 8\n",
    ")\n",
    "read_config= tfds.ReadConfig(\n",
    "    input_context=input_context\n",
    ")\n",
    "\n",
    "def load_dataset(files, shuffle=True, ordered=False, labeled = True, repeat=True, return_labels = False, cut = False):\n",
    "    if ordered:\n",
    "        dataset = tf.data.TFRecordDataset(files, num_parallel_reads=None)\n",
    "    else:\n",
    "        dataset = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n",
    "\n",
    "\n",
    "    def _parse_function(example_proto):\n",
    "        if labeled:\n",
    "            keys_to_feature = {\n",
    "                \"TimeSeries\":tf.io.FixedLenFeature([4096,3],tf.float32),\n",
    "                \"Target\":tf.io.FixedLenFeature([], tf.int64, default_value=0)}\n",
    "            if return_labels:\n",
    "                keys_to_feature[\"id\"]=tf.io.FixedLenFeature([],tf.string, default_value=\"\")\n",
    "        else:\n",
    "            keys_to_feature = {\n",
    "                \"TimeSeries\": tf.io.FixedLenFeature([4096,3],tf.float32)\n",
    "            }\n",
    "        parsed_features = tf.io.parse_single_example(example_proto, keys_to_feature)\n",
    "        if labeled:\n",
    "            if return_labels:\n",
    "                return parsed_features[\"TimeSeries\"], parsed_features[\"Target\"], parsed_features[\"id\"]\n",
    "            else:\n",
    "                return parsed_features[\"TimeSeries\"], parsed_features[\"Target\"]\n",
    "        else:\n",
    "            return parsed_features[\"TimeSeries\"]\n",
    "    \n",
    "    if not ordered:\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic=False\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "    # parse the record into tensors.\n",
    "    dataset = dataset.map(_parse_function, num_parallel_calls=AUTO)\n",
    "#     dataset = dataset.cache()\n",
    "\n",
    "    # Repeat the input infinitely\n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "\n",
    "    # shuffle the dataset\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=100000, reshuffle_each_iteration=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # Generate batches\n",
    "    dataset = dataset.batch(Params[\"batch_size\"])\n",
    "    if cut:\n",
    "        dataset = dataset.map(lambda x,y:tf.numpy_function(random_cut, inp=[x,y], Tout=[tf.float32, tf.int64]), num_parallel_calls=AUTO)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:49.644543Z",
     "iopub.status.busy": "2021-09-17T17:22:49.644208Z",
     "iopub.status.idle": "2021-09-17T17:22:53.400603Z",
     "shell.execute_reply": "2021-09-17T17:22:53.399615Z",
     "shell.execute_reply.started": "2021-09-17T17:22:49.644502Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_val_files(folders):\n",
    "    train_files = []\n",
    "    val_files = []\n",
    "    test_files = []\n",
    "    all_train_files = []\n",
    "    for folder in folders:\n",
    "        if KAGGLE:\n",
    "            TRAIN_FILES_PATH = KaggleDatasets().get_gcs_path(folder)\n",
    "            TEST_FILES_PATH = KaggleDatasets().get_gcs_path(f\"{folder}test\")\n",
    "            all_files_train = np.sort(tf.io.gfile.glob(f\"{TRAIN_FILES_PATH}/train_*.tfrec\"))\n",
    "            all_files_test = np.sort(tf.io.gfile.glob(f\"{TEST_FILES_PATH}/test_*.tfrec\"))\n",
    "        else:\n",
    "            all_files_train = np.sort(glob(f\"../input/{folder}/train*.tfrec\"))\n",
    "            all_files_test = np.sort(glob(f\"../input/{folder}/test*.tfrec\"))\n",
    "        train_files.extend(all_files_train[:-2])\n",
    "        val_files.extend(all_files_train[-2:])\n",
    "        test_files.append(all_files_test)\n",
    "        all_train_files.append(all_files_train)\n",
    "    return train_files, val_files, test_files, all_train_files\n",
    "\n",
    "train_files, val_files, test_files, all_train_files = get_train_val_files(tfrec_folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.43205000000000027\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def benchmark(dataset, num_epochs=2):\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        for sample in dataset:\n",
    "            # Performing a training step\n",
    "            time.sleep(0.01)\n",
    "    print(\"Execution time:\", time.perf_counter() - start_time)\n",
    "benchmark(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = [\"../input/synthetic-tfrec/train_250_Mpc.tfrec\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:53.402399Z",
     "iopub.status.busy": "2021-09-17T17:22:53.402073Z",
     "iopub.status.idle": "2021-09-17T17:22:53.738249Z",
     "shell.execute_reply": "2021-09-17T17:22:53.737171Z",
     "shell.execute_reply.started": "2021-09-17T17:22:53.402358Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ds = load_dataset(train_files, cut = True)\n",
    "# train_ds = load_dataset(val_files)\n",
    "val_ds = load_dataset(val_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 4096, 3)\n",
      "(256, 4096, 3)\n"
     ]
    }
   ],
   "source": [
    "for x in train_ds:\n",
    "    tens = x\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.4631109 , -0.20922244, -0.3535372 ], dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_ds:\n",
    "    tensor = x\n",
    "    input_tensor_shape = x[0].shape[1]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(256, 4096, 3), dtype=float32, numpy=\n",
       " array([[[ 0., -0., -0.],\n",
       "         [ 0., -0., -0.],\n",
       "         [ 0., -0., -0.],\n",
       "         ...,\n",
       "         [ 0., -0., -0.],\n",
       "         [ 0., -0., -0.],\n",
       "         [ 0., -0., -0.]],\n",
       " \n",
       "        [[ 0., -0., -0.],\n",
       "         [ 0., -0., -0.],\n",
       "         [ 0., -0., -0.],\n",
       "         ...,\n",
       "         [ 0., -0., -0.],\n",
       "         [ 0., -0., -0.],\n",
       "         [ 0., -0., -0.]],\n",
       " \n",
       "        [[ 0., -0., -0.],\n",
       "         [ 0., -0., -0.],\n",
       "         [ 0., -0., -0.],\n",
       "         ...,\n",
       "         [ 0., -0., -0.],\n",
       "         [ 0., -0., -0.],\n",
       "         [ 0., -0., -0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0., -0., -0.],\n",
       "         [ 0., -0., -0.],\n",
       "         [ 0., -0., -0.],\n",
       "         ...,\n",
       "         [ 0., -0., -0.],\n",
       "         [ 0., -0., -0.],\n",
       "         [ 0., -0., -0.]],\n",
       " \n",
       "        [[ 0., -0., -0.],\n",
       "         [ 0., -0., -0.],\n",
       "         [ 0., -0., -0.],\n",
       "         ...,\n",
       "         [ 0., -0., -0.],\n",
       "         [ 0., -0., -0.],\n",
       "         [ 0., -0., -0.]],\n",
       " \n",
       "        [[ 0., -0., -0.],\n",
       "         [ 0., -0., -0.],\n",
       "         [ 0., -0., -0.],\n",
       "         ...,\n",
       "         [ 0., -0., -0.],\n",
       "         [ 0., -0., -0.],\n",
       "         [ 0., -0., -0.]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(256,), dtype=int64, numpy=\n",
       " array([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "        0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "        0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
       "        1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "        1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1], dtype=int64)>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-15T15:02:39.670012Z",
     "iopub.status.busy": "2021-09-15T15:02:39.669804Z",
     "iopub.status.idle": "2021-09-15T15:02:40.728147Z",
     "shell.execute_reply": "2021-09-15T15:02:40.727152Z",
     "shell.execute_reply.started": "2021-09-15T15:02:39.669988Z"
    }
   },
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "with strategy.scope():\n",
    "    model = Sequential([\n",
    "        layers.Conv1D(filters=32, kernel_size=16, padding=\"causal\",input_shape=[4096,3]),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=64, kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=128, kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=256, kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1),\n",
    "        layers.Activation(\"sigmoid\", dtype=\"float32\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        tf.keras.optimizers.Adam(learning_rate = Params[\"lr\"]),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"AUC\"]\n",
    "    )\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 560000 // 16 * len(train_files) // Params[\"batch_size\"]\n",
    "validation_steps = 560000 // 16 * len(val_files) // Params[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:54.953060Z",
     "iopub.status.busy": "2021-09-17T17:22:54.952812Z",
     "iopub.status.idle": "2021-09-17T17:22:54.957958Z",
     "shell.execute_reply": "2021-09-17T17:22:54.957019Z",
     "shell.execute_reply.started": "2021-09-17T17:22:54.953033Z"
    }
   },
   "source": [
    "#best\n",
    "from tensorflow.keras import Sequential, layers\n",
    "with strategy.scope():\n",
    "    model = Sequential([\n",
    "        layers.Conv1D(filters=256, activation=\"relu\", kernel_size=3, padding=\"causal\",input_shape=[4096,3]),\n",
    "        layers.Conv1D(filters=256, activation=\"relu\", kernel_size=6, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=256, activation=\"relu\", kernel_size=6, padding=\"causal\"),\n",
    "        \n",
    "        layers.MaxPool1D(pool_size=2),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=64, activation=\"relu\", kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, activation=\"relu\", kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, activation=\"relu\", kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=128, activation=\"relu\", kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, activation=\"relu\",kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, activation=\"relu\",kernel_size=8, padding=\"causal\"),\n",
    "        \n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=256, activation=\"relu\", kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=256, activation=\"relu\",kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=256, activation=\"relu\",kernel_size=8, padding=\"causal\"),\n",
    "        \n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        \n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1),\n",
    "        layers.Activation(\"sigmoid\", dtype=\"float32\")\n",
    "    ])\n",
    "    \n",
    "    lr_decayed_fn = tf.keras.experimental.CosineDecay(\n",
    "        1e-3,\n",
    "        steps_per_epoch\n",
    "    )\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = Params[\"lr\"])\n",
    "\n",
    "    #opt = tfa.optimizers.AdamW(lr_decayed_fn, learning_rate = Params[\"lr\"])\n",
    "    model.compile(\n",
    "        opt,\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"AUC\"]\n",
    "    )\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 4094, 256)         2560      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 4089, 256)         393472    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 4084, 256)         393472    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 2042, 256)         0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2042, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 2035, 64)          131136    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 2028, 128)         65664     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 2021, 128)         131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 505, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 505, 128)          0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 505, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 498, 128)          131200    \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 491, 128)          131200    \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 484, 128)          131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 121, 128)          0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 121, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 114, 256)          262400    \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 107, 256)          524544    \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 100, 256)          524544    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 25, 256)           0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 25, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               819328    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,652,289\n",
      "Trainable params: 3,652,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#best\n",
    "from tensorflow.keras import Sequential, layers\n",
    "with strategy.scope():\n",
    "    model = Sequential([\n",
    "        layers.Conv1D(filters=256, activation=\"relu\", kernel_size=3, padding=\"valid\",input_shape=[4096,3]),\n",
    "        layers.Conv1D(filters=256, activation=\"relu\", kernel_size=6, padding=\"valid\"),\n",
    "        layers.Conv1D(filters=256, activation=\"relu\", kernel_size=6, padding=\"valid\"),\n",
    "        \n",
    "        layers.MaxPool1D(pool_size=2),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=64, activation=\"relu\", kernel_size=8, padding=\"valid\"),\n",
    "        layers.Conv1D(filters=128, activation=\"relu\", kernel_size=8, padding=\"valid\"),\n",
    "        layers.Conv1D(filters=128, activation=\"relu\", kernel_size=8, padding=\"valid\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=128, activation=\"relu\", kernel_size=8, padding=\"valid\"),\n",
    "        layers.Conv1D(filters=128, activation=\"relu\",kernel_size=8, padding=\"valid\"),\n",
    "        layers.Conv1D(filters=128, activation=\"relu\",kernel_size=8, padding=\"valid\"),\n",
    "        \n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=256, activation=\"relu\", kernel_size=8, padding=\"valid\"),\n",
    "        layers.Conv1D(filters=256, activation=\"relu\",kernel_size=8, padding=\"valid\"),\n",
    "        layers.Conv1D(filters=256, activation=\"relu\",kernel_size=8, padding=\"valid\"),\n",
    "        \n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        \n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1),\n",
    "        layers.Activation(\"sigmoid\", dtype=\"float32\")\n",
    "    ])\n",
    "    \n",
    "    lr_decayed_fn = tf.keras.experimental.CosineDecay(\n",
    "        1e-3,\n",
    "        steps_per_epoch\n",
    "    )\n",
    "    \n",
    "#     opt = tf.keras.optimizers.Adam(learning_rate = Params[\"lr\"])\n",
    "    #opt = tf.keras.optimizers.SGD(learning_rate=Params[\"lr\"])\n",
    "    opt = tfa.optimizers.Lookahead(\n",
    "        tf.keras.optimizers.Adam(learning_rate = Params[\"lr\"]),\n",
    "        sync_period = 6\n",
    "    )\n",
    "#     opt = tfa.optimizers.AdamW(lr_decayed_fn, learning_rate = Params[\"lr\"])\n",
    "    model.compile(\n",
    "        opt,\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"AUC\"]\n",
    "    )\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_callback(batch_size=8):\n",
    "    lr_start = Params[\"lr\"]\n",
    "    lr_max = 0.0000015 * batch_size\n",
    "    lr_min = 1e-6\n",
    "    lr_ramp_ep = 3\n",
    "    lr_sus_ep = 0\n",
    "    lr_decay = 0.7\n",
    "    \n",
    "    def lrfn(epoch):\n",
    "        initial_epochs = 4\n",
    "        \n",
    "        epoch = epoch - initial_epochs\n",
    "        if epoch < -1 * initial_epochs + 2:\n",
    "            return lr_start/10\n",
    "        elif epoch < 0:\n",
    "            lr = lr_start\n",
    "        elif epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "        return lr\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
    "    return lr_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:54.960073Z",
     "iopub.status.busy": "2021-09-17T17:22:54.959840Z",
     "iopub.status.idle": "2021-09-17T17:22:54.971237Z",
     "shell.execute_reply": "2021-09-17T17:22:54.970202Z",
     "shell.execute_reply.started": "2021-09-17T17:22:54.960046Z"
    }
   },
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.2,\n",
    "    patience=5, min_lr = 0.000001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta = 0.0003,\n",
    "    mode=\"min\",\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    f\"{MDL_PATH}/model_{Params['version']:03}.h5\",\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weight_only=False,\n",
    "    mode=\"auto\",\n",
    "    save_freq=\"epoch\"\n",
    ")\n",
    "\n",
    "callbacks=[get_lr_callback(Params[\"batch_size\"]) ,reduce_lr, early_stop, model_checkpoint]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with strategy.scope():\n",
    "    model = tf.keras.models.load_model(\"../models/models_v244/model_244.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (<unknown>, <unknown>), types: (tf.float32, tf.int64)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch=100000//256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:54.972832Z",
     "iopub.status.busy": "2021-09-17T17:22:54.972552Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 1e-05.\n",
      "390/390 [==============================] - 175s 412ms/step - loss: 0.6904 - auc: 0.4964 - val_loss: 0.6932 - val_auc: 0.5000901 - auc:  - ETA: 1s - loss: 0.6902 - auc:\n",
      "Epoch 2/35\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 1e-05.\n",
      "390/390 [==============================] - 198s 507ms/step - loss: 0.6911 - auc: 0.5236 - val_loss: 0.6931 - val_auc: 0.5000: 25s - loss: 0.68 - ETA: 24s - loss: 0.6885 - auc: 0. - ETA: 19s - loss: 0.6891 - a - ETA: 18s - loss: 0.6892 - a - ETA: 17s - loss: 0.6893 - auc - ETA:  - ETA: 1s - loss: 0.6911 - au\n",
      "Epoch 3/35\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.0001.\n",
      "390/390 [==============================] - 195s 499ms/step - loss: 0.6860 - auc: 0.5708 - val_loss: 0.6979 - val_auc: 0.5087849 - auc: 0 - ETA: 1:14 - loss: 0.6858 - auc: 0 - ETA: 1:12 - ETA: 57s -  - ETA: 55s - lo - ETA: 52s  - ETA: 49s - loss: 0. - - ETA: 15s - lo - ETA: 13s - loss: 0.6867 - auc: 0.55 - ETA: 13s - loss: 0.6867 - ETA: 12s  - ETA: 0s - loss: 0.6862 - auc: 0\n",
      "Epoch 4/35\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.0001.\n",
      "390/390 [==============================] - 199s 511ms/step - loss: 0.3810 - auc: 0.8982 - val_loss: 0.9078 - val_auc: 0.5919- loss: 0.597 - ETA: 1:09 - loss: 0.5514 - - ETA: 49s - loss: 0.4630 - - ETA: 48s - loss: 0.4584 - auc: 0.85 - ETA: 48s -  - ETA: 40s - loss: 0.4367 - auc - ETA: 40s - loss: 0.4345 - - ETA: 38s - loss: 0.4312 - auc:  - - ETA: 30s - loss: 0.4129 - auc: 0. - ETA: 29s - loss: 0.4125 - - ETA: 28s - loss: 0.4101 - auc: 0. - ETA:  - ETA: 24s - loss: 0.4055 - - ETA: 23s - lo - ETA: 12s - lo - ETA: 10s - lo - ETA: 6s - loss: 0.3865 - auc: - ETA: 4s - loss: 0.3853 - auc: 0.895 - ETA: 4s - loss: 0 - ETA: 0s - loss: 0.3821 - auc: \n",
      "Epoch 5/35\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.0001.\n",
      "390/390 [==============================] - 200s 514ms/step - loss: 0.3043 - auc: 0.9308 - val_loss: 0.8559 - val_auc: 0.5629- ETA: 40s -  - ETA: 37s - loss: 0.3126 - - ETA: 36s - loss - ETA: 34s - loss:  - - ETA - ETA: 13s - loss: 0.3092 - auc - ETA: 13s - loss: 0.3086 - auc: 0.92 - ETA: 12s - loss: 0.3085 - a - ETA: 11s - loss: 0.3080 - - ETA - ETA: 5s \n",
      "Epoch 6/35\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.00019466666666666666.\n",
      "390/390 [==============================] - 205s 526ms/step - loss: 0.2916 - auc: 0.9341 - val_loss: 0.9634 - val_auc: 0.5677TA: 1:15 - loss: 0.2778 - auc: 0. - ETA: 1:14 - loss: 0.2790 - auc: 0.926 - ETA: 1:14 - loss: 0.2782 - auc: 0 - ETA: 1:13 - loss: 0.269 - ETA: 1: - ETA: 1:03 - loss: 0.2927 - auc: 0.927 - ETA: 1:02 - loss: 0.2925 - - ETA: 1:00  - ETA: 57s - loss: 0.2918 - auc: 0.92 - ETA:  - ETA: 27s - loss: 0.2961 - auc: 0. - ETA: 27s - loss: 0.2960 - ETA: 25s - loss: 0.2963 - auc: 0. - ETA: 25s - loss: 0.2964 - auc: 0.93 - ETA: 25s  - ETA: 2 - ETA: 5s - loss: 0 - ETA: 2s - loss: 0.2922 \n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 3.8933331961743536e-05.\n",
      "Epoch 7/35\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.00028933333333333334.\n",
      "390/390 [==============================] - 208s 534ms/step - loss: 0.2814 - auc: 0.9382 - val_loss: 1.0820 - val_auc: 0.5780 loss: 0.2807 - a - ETA: 1:09 - loss: 0.2710  - ETA: 1:07 - loss:  - ETA: 34s - loss: 0.2873 - auc: 0. - ETA: 34s - loss: 0.2871 - ETA: 21s -  - ETA: 11s - loss: 0.2845 - - ETA: 9s - loss: - ETA: 5s - loss: 0.2826 - auc: 0 - ETA: 4s - loss: 0.2823 - auc: 0.938 - ETA: 4s - loss: 0.2822 - auc: 0. - ETA: 3s - loss: 0.2818 - auc: 0 - ETA: 2s - loss: 0.2817  - ETA: 0s - loss: 0.2815 - auc: 0.9\n",
      "Epoch 8/35\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.000384.\n",
      "390/390 [==============================] - 208s 533ms/step - loss: 0.2752 - auc: 0.9401 - val_loss: 0.9524 - val_auc: 0.5766 - auc: 0.934 - ETA: 1:08 - loss: 0.2727 - auc:  - ETA: 1:06 - lo - ETA: 53 - ETA: 50s - loss: 0. - ETA: 48s - loss: 0.2778 - - ETA: 47s - loss: 0.2783 - auc: 0. - ETA: 46s - lo - ETA: 29s - loss: 0.2795 - a - ETA: 28s - loss: 0.2794 - auc: 0.93 - ETA:  - ETA: 25s - loss: 0.27 - ETA: 23s - loss: 0.2786 - auc: 0. - ETA: 23s - loss: 0. - ETA: 21s - loss: 0.27 - ETA: 19s - loss: 0.2775 - auc: 0.94 - ETA: 19s - loss:  - ETA: 17s - loss:  - ETA: 12s - loss:  - ETA: 10s - loss: - ETA: 6s - loss: 0.2759 - au - ETA: 4s - loss: 0.2765 - auc:  - ETA: 3s - loss: 0.2764 - auc: 0.939 - ETA: 3s - loss: 0.2763 - auc: 0.939 - ETA: 3s - loss: 0.2\n",
      "Epoch 9/35\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.0002691.\n",
      "390/390 [==============================] - 212s 543ms/step - loss: 0.2661 - auc: 0.9431 - val_loss: 0.9852 - val_auc: 0.5880 - auc - ETA: 12s - loss: 0.2697 - ETA: 11s - loss: 0.2691 - ETA: 9s - loss: 0.26 - ETA: 6s - loss - ETA: 2s - loss: 0.2671 - auc: 0.9 - ETA: 1s - loss: 0.2665 - \n",
      "Epoch 10/35\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.00018866999999999997.\n",
      "390/390 [==============================] - 216s 553ms/step - loss: 0.2630 - auc: 0.9442 - val_loss: 1.0179 - val_auc: 0.57352635 - ETA: 2s - loss: 0.2632 -\n",
      "Epoch 11/35\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.00013236899999999997.\n",
      "390/390 [==============================] - 222s 570ms/step - loss: 0.2574 - auc: 0.9459 - val_loss: 1.0922 - val_auc: 0.5771 - loss: 0.2489 - auc: 0.937 - ETA: 1:10 - loss: 0.2 - ETA: 1:08 - loss: 0.2565 - auc:  - ETA: 1:07 - loss: 0.2585 - auc: 0.936 - ETA: 1:06 - loss: 0.2563 - ETA: 1:04 - loss: 0.2542 - auc:  - ETA: 1:03 - loss: 0.2548 - auc: 0.93 - ETA: - ETA: 57s - loss: 0.2548 - a - ETA: 57s - loss: 0.25 - ETA: 55s  - ETA: 52s - loss: 0.2527 - a - ETA: 48s - loss:  - ETA: 38s -  - ETA: 36s - loss: 0.2586 - auc: 0. - ETA: 35s - loss: 0.2584 - - ETA: 34s - loss:  - ETA: 32 - ETA: 29s -  - ETA: 27s - loss: 0. - ETA: 25s - loss - ETA: 23s - loss: 0.2583 - auc: 0. - ETA: 23s - loss: 0.2592 - ETA: 21s - loss: 0.2591 - auc: 0. - ETA: 21s -  - ETA: 18s - loss: 0.2591 - auc - ETA: 18s - loss: 0.25 - ETA: 16s -  - ETA: 13s - loss:  - ET - ETA: - ETA: 1s - loss: 0.2580 - auc:\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 2.647379878908396e-05.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, validation_data = val_ds, epochs = Params[\"epochs\"], shuffle=True,\n",
    "                    steps_per_epoch = steps_per_epoch, validation_steps=validation_steps,\n",
    "                    verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "historyFrame = pd.DataFrame(history.history)\n",
    "historyFrame[[\"auc\", \"val_auc\"]].plot()\n",
    "historyFrame[[\"loss\", \"val_loss\"]].plot()\n",
    "historyFrame.to_csv(f\"{MDL_PATH}/history_mdl{Params['version']:03}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "historyFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch = historyFrame.val_auc.argmax()\n",
    "best_loss = historyFrame.iloc[best_epoch].loss\n",
    "best_auc = historyFrame.iloc[best_epoch].val_auc\n",
    "print(\"best epoch:\", best_epoch,\n",
    "      \"| best loss:\", best_loss,\n",
    "      \"| best auc:\", best_auc\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Params.copy()\n",
    "result[\"bavg_epoch\"] = int(best_epoch)\n",
    "result[\"bavg_loss\"] = float(best_loss)\n",
    "result[\"bavg_auc\"] = float(best_auc)\n",
    "with open(f\"{MDL_PATH}/params.json\", \"w\") as file:\n",
    "    json.dump(result, file)\n",
    "\n",
    "if not os.path.exists(f\"{MDLS_PATH}/results.csv\"):\n",
    "    df_save = pd.DataFrame(result, index=[0])\n",
    "    df_save.to_csv(f\"{MDLS_PATH}/results.csv\")\n",
    "else:\n",
    "    df_old = pd.read_csv(f\"{MDLS_PATH}/results.csv\", index_col=0)\n",
    "    df_save = pd.DataFrame(result, index = [df_old.index.max() + 1])\n",
    "    df_save = df_old.append(df_save, ignore_index=True)\n",
    "    df_save.to_csv(f\"{MDLS_PATH}/results.csv\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(f\"{MDLS_PATH}/results.csv\",index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_ds, val_ds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_scores = []\n",
    "with strategy.scope():\n",
    "    model = tf.keras.models.load_model(f\"{MDL_PATH}/model_{Params['version']:03}.h5\")\n",
    "train_df = pd.read_csv(\"../input/g2net-gravitational-wave-detection/training_labels.csv\")\n",
    "for ds_ind in range(len(all_train_files)):\n",
    "    train_set = load_dataset(all_train_files[ds_ind], shuffle=False, ordered=True, labeled=True, repeat=False, return_labels=False)\n",
    "    prediction = model.predict(train_set)\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    prediction_scores.append(roc_auc_score(train_df.target, prediction))\n",
    "print(prediction_scores)\n",
    "best_pred = np.array(prediction_scores).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = load_dataset(test_files[best_pred], shuffle=False, ordered=True, labeled=False, repeat=False, return_labels=False)\n",
    "test_prediction = model.predict(test_set)\n",
    "sub = pd.read_csv(\"../input/g2net-gravitational-wave-detection/sample_submission.csv\")\n",
    "sub.target = test_prediction.flatten()\n",
    "sub.to_csv(f\"{MDL_PATH}/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
