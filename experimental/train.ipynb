{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:41.713595Z",
     "iopub.status.busy": "2021-09-17T17:22:41.713335Z",
     "iopub.status.idle": "2021-09-17T17:22:41.725791Z",
     "shell.execute_reply": "2021-09-17T17:22:41.725140Z",
     "shell.execute_reply.started": "2021-09-17T17:22:41.713521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KAGGLE: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "KAGGLE = True\n",
    "if os.name == \"nt\":\n",
    "    KAGGLE = False\n",
    "print(f\"KAGGLE: {KAGGLE}\")\n",
    "if not KAGGLE:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:41.727703Z",
     "iopub.status.busy": "2021-09-17T17:22:41.726861Z",
     "iopub.status.idle": "2021-09-17T17:22:43.782183Z",
     "shell.execute_reply": "2021-09-17T17:22:43.781042Z",
     "shell.execute_reply.started": "2021-09-17T17:22:41.727659Z"
    }
   },
   "outputs": [],
   "source": [
    "# general\n",
    "import gc\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "from glob import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from IPython.core.display import display, HTML\n",
    "if KAGGLE:\n",
    "    from kaggle_datasets import KaggleDatasets\n",
    "\n",
    "# ML\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# DL\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:43.784172Z",
     "iopub.status.busy": "2021-09-17T17:22:43.783660Z",
     "iopub.status.idle": "2021-09-17T17:22:43.799591Z",
     "shell.execute_reply": "2021-09-17T17:22:43.798788Z",
     "shell.execute_reply.started": "2021-09-17T17:22:43.784124Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tf.compat.v1.disable_eager_execution()\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:43.803149Z",
     "iopub.status.busy": "2021-09-17T17:22:43.802737Z",
     "iopub.status.idle": "2021-09-17T17:22:43.811423Z",
     "shell.execute_reply": "2021-09-17T17:22:43.810381Z",
     "shell.execute_reply.started": "2021-09-17T17:22:43.803115Z"
    }
   },
   "outputs": [],
   "source": [
    "def auto_select_accelerator():\n",
    "    TPU_DETECTED = False\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "        tf.keras.mixed_precision.set_global_policy(\"mixed_bfloat16\")\n",
    "        print(f\"Running on TPU: {tpu.master()}\")\n",
    "        TPU_DETECTED = True\n",
    "    except ValueError:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "        tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    num_replicas = strategy.num_replicas_in_sync\n",
    "    print(f\"Running on {num_replicas} replica{'s' if num_replicas > 1 else ''}\")\n",
    "    return strategy, TPU_DETECTED, num_replicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:43.813082Z",
     "iopub.status.busy": "2021-09-17T17:22:43.812779Z",
     "iopub.status.idle": "2021-09-17T17:22:49.385095Z",
     "shell.execute_reply": "2021-09-17T17:22:49.384154Z",
     "shell.execute_reply.started": "2021-09-17T17:22:43.813043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "Running on 1 replica\n"
     ]
    }
   ],
   "source": [
    "strategy, TPU_Detected, REPLICAS = auto_select_accelerator()\n",
    "INPUT_DIR = \"../input/g2net-gravitational-wave-detection\"\n",
    "MDLS_PATH = \".\" if KAGGLE else \"../models\"\n",
    "# TRAIN_FILES_PATH = \"../input/filtered*_tfrec\"\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "tfrec_folders = [\"filtered-whitened-tfrec\"]#[\"filtered-tfrec\", \"filtered-whitened-tfrec\", \"filtered-whitened-inverted-tfrec\", \"whitened-tfrec\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:10:49.388016Z",
     "iopub.status.busy": "2021-09-17T17:10:49.387794Z",
     "iopub.status.idle": "2021-09-17T17:10:49.406903Z",
     "shell.execute_reply": "2021-09-17T17:10:49.405619Z",
     "shell.execute_reply.started": "2021-09-17T17:10:49.387988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>version</th>\n",
       "      <th>train_mode</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>bavg_epoch</th>\n",
       "      <th>bavg_loss</th>\n",
       "      <th>bavg_auc</th>\n",
       "      <th>changelog</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>76</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.469206</td>\n",
       "      <td>0.832270</td>\n",
       "      <td>base model with same padding</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>78</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "      <td>0.442443</td>\n",
       "      <td>0.841306</td>\n",
       "      <td>doubled last layer in all blocks. lr/10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>79</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>25</td>\n",
       "      <td>0.439258</td>\n",
       "      <td>0.842578</td>\n",
       "      <td>doubled last layer in all blocks doubled size ...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>83</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>27</td>\n",
       "      <td>0.439051</td>\n",
       "      <td>0.841536</td>\n",
       "      <td>added dropout layers to dense layers</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>88</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.453092</td>\n",
       "      <td>0.834111</td>\n",
       "      <td>everything had 3 layers now and relu activation</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>92</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>17</td>\n",
       "      <td>0.434836</td>\n",
       "      <td>0.832301</td>\n",
       "      <td>dropout layers</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>110</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>11</td>\n",
       "      <td>0.470093</td>\n",
       "      <td>0.815836</td>\n",
       "      <td>dropout layers</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>113</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "      <td>0.476270</td>\n",
       "      <td>0.813889</td>\n",
       "      <td>dropout layers</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>119</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>36</td>\n",
       "      <td>0.463551</td>\n",
       "      <td>0.812370</td>\n",
       "      <td>new dataset using best model (base model with ...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.00020</td>\n",
       "      <td>124</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>23</td>\n",
       "      <td>0.448196</td>\n",
       "      <td>0.827187</td>\n",
       "      <td>10x lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>134</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>12</td>\n",
       "      <td>0.467185</td>\n",
       "      <td>0.826250</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>135</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>9</td>\n",
       "      <td>0.444164</td>\n",
       "      <td>0.848162</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>136</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>21</td>\n",
       "      <td>0.420500</td>\n",
       "      <td>0.854332</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>137</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>31</td>\n",
       "      <td>0.364450</td>\n",
       "      <td>0.876402</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>139</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.436394</td>\n",
       "      <td>0.845982</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>139</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.436394</td>\n",
       "      <td>0.845982</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>140</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>0.423539</td>\n",
       "      <td>0.844709</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>142</td>\n",
       "      <td>full</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.447673</td>\n",
       "      <td>0.836974</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>164</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>9</td>\n",
       "      <td>0.438891</td>\n",
       "      <td>0.844589</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>200</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>36</td>\n",
       "      <td>0.464103</td>\n",
       "      <td>0.839057</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lr  version train_mode  batch_size  epochs  bavg_epoch  bavg_loss  \\\n",
       "24  0.00100       76       full         256      10           6   0.469206   \n",
       "25  0.00010       78       full         256      40          12   0.442443   \n",
       "26  0.00050       79       full         256      40          25   0.439258   \n",
       "27  0.00050       83       full         256      40          27   0.439051   \n",
       "28  0.00010       88       full         256      40           8   0.453092   \n",
       "29  0.00010       92       full         256     100          17   0.434836   \n",
       "30  0.00010      110       full         256     100          11   0.470093   \n",
       "31  0.00010      113       full         256     100           8   0.476270   \n",
       "32  0.00002      119       full         256     100          36   0.463551   \n",
       "33  0.00020      124       full         256     100          23   0.448196   \n",
       "34  0.00010      134       full         256     100          12   0.467185   \n",
       "35  0.00010      135       full         256     100           9   0.444164   \n",
       "36  0.00010      136       full         256     100          21   0.420500   \n",
       "37  0.00010      137       full         256     100          31   0.364450   \n",
       "38  0.00010      139       full         256     100          10   0.436394   \n",
       "39  0.00010      139       full         256     100          10   0.436394   \n",
       "40  0.00010      140       full         256      15          12   0.423539   \n",
       "41  0.00010      142       full         128       5           4   0.447673   \n",
       "42  0.00010      164       full         256     100           9   0.438891   \n",
       "43  0.00001      200       full         256     100          36   0.464103   \n",
       "\n",
       "    bavg_auc                                          changelog  seed  \n",
       "24  0.832270                       base model with same padding  42.0  \n",
       "25  0.841306            doubled last layer in all blocks. lr/10  42.0  \n",
       "26  0.842578  doubled last layer in all blocks doubled size ...  42.0  \n",
       "27  0.841536               added dropout layers to dense layers  42.0  \n",
       "28  0.834111    everything had 3 layers now and relu activation  42.0  \n",
       "29  0.832301                                     dropout layers  42.0  \n",
       "30  0.815836                                     dropout layers  42.0  \n",
       "31  0.813889                                     dropout layers  42.0  \n",
       "32  0.812370  new dataset using best model (base model with ...  42.0  \n",
       "33  0.827187                                             10x lr  42.0  \n",
       "34  0.826250                                             1/4 lr  42.0  \n",
       "35  0.848162                                             1/4 lr  42.0  \n",
       "36  0.854332                                             1/4 lr  42.0  \n",
       "37  0.876402                                             1/4 lr  42.0  \n",
       "38  0.845982                                             1/4 lr  42.0  \n",
       "39  0.845982                                             1/4 lr  42.0  \n",
       "40  0.844709                                             1/4 lr  42.0  \n",
       "41  0.836974                                             1/4 lr  42.0  \n",
       "42  0.844589                                             1/4 lr  42.0  \n",
       "43  0.839057                                             1/4 lr  42.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = \"no file\"\n",
    "if not KAGGLE:\n",
    "    results_df = pd.read_csv(\"../models/results.csv\", index_col=[0]).tail(20)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:49.386775Z",
     "iopub.status.busy": "2021-09-17T17:22:49.386544Z",
     "iopub.status.idle": "2021-09-17T17:22:49.599221Z",
     "shell.execute_reply": "2021-09-17T17:22:49.598282Z",
     "shell.execute_reply.started": "2021-09-17T17:22:49.386749Z"
    }
   },
   "outputs": [],
   "source": [
    "if KAGGLE:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    user_credential = user_secrets.get_gcloud_credential()\n",
    "    user_secrets.set_tensorflow_credential(user_credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:49.601675Z",
     "iopub.status.busy": "2021-09-17T17:22:49.601417Z",
     "iopub.status.idle": "2021-09-17T17:22:49.608611Z",
     "shell.execute_reply": "2021-09-17T17:22:49.607652Z",
     "shell.execute_reply.started": "2021-09-17T17:22:49.601646Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{MDLS_PATH}/results.csv\"):\n",
    "    VER = 1\n",
    "else:\n",
    "    results = pd.read_csv(f\"{MDLS_PATH}/results.csv\", index_col=[0])\n",
    "    VER = int(results.version.max())\n",
    "Params ={\n",
    "    \"lr\": 0.001 * REPLICAS,\n",
    "    \"version\": VER,\n",
    "    \"train_mode\": \"full\", #test, full\n",
    "    \"batch_size\": 256 * REPLICAS,\n",
    "    \"epochs\":100,\n",
    "    \"seed\": 42,\n",
    "    \"changelog\": \"1/4 lr\",\n",
    "}\n",
    "seed_everything(Params[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:49.609927Z",
     "iopub.status.busy": "2021-09-17T17:22:49.609716Z",
     "iopub.status.idle": "2021-09-17T17:22:49.626003Z",
     "shell.execute_reply": "2021-09-17T17:22:49.624837Z",
     "shell.execute_reply.started": "2021-09-17T17:22:49.609902Z"
    }
   },
   "outputs": [],
   "source": [
    "VER = Params[\"version\"]\n",
    "MDL_PATH = f\"{MDLS_PATH}/models_v{VER:03}\"\n",
    "while os.path.exists(MDL_PATH):\n",
    "    VER += 1\n",
    "    MDL_PATH = f\"{MDLS_PATH}/models_v{VER:03}\"\n",
    "Params[\"version\"]=VER\n",
    "os.mkdir(MDL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_cut(x, y):\n",
    "    a = np.zeros(x.shape, dtype=np.float32)\n",
    "    dt = np.random.randint(2,512)\n",
    "    t0 = np.random.randint(1,dt)\n",
    "    t1 = np.random.randint(0,t0)\n",
    "    a[:,t1:t1+(3584)] = x[:,t0:t0+(3584)]\n",
    "    a[:] *= np.array([-1 if random.random() > 0.5 else 1,-1 if random.random() > 0.5 else 1, -1 if random.random() > 0.5 else 1])\n",
    "    return a,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:49.628201Z",
     "iopub.status.busy": "2021-09-17T17:22:49.627767Z",
     "iopub.status.idle": "2021-09-17T17:22:49.642197Z",
     "shell.execute_reply": "2021-09-17T17:22:49.641358Z",
     "shell.execute_reply.started": "2021-09-17T17:22:49.628155Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "input_context = tf.distribute.InputContext(\n",
    "    input_pipeline_id=1,\n",
    "    num_input_pipelines = 8\n",
    ")\n",
    "read_config= tfds.ReadConfig(\n",
    "    input_context=input_context\n",
    ")\n",
    "\n",
    "def load_dataset(files, shuffle=True, ordered=False, labeled = True, repeat=True, return_labels = False, cut = False):\n",
    "    if ordered:\n",
    "        dataset = tf.data.TFRecordDataset(files, num_parallel_reads=None)\n",
    "    else:\n",
    "        dataset = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n",
    "\n",
    "\n",
    "    def _parse_function(example_proto):\n",
    "        if labeled:\n",
    "            keys_to_feature = {\n",
    "                \"TimeSeries\":tf.io.FixedLenFeature([4096,3],tf.float32),\n",
    "                \"Target\":tf.io.FixedLenFeature([], tf.int64, default_value=0)}\n",
    "            if return_labels:\n",
    "                keys_to_feature[\"id\"]=tf.io.FixedLenFeature([],tf.string, default_value=\"\")\n",
    "        else:\n",
    "            keys_to_feature = {\n",
    "                \"TimeSeries\": tf.io.FixedLenFeature([4096,3],tf.float32)\n",
    "            }\n",
    "        parsed_features = tf.io.parse_single_example(example_proto, keys_to_feature)\n",
    "        if labeled:\n",
    "            if return_labels:\n",
    "                return parsed_features[\"TimeSeries\"], parsed_features[\"Target\"], parsed_features[\"id\"]\n",
    "            else:\n",
    "                return parsed_features[\"TimeSeries\"], parsed_features[\"Target\"]\n",
    "        else:\n",
    "            return parsed_features[\"TimeSeries\"]\n",
    "    \n",
    "    if not ordered:\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic=False\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "    # parse the record into tensors.\n",
    "    dataset = dataset.map(_parse_function, num_parallel_calls=AUTO)\n",
    "#     dataset = dataset.cache()\n",
    "\n",
    "    # Repeat the input infinitely\n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "\n",
    "    # shuffle the dataset\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=10000, reshuffle_each_iteration=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # Generate batches\n",
    "    dataset = dataset.batch(Params[\"batch_size\"])\n",
    "    if cut:\n",
    "        dataset = dataset.map(lambda x,y:tf.numpy_function(random_cut, inp=[x,y], Tout=[tf.float32, tf.int64]), num_parallel_calls=AUTO)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:49.644543Z",
     "iopub.status.busy": "2021-09-17T17:22:49.644208Z",
     "iopub.status.idle": "2021-09-17T17:22:53.400603Z",
     "shell.execute_reply": "2021-09-17T17:22:53.399615Z",
     "shell.execute_reply.started": "2021-09-17T17:22:49.644502Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_val_files(folders):\n",
    "    train_files = []\n",
    "    val_files = []\n",
    "    test_files = []\n",
    "    all_train_files = []\n",
    "    for folder in folders:\n",
    "        if KAGGLE:\n",
    "            TRAIN_FILES_PATH = KaggleDatasets().get_gcs_path(folder)\n",
    "            TEST_FILES_PATH = KaggleDatasets().get_gcs_path(f\"{folder}test\")\n",
    "            all_files_train = np.sort(tf.io.gfile.glob(f\"{TRAIN_FILES_PATH}/train_*.tfrec\"))\n",
    "            all_files_test = np.sort(tf.io.gfile.glob(f\"{TEST_FILES_PATH}/test_*.tfrec\"))\n",
    "        else:\n",
    "            all_files_train = np.sort(glob(f\"../input/{folder}/train_*.tfrec\"))\n",
    "            all_files_test = np.sort(glob(f\"../input/{folder}/test_*.tfrec\"))\n",
    "        train_files.extend(all_files_train[:-2])\n",
    "        val_files.extend(all_files_train[-2:])\n",
    "        test_files.append(all_files_test)\n",
    "        all_train_files.append(all_files_train)\n",
    "    return train_files, val_files, test_files, all_train_files\n",
    "\n",
    "train_files, val_files, test_files, all_train_files = get_train_val_files(tfrec_folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.4423501999999999\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def benchmark(dataset, num_epochs=2):\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        for sample in dataset:\n",
    "            # Performing a training step\n",
    "            time.sleep(0.01)\n",
    "    print(\"Execution time:\", time.perf_counter() - start_time)\n",
    "benchmark(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:53.402399Z",
     "iopub.status.busy": "2021-09-17T17:22:53.402073Z",
     "iopub.status.idle": "2021-09-17T17:22:53.738249Z",
     "shell.execute_reply": "2021-09-17T17:22:53.737171Z",
     "shell.execute_reply.started": "2021-09-17T17:22:53.402358Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ds = load_dataset(train_files, cut = True)\n",
    "val_ds = load_dataset(val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_ds:\n",
    "    tensor = x\n",
    "    input_tensor_shape = x[0].shape[1]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrfn(epoch):\n",
    "    epoch = epoch - 2\n",
    "    lr_start = 1e-4\n",
    "    lr_max = 0.0000015 * Params[\"batch_size\"]\n",
    "    lr_min = 1e-7\n",
    "    lr_ramp_ep = 3\n",
    "    lr_sus_ep = 0\n",
    "    lr_decay = 0.7\n",
    "    if epoch < 0:\n",
    "        lr = lr_start\n",
    "    elif epoch < lr_ramp_ep:\n",
    "        lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "    elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "        lr = lr_max\n",
    "    else:\n",
    "        lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "    return lr\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x18f27e6ad88>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD7CAYAAACWq8i5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAl1ElEQVR4nO3de5Cc1X3m8e8zd2mk0UjWBVkSlojHxgI2XCZCyeJdx4RYIiTC62UDRSwtJqVgg+Ok1rUWu8vupsrZIqnUJmFNRHCCkZy1WSo3JokcgkXYtR1jIwwGBCgai4sEsjTiMqPLjOb22z/6tGhGM92vNN2a6dbzKbq63/Oe877njMT8dM55+xxFBGZmZlnUTXUFzMysejhomJlZZg4aZmaWmYOGmZll5qBhZmaZOWiYmVlmmYKGpDWSdknqlrRpnPOSdFc6/4ykS0uVlTRP0iOSdqf3uWOuea6kI5I+X5B2maRn07XukqTTa7aZmZ2OkkFDUj1wN7AWWAncIGnlmGxrgY702ghszlB2E7A9IjqA7em40O8D3xiTtjldP3+vNaWbaGZm5dKQIc8qoDsi9gBIegBYBzxfkGcdsDVy3xR8XFK7pMXA8iJl1wEfSeW3AI8BX0j5rgX2AEfzN0jXa4uI76bjrcC1nBxY3mX+/PmxfPnyDM00M7O8J5988lBELBibniVoLAH2FhzvAy7PkGdJibKLImI/QETsl7QQQFIrueBxFfD5grJLUvmx9yhq+fLl7Nixo1Q2MzMrIOmV8dKzzGmMN28wdu2RifJkKTvWbwG/HxFHTqMeuYzSRkk7JO3o6ekpcTszM8sqS09jH7Cs4Hgp8HrGPE1Fyh6QtDj1MhYDB1P65cC/lfS7QDswKmkA+ItUvlg9AIiIe4F7ATo7O724lplZmWTpaTwBdEhaIakJuB7oGpOnC1ifnqJaDfSmoadiZbuADenzBuAhgIj4cEQsj4jlwB8A/yMivpSud1jS6vTU1Pp8GTMzOzNK9jQiYljSbcDDQD1wX0TslHRLOn8PsA24GugGjgE3FSubLn0n8KCkm4FXgesy1PfTwP3ADHIT4EUnwc3MrLxU60ujd3Z2hifCzcxOjaQnI6JzbLq/EW5mZpk5aJiZWWYOGhV2eGCIv37qtamuhplZWThoVNjfPrOf3/g/T/Pa2/1TXRUzs0lz0Kiwt44NAtDXPzTFNTEzmzwHjQrr6x8G4Ojx4SmuiZnZ5DloVFjfQK6HcdhBw8xqgINGheWHpdzTMLNa4KBRYX0DHp4ys9rhoFFh+Z7GkeMjU1wTM7PJc9CosBNBY8A9DTOrfg4aFZafCD866KBhZtXPQaOCIuLEI7dHPKdhZjXAQaOCjg+PMjgyCnh4ysxqg4NGBRV+C9xPT5lZLXDQqKD8fAZ4eMrMaoODRgX1pvmMxnp5ItzMakKmoCFpjaRdkrolbRrnvCTdlc4/I+nSUmUlzZP0iKTd6X1uSl8l6en0+qGkjxeUeSxdK39+4eSaX1n54anFc2Z4TsPMakLJoCGpHrgbWAusBG6QtHJMtrVAR3ptBDZnKLsJ2B4RHcD2dAzwHNAZERcDa4A/llS4l/mNEXFxeh08xfaeUfnhqcVzWvzlPjOrCVl6GquA7ojYExGDwAPAujF51gFbI+dxoF3S4hJl1wFb0uctwLUAEXEsIvL/LG8BqnYT83xP473tMzwRbmY1IUvQWALsLTjel9Ky5ClWdlFE7AdI7yeGmiRdLmkn8CxwS0EQAfhKGpq6Q5Iy1H/K5NedWjynhf6hEUZGqzb+mZkB2YLGeL+Yx/72myhPlrInZ4j4XkRcAPwUcLuklnTqxoi4CPhwen1y3ApLGyXtkLSjp6en1O0qpq9/iOaGOua1NgF+gsrMql+WoLEPWFZwvBR4PWOeYmUPpCEs0vtJ8xMR8QJwFLgwHb+W3g8DXyM3/HWSiLg3IjojonPBggUZmlgZfQNDtM1oZFZzbkrGQ1RmVu2yBI0ngA5JKyQ1AdcDXWPydAHr01NUq4HeNORUrGwXsCF93gA8BJDyNqTP7wM+CLwsqUHS/JTeCFxDbtJ82urrH6atpYFWBw0zqxENpTJExLCk24CHgXrgvojYKemWdP4eYBtwNdANHANuKlY2XfpO4EFJNwOvAtel9CuATZKGgFHgMxFxSFIr8HAKGPXAN4EvT/onUEG9/UPMmdHIrJbcj9m795lZtSsZNAAiYhu5wFCYdk/B5wBuzVo2pb8BXDlO+leBr46TfhS4LEt9p4u+gSHmtTZ5eMrMaoa/EV5Bff1DtLU00trkoGFmtcFBo4L6BoZpm9FwoqfhL/iZWbVz0KiQ3F4auZ5Gfk7jSMEChmZm1chBo0L6h0YYHg3aZjTS2lwPwNFB9zTMrLo5aFRIfse+OTMaaW6op7Fe/nKfmVU9B40K6U3rTrW1NAIwq7nBE+FmVvUcNCokv8Jt24zcfEZrc4OXRzezquegUSF94/Q0PDxlZtXOQaNC3ulp5IJGa3ODd+8zs6rnoFEh+YnwtvS47SwPT5lZDXDQqJATw1MzPDxlZrXDQaNC+gaGmNlUT2N97kfc2lzPUX8j3MyqnINGhfSmb4PntfqRWzOrAQ4aFdLXP3zicVuA2c0NHBkcJrcgsJlZdXLQqJC+gZN7GhFwzEuJmFkVc9CokPxWr3nevc/MaoGDRoXkt3rNm51f6dZBw8yqWKagIWmNpF2SuiVtGue8JN2Vzj8j6dJSZSXNk/SIpN3pfW5KXyXp6fT6oaSPF5S5TNKz6Vp3SdLkml85fQO5rV7z8hsxOWiYWTUrGTQk1QN3A2uBlcANklaOybYW6EivjcDmDGU3AdsjogPYno4BngM6I+JiYA3wx5Ly/2TfnK6fv9eaU2zvGXFiL41xhqccNMysmmXpaawCuiNiT0QMAg8A68bkWQdsjZzHgXZJi0uUXQdsSZ+3ANcCRMSxiMj/Zm0BAiBdry0ivpv2JN+aLzPdHDk+zGjwronwd/YJ90S4mVWvLEFjCbC34HhfSsuSp1jZRRGxHyC9L8xnknS5pJ3As8AtKYgsSeWL1SNffqOkHZJ29PT0ZGhiefWl5UIKH7k9sXvfce/eZ2bVK0vQGG/eYOyXDSbKk6XsyRkivhcRFwA/BdwuqeVUrhUR90ZEZ0R0LliwoNTtym7sCrfAid37vE+4mVWzLEFjH7Cs4Hgp8HrGPMXKHkhDTvmhp4NjbxwRLwBHgQvTtZaWqMe0MHbdKSgcnvKchplVryxB4wmgQ9IKSU3A9UDXmDxdwPr0FNVqoDcNORUr2wVsSJ83AA8BpLwN6fP7gA8CL6frHZa0Oj01tT5fZro5MTxV0NOY0VhPnRw0zKy6NZTKEBHDkm4DHgbqgfsiYqekW9L5e4BtwNVAN3AMuKlY2XTpO4EHJd0MvApcl9KvADZJGgJGgc9ExKF07tPA/cAM4BvpNe3kexqFj9xKorW5gcNeHt3MqljJoAEQEdvIBYbCtHsKPgdwa9ayKf0N4Mpx0r8KfHWCa+0gN1Q1rY3d6jXP+4SbWbXzN8IroDf1NPLzGHnevc/Mqp2DRgX09Q8zq7mBhvp3/3hbmxv89JSZVTUHjQrIrXB78sjf7OYGjgz4expmVr0cNCpg7BIied69z8yqnYNGBYxdFj2v1fuEm1mVc9CogNyy6CcHjdkOGmZW5Rw0KiDX0zh5TiO/T7i3fDWzauWgUQG9/UPj9jRamxsYHg2OD49OQa3MzCbPQaPMRkeDI8eHx53T8PpTZlbtHDTK7PDxYSIY95HbWd6IycyqnINGmY23wm1efk8Nrz9lZtXKQaPM8utOzRknaOTT8suMmJlVGweNMuvrP3lZ9Ly5M5sAeOvY4Bmtk5lZuTholNlEK9wCzJ2ZCyRvHXNPw8yqk4NGmfWOs9VrXnvqabx91D0NM6tODhplVmwivKmhjtamevc0zKxqZQoaktZI2iWpW9Kmcc5L0l3p/DOSLi1VVtI8SY9I2p3e56b0qyQ9KenZ9P7RgjKPpWs9nV4LJ9f88usbGEbKLRkynvaZTbztOQ0zq1Ilg4akeuBuYC2wErhB0sox2dYCHem1EdicoewmYHtEdADb0zHAIeAXI+IicnuHj93F78aIuDi9Dp5KY8+Evv4hZjU3UFencc/PbW30RLiZVa0sPY1VQHdE7ImIQeABYN2YPOuArZHzONAuaXGJsuuALenzFuBagIh4KiJeT+k7gRZJzafXvDOvb2Bo3Mdt8+bObPLwlJlVrSxBYwmwt+B4X0rLkqdY2UURsR8gvY831PQJ4KmIOF6Q9pU0NHWHpPH/OT+FJlrhNs/DU2ZWzbIEjfF+MY9dpnWiPFnKjn9T6QLgd4BfK0i+MQ1bfTi9PjlB2Y2Sdkja0dPTk+V2ZZPbgGn8+QzIPXbrnoaZVassQWMfsKzgeCnwesY8xcoeSENYpPcT8xOSlgJ/BayPiB/l0yPitfR+GPgaueGvk0TEvRHRGRGdCxYsyNDE8slt9Vq8p9E3MMTIqJdHN7PqkyVoPAF0SFohqQm4Hugak6cLWJ+eoloN9KYhp2Jlu8hNdJPeHwKQ1A78HXB7RHwnfwNJDZLmp8+NwDXAc6fa4EqbaKvXvLkzG4nwUiJmVp0mHkdJImJY0m3Aw0A9cF9E7JR0Szp/D7ANuBroBo4BNxUrmy59J/CgpJuBV4HrUvptwPuBOyTdkdJ+HjgKPJwCRj3wTeDLk2l8JfQNFJ/TKFxKZF5r05mqlplZWZQMGgARsY1cYChMu6fgcwC3Zi2b0t8Arhwn/YvAFyeoymVZ6jtVhkdG014aE/9Y29NSIp4MN7Nq5G+El1F+n4xij9zmlxJ566iHp8ys+jholFGxFW7z8osWvu05DTOrQg4aZdRbZN2pvBOLFnp4ysyqkINGGZ1YFn2crV7z2loaqK+TlxIxs6rkoFFGxVa4zZNE+wx/wc/MqpODRhm9swHTxEEDck9QeXjKzKqRg0YZvTMRXvxJ5rkzm/z0lJlVJQeNMuobGKJOMGuCvTTy2mc2eU7DzKqSg0YZ5ZcQKbX47tyZjbztOQ0zq0IOGmXU2198scK8ua3uaZhZdXLQKKO+geJLiOS1z2zk+PAo/YMjZ6BWZmbl46BRRn1ZexoFixaamVUTB40yKrWXRl5+KREHDTOrNg4aZdTXn3V4Kr+UiCfDzay6OGiUUd/AUNEVbvM8PGVm1cpBo0yGRkY5NjhyisNT7mmYWXVx0CiTLOtO5Z0YnjrqnoaZVZdMQUPSGkm7JHVL2jTOeUm6K51/RtKlpcpKmifpEUm70/vclH6VpCclPZveP1pQ5rKU3p3uV/xbdGdQ30BaQiTDnEZTQx2tTfXuaZhZ1SkZNCTVA3cDa4GVwA2SVo7JthboSK+NwOYMZTcB2yOiA9iejgEOAb8YERcBG4CvFtxnc7p+/l5rTqWxlXSip5FheApyvQ0vWmhm1SZLT2MV0B0ReyJiEHgAWDcmzzpga+Q8DrRLWlyi7DpgS/q8BbgWICKeiojXU/pOoEVSc7peW0R8N+1JvjVfZjrIusJt3tzWRk+Em1nVyRI0lgB7C473pbQseYqVXRQR+wHS+8Jx7v0J4KmIOJ7K7StRjymTZavXQu0zmjw8ZWZVJ0vQGG/eIDLmyVJ2/JtKFwC/A/zaKdQjX3ajpB2SdvT09GS53aTlexpZHrkF76lhZtUpS9DYBywrOF4KvJ4xT7GyB9KQE+n9YD6TpKXAXwHrI+JHBfdYWqIeAETEvRHRGRGdCxYsKNnAcnjn6anSE+GQ9tRwT8PMqkyWoPEE0CFphaQm4Hqga0yeLmB9eopqNdCbhpyKle0iN9FNen8IQFI78HfA7RHxnfwN0vUOS1qdnppany8zHfT2D9FQJ2Y01mfKP3dmI30DQ4yMZup4mZlNCyWDRkQMA7cBDwMvAA9GxE5Jt0i6JWXbBuwBuoEvA58pVjaVuRO4StJu4Kp0TMr/fuAOSU+nV36+49PAn6T7/Aj4xmm3vMz6BrLtpZHXPrOJiFywMTOrFpnGUiJiG7nAUJh2T8HnAG7NWjalvwFcOU76F4EvTnCtHcCFWep8pvX1D5fc5rXQ3NZ3Fi2c19pUqWqZmZWVvxFeJvmeRlbvLFroyXAzqx4OGmWSdS+NvPmtzQD0HHbQMLPq4aBRJn0Dw5kftwVYNCcXNA70DVSqSmZmZeegUSa9/UOZH7eFXE+joU782EHDzKqIg0aZnOrwVF2dWDi7mQO9DhpmVj0cNMpgYGiE48OjpzQRDnDOnBb3NMysqjholMHh/LLop/DILThomFn1cdAog1Nd4TZvUVuLh6fMrKo4aJTBqe6lkXdOWwtHB0c4POBvhZtZdXDQKIN3du079TkN8GO3ZlY9HDTKIL9+1JxTeOQWcsNTAD/uPV72OpmZVYKDRhlMZngKYH9vf9nrZGZWCQ4aZXC6E+EenjKzauOgUQZ9/cM01dfR3HBqP86WxnrmzGj0Y7dmVjUcNMogt8JtQ+a9NAqd09biOQ0zqxoOGmXQ139qy6IXWjSnxcNTZlY1HDTKoPcU150qtLjN3wo3s+qRKWhIWiNpl6RuSZvGOS9Jd6Xzz0i6tFRZSfMkPSJpd3qfm9LfI+kfJR2R9KUx93ksXWvsNrBTqm9geFI9jUNHjjM0MlrmWpmZlV/JoCGpHrgbWAusBG6QtHJMtrVAR3ptBDZnKLsJ2B4RHcD2dAwwANwBfH6CKt0YERen18FMrayww/1Dp7zuVN45bS1EQM9hz2uY2fSXpaexCuiOiD0RMQg8AKwbk2cdsDVyHgfaJS0uUXYdsCV93gJcCxARRyPi2+SCR1U41a1eC52TNmPyEJWZVYMsQWMJsLfgeF9Ky5KnWNlFEbEfIL1nHWr6ShqaukOn87hSmUUEff3Dpz2nkf9WuBcuNLNqkCVojPeLOTLmyVL2VNwYERcBH06vT46XSdJGSTsk7ejp6ZnE7Uo7PjzK4MjoKe3aVyj/rXD3NMysGmQJGvuAZQXHS4HXM+YpVvZAGsIivZecn4iI19L7YeBr5Ia/xst3b0R0RkTnggULSl12UvpOrDt1ej2Nea1NNNXXOWiYWVXIEjSeADokrZDUBFwPdI3J0wWsT09RrQZ605BTsbJdwIb0eQPwULFKSGqQND99bgSuAZ7LUP+K6j3NdafyJLGwzdu+mll1KDmmEhHDkm4DHgbqgfsiYqekW9L5e4BtwNVAN3AMuKlY2XTpO4EHJd0MvApcl7+npJeBNqBJ0rXAzwOvAA+ngFEPfBP48qRaXwanu+5UoXPaWtjvoGFmVSDTQHxEbCMXGArT7in4HMCtWcum9DeAKycos3yCqlyWpb5nUl//6W31WmjRnBZ2vtZbriqZmVWMvxE+SeXoaeS/FZ6LvWZm05eDxiSd7l4ahc6Z08LA0OiJXouZ2XTloDFJ72z1OonhKT92a2ZVwkFjkvr6h2hprKO5of60r5HfjMlBw8ymOweNSZrMCrd5S9pnALD3zWPlqJKZWcU4aEzSZNadyjunrYWWxjpeOnS0TLUyM6sMB41Jyq07dfrzGQB1dWLF/Fns6TlSplqZmVWGg8YklaOnAXDe/Fb3NMxs2nPQmKS+MsxpAJy3oJW9b/UzOOzNmMxs+nLQmKTcrn2TG56CXNAYGQ1efdO9DTObvhw0JiEi6O0fOu0VbgudN38WAHt6HDTMbPpy0JiEY4MjjIxGWYanVixoBWCP5zXMbBpz0JiEcqw7ldfW0sj8Wc1+gsrMpjUHjUl4Z4XbyQcNyM1r+AkqM5vOHDQm4Z2exuQnwiH32K3nNMxsOnPQmIRyrHBb6LwFrbxxdJDeY0NluZ6ZWbk5aExCOec0oOAJqkOe1zCz6SlT0JC0RtIuSd2SNo1zXpLuSuefkXRpqbKS5kl6RNLu9D43pb9H0j9KOiLpS2Puc5mkZ9O17pKk02/65OV7BOV45BZyPQ3wY7dmNn2VDBqS6oG7gbXASuAGSSvHZFsLdKTXRmBzhrKbgO0R0QFsT8cAA8AdwOfHqc7mdP38vdZkamWF5PfSmD3Jtafyls2bSUOd3NMws2kry2+7VUB3ROwBkPQAsA54viDPOmBr2iv8cUntkhYDy4uUXQd8JJXfAjwGfCEijgLflvT+wkqk67VFxHfT8VbgWuAbp9bkbHa8/CbHBkeK5nnxx33MbKqnsb48o3yN9XWcO2+mn6Ays2krS9BYAuwtON4HXJ4hz5ISZRdFxH6AiNgvaWGGeuwb5x4nkbSRXI+Ec889t8Rlx3f7Xz7L7oOl/8V/3vzW07r+hNdb4CeozGz6yhI0xps3iIx5spTNKvO1IuJe4F6Azs7O07rf7//yxRwfLt7TAFg2d+bpXH5C5y2Yxbd2H2J0NKirm9IpGzOzk2QJGvuAZQXHS4HXM+ZpKlL2gKTFqZexGDiYoR5LS9SjbC5cMqdSly5qxfxWjg+P8npvP0vLHJDMzCYry2D8E0CHpBWSmoDrga4xebqA9ekpqtVAbxp6Kla2C9iQPm8AHipWiXS9w5JWp6em1pcqU43yw10eojKz6ahkTyMihiXdBjwM1AP3RcROSbek8/cA24CrgW7gGHBTsbLp0ncCD0q6GXgVuC5/T0kvA21Ak6RrgZ+PiOeBTwP3AzPITYBXZBJ8Kv3Ewtx3Nf75wGH+1QcWTHFtzMzeLdOzohGxjVxgKEy7p+BzALdmLZvS3wCunKDM8gnSdwAXZqlztZo/q5n3zmnhh/t6p7oqZmYn8TfCp6FLzp3L03vfmupqmJmdxEFjGrp4WTt73+zn0JHjU10VM7N3cdCYhi4+tx2Ap199e0rrYWY2loPGNHThe+fQUCee3vv2VFfFzOxdHDSmoRlN9Zy/eLaDhplNOw4a09TFy9r54d63GR093S/Qm5mVn4PGNHXxsrkcPj7Mj7xnuJlNIw4a09QlaTL8KQ9Rmdk04qAxTa14TyttLQ085SeozGwacdCYpurqxE8ua/dkuJlNKw4a09gly9rZ9eM+jg0OT3VVzMwAB41p7ZJz5zIa8KzXoTKzacJBYxr7yWXtAOx4xetQmdn04KAxjc1rbeLCJW08+mKp/anMzM4MB41p7qoPncMPXn2LnsNevNDMpp6DxjR31cpFRMCjLx6Y6qqYmWULGpLWSNolqVvSpnHOS9Jd6fwzki4tVVbSPEmPSNqd3ucWnLs95d8l6WMF6Y+ltKfTa+HpN706fGjxbJa0z+CR5x00zGzqlQwakuqBu4G1wErgBkkrx2RbC3Sk10Zgc4aym4DtEdEBbE/HpPPXAxcAa4A/StfJuzEiLk6vmh/sl8RVKxfxrd2H/OitmU25LD2NVUB3ROyJiEHgAWDdmDzrgK2R8zjQLmlxibLrgC3p8xbg2oL0ByLieES8RG7f8VWn17zacNXKRRwfHuVbuw9NdVXM7CyXJWgsAfYWHO9LaVnyFCu7KCL2A6T3/FBTqft9JQ1N3SFJGepf9VatmEdbS4OHqMxsymUJGuP9Yh67XvdEebKUPZX73RgRFwEfTq9PjnsBaaOkHZJ29PT0lLjd9NdYX8fPnr+QR188yIiXSjezKZQlaOwDlhUcLwVez5inWNkDaQiL9J6fn5iwTES8lt4PA19jgmGriLg3IjojonPBggUZmjj9XbVyEW8eHeRJf9HPzKZQlqDxBNAhaYWkJnKT1F1j8nQB69NTVKuB3jTkVKxsF7Ahfd4APFSQfr2kZkkryE2uf19Sg6T5AJIagWuA506jzVXpX39gAU31dWx7dv9UV8XMzmIlg0ZEDAO3AQ8DLwAPRsROSbdIuiVl2wbsITdp/WXgM8XKpjJ3AldJ2g1clY5J5x8Engf+Hrg1IkaAZuBhSc8ATwOvpXudFWa3NHL1Refw50/u4/DA0FRXx8zOUoqo7THyzs7O2LFjx1RXoyye3dfLL37p2/yXX/gQv/rh86a6OmZWwyQ9GRGdY9P9jfAqctHSOfzU8rnc/08ve0LczKaEg0aVufmKFex7q59/2Pnjqa6KmZ2FHDSqzFUrz2HZvBn86bdfmuqqmNlZyEGjytTXiX//MyvY8cpb/NBbwZrZGeagUYX+XedSZjU38L8e3T3VVTGzs4yDRhWa3dLIbR99P9984SB//5znNszszHHQqFI3X7GC88+ZzX/v2unvbZjZGeOgUaUa6+u48xP/ggOHB/i9h3dNdXXM7CzhoFHFLl7WzvrV72Pr46/w1Ktek8rMKs9Bo8p9/mMfZNHsFj73wNPeR9zMKs5Bo8rNbmlk869cSs/h49x0//c5cty7+5lZ5Tho1IBLzp3L3Tdewgv7D/PpP3uSweHRqa6SmdUoB40a8dHzF3Hnv7mIb+0+xGe//gOOusdhZhXgoFFDrutcxn+9ZiWPPH+Aj//Rd9jTc2Sqq2RmNcZBo8Z86ooVbP3U5Rw6Msgvfek7/M0PX6fWl783szPHQaMGXdExn7/57BX8xMJZfPbrT3HdPd/1NrFmVhYOGjVqSfsM/uKWn+a3P34hr7x5jE9s/id+dcsTPPriAYZHPFFuZqcnU9CQtEbSLkndkjaNc16S7krnn5F0aamykuZJekTS7vQ+t+Dc7Sn/LkkfK0i/TNKz6dxdknT6Ta99DfV13Hj5+3js8x/hN3/uAzz16tt86v4d/Mydj/Lbf/c83959iIGhkamupplVkZLbvUqqB/6Z3D7e+4AngBsi4vmCPFcDnwWuBi4H/jAiLi9WVtLvAm9GxJ0pmMyNiC9IWgl8HVgFvBf4JvCBiBiR9H3gc8Dj5PYlvysivlGs/rW03etkDY2M8uiLB3nwib3833/uYXg0aGqo47Jz53LR0jl8aPFszj+njfe9ZyYzmxqmurpmNoUm2u41y2+GVUB3ROxJF3oAWAc8X5BnHbA1chHocUntkhYDy4uUXQd8JJXfAjwGfCGlPxARx4GXJHUDqyS9DLRFxHfTtbYC1wJFg4a9o7G+jo9dcA4fu+Acjh4f5vsvvcm3uw/x/Zfe5P5/evld3++YP6uJJXNnsnB2MwtmNzN/VjPtMxppm9FIW0sDs5obmNFUz8ymBmY01tPcWEdzQx2N9fmXcEfQrPZkCRpLgL0Fx/vI9SZK5VlSouyiiNgPEBH7JS0suNbj41xrKH0em26nobW5gZ89fyE/e37uxz48MsqeQ0d58ceH2fvmMfa+eYx9b/Xz6hvH+MErb/HmsUFO9SGs+jpRXyca6kS9RF06rhNIufc6CcGJACOlF0rvpPR3AtC7QpHG/fgulQpeDok23f3tr19Bc0N9Wa+ZJWiM9//G2F8fE+XJUjbr/TJfS9JGYCPAueeeW+J2Brn5jw8sms0HFs0e9/zwyChHjg/T2z9EX/8wRweH6R8c4ejgMMeHRjk+PMrA0AhDI6MMjwaDw6OMjAbDo8HwyCgjEYyORu49IAIigpHRIEjHBOk/IuLEH25hsCr8Ay8cWp3wL1WFnjaOSl3YrIxUgX/aZAka+4BlBcdLgdcz5mkqUvaApMWpl7EYOFjiWvvS52L1ACAi7gXuhdycRrHGWTYN9XW0z2yifWbTVFfFzKZQlqenngA6JK2Q1ARcD3SNydMFrE9PUa0GetPQU7GyXcCG9HkD8FBB+vWSmiWtADqA76frHZa0Oj01tb6gjJmZnQElexoRMSzpNuBhoB64LyJ2Srolnb+H3JNMVwPdwDHgpmJl06XvBB6UdDPwKnBdKrNT0oPkJsuHgVsjIv9c6KeB+4EZ5CbAPQluZnYGlXzkttr5kVszs1M30SO3/ka4mZll5qBhZmaZOWiYmVlmDhpmZpaZg4aZmWVW809PSeoBXjnN4vOBQ2WsTjU4G9sMZ2e7z8Y2w9nZ7tNp8/siYsHYxJoPGpMhacd4j5zVsrOxzXB2tvtsbDOcne0uZ5s9PGVmZpk5aJiZWWYOGsXdO9UVmAJnY5vh7Gz32dhmODvbXbY2e07DzMwyc0/DzMwyc9AYh6Q1knZJ6k77l9ckScsk/aOkFyTtlPS5lD5P0iOSdqf3uVNd13KTVC/pKUl/m47Phja3S/pzSS+mP/OfrvV2S/rN9Hf7OUlfl9RSi22WdJ+kg5KeK0ibsJ2Sbk+/33ZJ+tip3MtBYwxJ9cDdwFpgJXCDpJVTW6uKGQb+Q0R8CFgN3JraugnYHhEdwPZ0XGs+B7xQcHw2tPkPgb+PiPOBnyTX/pptt6QlwK8DnRFxIbntGa6nNtt8P7BmTNq47Uz/j18PXJDK/FH6vZeJg8bJVgHdEbEnIgaBB4B1U1ynioiI/RHxg/T5MLlfIkvItXdLyrYFuHZKKlghkpYCvwD8SUFyrbe5DfhXwJ8CRMRgRLxNjbeb3J5BMyQ1ADPJ7fZZc22OiP8HvDkmeaJ2rgMeiIjjEfESuX2QVmW9l4PGyZYAewuO96W0miZpOXAJ8D1gUdopkfS+cAqrVgl/APxHYLQgrdbbfB7QA3wlDcv9iaRWarjdEfEa8HvkNnnbT25H0X+ghts8xkTtnNTvOAeNk423E3tNP2ImaRbwF8BvRETfVNenkiRdAxyMiCenui5nWANwKbA5Ii4BjlIbwzITSmP464AVwHuBVkm/MrW1mhYm9TvOQeNk+4BlBcdLyXVpa5KkRnIB439HxF+m5AOSFqfzi4GDU1W/CviXwC9Jepnc0ONHJf0Ztd1myP293hcR30vHf04uiNRyu38OeCkieiJiCPhL4Geo7TYXmqidk/od56BxsieADkkrJDWRmzDqmuI6VYQkkRvjfiEi/mfBqS5gQ/q8AXjoTNetUiLi9ohYGhHLyf3ZPhoRv0INtxkgIn4M7JX0wZR0JfA8td3uV4HVkmamv+tXkpu3q+U2F5qonV3A9ZKaJa0AOoDvZ72ov9w3DklXkxv3rgfui4jfntoaVYakK4BvAc/yzvj+fyI3r/EgcC65//Gui4ixk2xVT9JHgM9HxDWS3kONt1nSxeQm/5uAPcBN5P7hWLPtlvRbwC+Te1LwKeBXgVnUWJslfR34CLnVbA8A/w34ayZop6T/DHyK3M/lNyLiG5nv5aBhZmZZeXjKzMwyc9AwM7PMHDTMzCwzBw0zM8vMQcPMzDJz0DAzs8wcNMzMLDMHDTMzy+z/A6uktBgzXfx7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrs_list = []\n",
    "for i in range(100):\n",
    "    lrs_list.append(lrfn(i))\n",
    "plt.plot(lrs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tensor[0][0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-15T15:02:39.670012Z",
     "iopub.status.busy": "2021-09-15T15:02:39.669804Z",
     "iopub.status.idle": "2021-09-15T15:02:40.728147Z",
     "shell.execute_reply": "2021-09-15T15:02:40.727152Z",
     "shell.execute_reply.started": "2021-09-15T15:02:39.669988Z"
    }
   },
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "with strategy.scope():\n",
    "    model = Sequential([\n",
    "        layers.Conv1D(filters=32, kernel_size=16, padding=\"causal\",input_shape=[4096,3]),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=64, kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=128, kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=256, kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1),\n",
    "        layers.Activation(\"sigmoid\", dtype=\"float32\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        tf.keras.optimizers.Adam(learning_rate = Params[\"lr\"]),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"AUC\"]\n",
    "    )\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 560000 // 16 * len(train_files) // Params[\"batch_size\"]\n",
    "validation_steps = 560000 // 16 * len(val_files) // Params[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:53.740013Z",
     "iopub.status.busy": "2021-09-17T17:22:53.739592Z",
     "iopub.status.idle": "2021-09-17T17:22:54.947598Z",
     "shell.execute_reply": "2021-09-17T17:22:54.946600Z",
     "shell.execute_reply.started": "2021-09-17T17:22:53.739983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_9 (Conv1D)            (None, 4096, 32)          1568      \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 4096, 32)          16416     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1024, 32)          0         \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1024, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 1024, 64)          16448     \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 1024, 128)         65664     \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 1024, 128)         131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 256, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256, 128)          0         \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 256, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 256, 128)          131200    \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 256, 128)          131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 64, 256)           262400    \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 64, 256)           524544    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 16, 256)           0         \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               524416    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,813,377\n",
      "Trainable params: 1,813,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "with strategy.scope():\n",
    "    model = Sequential([\n",
    "        layers.Conv1D(filters=32, activation=\"relu\", kernel_size=16, padding=\"causal\",input_shape=[4096,3]),\n",
    "        layers.Conv1D(filters=32, activation=\"relu\", kernel_size=16, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=64, activation=\"relu\", kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, activation=\"relu\", kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, activation=\"relu\", kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=128, activation=\"relu\", kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, activation=\"relu\",kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=256, activation=\"relu\", kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=256, activation=\"relu\",kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1),\n",
    "        layers.Activation(\"sigmoid\", dtype=\"float32\")\n",
    "    ])\n",
    "    \n",
    "    lr_decayed_fn = tf.keras.experimental.CosineDecay(\n",
    "        1e-3,\n",
    "        steps_per_epoch\n",
    "    )\n",
    "    \n",
    "#     opt = tf.keras.optimizers.Adam(learning_rate = Params[\"lr\"])\n",
    "\n",
    "    opt = tfa.optimizers.AdamW(lr_decayed_fn, learning_rate = Params[\"lr\"])\n",
    "    model.compile(\n",
    "        ,\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"AUC\"]\n",
    "    )\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:54.953060Z",
     "iopub.status.busy": "2021-09-17T17:22:54.952812Z",
     "iopub.status.idle": "2021-09-17T17:22:54.957958Z",
     "shell.execute_reply": "2021-09-17T17:22:54.957019Z",
     "shell.execute_reply.started": "2021-09-17T17:22:54.953033Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_callback(batch_size=8):\n",
    "    lr_start = 1e-5\n",
    "    lr_max = 0.000015 * batch_size\n",
    "    lr_min = 1e-7\n",
    "    lr_amp_ep = 3\n",
    "    lr_sus_ep = 0\n",
    "    lr_decay = 0.7\n",
    "    \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "        return lr\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
    "    return lr_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_step_decay(epoch, lr):\n",
    "    drop_rate = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    return initial_learning_rate * math.pow(drop_rate, math.floor(epoch/epochs_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:54.960073Z",
     "iopub.status.busy": "2021-09-17T17:22:54.959840Z",
     "iopub.status.idle": "2021-09-17T17:22:54.971237Z",
     "shell.execute_reply": "2021-09-17T17:22:54.970202Z",
     "shell.execute_reply.started": "2021-09-17T17:22:54.960046Z"
    }
   },
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.2,\n",
    "    patience=5, min_lr = 0.000001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    f\"{MDL_PATH}/model_{Params['version']:03}.h5\",\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weight_only=False,\n",
    "    mode=\"auto\",\n",
    "    save_freq=\"epoch\"\n",
    ")\n",
    "\n",
    "callbacks=[get_lr_callback(Params[\"batch_size\"]) ,reduce_lr, early_stop, model_checkpoint]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = tf.keras.models.load_model(\"../models/models_v040/model_040.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:54.972832Z",
     "iopub.status.busy": "2021-09-17T17:22:54.972552Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1914/1914 [==============================] - 84s 43ms/step - loss: 0.6008 - auc: 0.7003 - val_loss: 0.5068 - val_auc: 0.8013\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1914/1914 [==============================] - 83s 43ms/step - loss: 0.4991 - auc: 0.8050 - val_loss: 0.4817 - val_auc: 0.8202\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1914/1914 [==============================] - 82s 43ms/step - loss: 0.4859 - auc: 0.8154 - val_loss: 0.4744 - val_auc: 0.8270\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.0001.\n",
      "1914/1914 [==============================] - 82s 43ms/step - loss: 0.4791 - auc: 0.8208 - val_loss: 0.4667 - val_auc: 0.8319\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.00019466666666666666.\n",
      "1914/1914 [==============================] - 83s 43ms/step - loss: 0.4785 - auc: 0.8209 - val_loss: 0.4677 - val_auc: 0.8313\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.00028933333333333334.\n",
      "1914/1914 [==============================] - 82s 43ms/step - loss: 0.4763 - auc: 0.8222 - val_loss: 0.4655 - val_auc: 0.8313\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.000384.\n",
      "1914/1914 [==============================] - 83s 43ms/step - loss: 0.4753 - auc: 0.8227 - val_loss: 0.4733 - val_auc: 0.8296\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.00026883.\n",
      "1914/1914 [==============================] - 83s 43ms/step - loss: 0.4659 - auc: 0.8301 - val_loss: 0.4566 - val_auc: 0.8373\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.00018821099999999998.\n",
      "1914/1914 [==============================] - 83s 43ms/step - loss: 0.4605 - auc: 0.8340 - val_loss: 0.4501 - val_auc: 0.8415\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.00013177769999999996.\n",
      "1914/1914 [==============================] - 84s 44ms/step - loss: 0.4561 - auc: 0.8371 - val_loss: 0.4490 - val_auc: 0.8420\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 9.227438999999999e-05.\n",
      "1914/1914 [==============================] - 84s 44ms/step - loss: 0.4532 - auc: 0.8392 - val_loss: 0.4472 - val_auc: 0.8442\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 6.462207299999999e-05.\n",
      "1914/1914 [==============================] - 84s 44ms/step - loss: 0.4505 - auc: 0.8413 - val_loss: 0.4465 - val_auc: 0.8440\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 4.5265451099999986e-05.\n",
      "1914/1914 [==============================] - 85s 44ms/step - loss: 0.4481 - auc: 0.8432 - val_loss: 0.4449 - val_auc: 0.8446\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 3.171581576999999e-05.\n",
      "1914/1914 [==============================] - 88s 46ms/step - loss: 0.4479 - auc: 0.8430 - val_loss: 0.4438 - val_auc: 0.8459\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 2.2231071038999987e-05.\n",
      "1914/1914 [==============================] - 85s 44ms/step - loss: 0.4463 - auc: 0.8441 - val_loss: 0.4444 - val_auc: 0.8457\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 1.559174972729999e-05.\n",
      "1914/1914 [==============================] - 85s 44ms/step - loss: 0.4455 - auc: 0.8451 - val_loss: 0.4448 - val_auc: 0.8452\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 1.0944224809109993e-05.\n",
      "1914/1914 [==============================] - 85s 44ms/step - loss: 0.4459 - auc: 0.8446 - val_loss: 0.4432 - val_auc: 0.8459\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 7.690957366376995e-06.\n",
      "1914/1914 [==============================] - 86s 45ms/step - loss: 0.4447 - auc: 0.8455 - val_loss: 0.4447 - val_auc: 0.8458\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 5.413670156463896e-06.\n",
      "1914/1914 [==============================] - 87s 46ms/step - loss: 0.4446 - auc: 0.8456 - val_loss: 0.4442 - val_auc: 0.8466\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 3.819569109524727e-06.\n",
      "1914/1914 [==============================] - 88s 46ms/step - loss: 0.4444 - auc: 0.8456 - val_loss: 0.4449 - val_auc: 0.8469\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 2.7036983766673087e-06.\n",
      "1914/1914 [==============================] - 88s 46ms/step - loss: 0.4444 - auc: 0.8457 - val_loss: 0.4445 - val_auc: 0.8462\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 1.922588863667116e-06.\n",
      "1914/1914 [==============================] - 88s 46ms/step - loss: 0.4442 - auc: 0.8458 - val_loss: 0.4434 - val_auc: 0.8469\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 1.3758122045669812e-06.\n",
      "1914/1914 [==============================] - 88s 46ms/step - loss: 0.4442 - auc: 0.8459 - val_loss: 0.4430 - val_auc: 0.8469\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 00024: LearningRateScheduler setting learning rate to 9.930685431968867e-07.\n",
      "1914/1914 [==============================] - 89s 47ms/step - loss: 0.4445 - auc: 0.8456 - val_loss: 0.4435 - val_auc: 0.8468\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 00025: LearningRateScheduler setting learning rate to 7.251479802378206e-07.\n",
      "1914/1914 [==============================] - 88s 46ms/step - loss: 0.4440 - auc: 0.8460 - val_loss: 0.4426 - val_auc: 0.8474\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 00026: LearningRateScheduler setting learning rate to 5.376035861664744e-07.\n",
      "1914/1914 [==============================] - 88s 46ms/step - loss: 0.4441 - auc: 0.8459 - val_loss: 0.4437 - val_auc: 0.8465\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 00027: LearningRateScheduler setting learning rate to 4.063225103165321e-07.\n",
      "1914/1914 [==============================] - 88s 46ms/step - loss: 0.4440 - auc: 0.8460 - val_loss: 0.4429 - val_auc: 0.8473\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 00028: LearningRateScheduler setting learning rate to 3.1442575722157244e-07.\n",
      "1914/1914 [==============================] - 89s 46ms/step - loss: 0.4443 - auc: 0.8456 - val_loss: 0.4446 - val_auc: 0.8464\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 00029: LearningRateScheduler setting learning rate to 2.500980300551007e-07.\n",
      "1914/1914 [==============================] - 88s 46ms/step - loss: 0.4438 - auc: 0.8462 - val_loss: 0.4436 - val_auc: 0.8465\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 00030: LearningRateScheduler setting learning rate to 2.050686210385705e-07.\n",
      "1914/1914 [==============================] - 87s 45ms/step - loss: 0.4445 - auc: 0.8458 - val_loss: 0.4446 - val_auc: 0.8463\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 00031: LearningRateScheduler setting learning rate to 1.7354803472699933e-07.\n",
      "1914/1914 [==============================] - 88s 46ms/step - loss: 0.4442 - auc: 0.8457 - val_loss: 0.4432 - val_auc: 0.8471\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 00032: LearningRateScheduler setting learning rate to 1.5148362430889952e-07.\n",
      "1914/1914 [==============================] - 93s 48ms/step - loss: 0.4442 - auc: 0.8458 - val_loss: 0.4434 - val_auc: 0.8466 loss: 0.4\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 00033: LearningRateScheduler setting learning rate to 1.3603853701622966e-07.\n",
      "1914/1914 [==============================] - 98s 51ms/step - loss: 0.4442 - auc: 0.8459 - val_loss: 0.4441 - val_auc: 0.8464\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 00034: LearningRateScheduler setting learning rate to 1.2522697591136076e-07.\n",
      "1914/1914 [==============================] - 97s 50ms/step - loss: 0.4441 - auc: 0.8462 - val_loss: 0.4436 - val_auc: 0.84640s - loss: 0.4441 - auc: \n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 00035: LearningRateScheduler setting learning rate to 1.1765888313795253e-07.\n",
      "1914/1914 [==============================] - 97s 51ms/step - loss: 0.4441 - auc: 0.8460 - val_loss: 0.4430 - val_auc: 0.8471\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, validation_data = val_ds, epochs = Params[\"epochs\"], shuffle=True,\n",
    "                    steps_per_epoch = steps_per_epoch, validation_steps=validation_steps,\n",
    "                    verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "historyFrame = pd.DataFrame(history.history)\n",
    "historyFrame[[\"auc\", \"val_auc\"]].plot()\n",
    "historyFrame[[\"loss\", \"val_loss\"]].plot()\n",
    "historyFrame.to_csv(f\"{MDL_PATH}/history_mdl{Params['version']:03}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>auc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_auc</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.600826</td>\n",
       "      <td>0.700335</td>\n",
       "      <td>0.506785</td>\n",
       "      <td>0.801284</td>\n",
       "      <td>1.000000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.499083</td>\n",
       "      <td>0.804990</td>\n",
       "      <td>0.481724</td>\n",
       "      <td>0.820202</td>\n",
       "      <td>1.000000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.485878</td>\n",
       "      <td>0.815358</td>\n",
       "      <td>0.474436</td>\n",
       "      <td>0.826956</td>\n",
       "      <td>1.000000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.479117</td>\n",
       "      <td>0.820763</td>\n",
       "      <td>0.466680</td>\n",
       "      <td>0.831916</td>\n",
       "      <td>1.000000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.478497</td>\n",
       "      <td>0.820899</td>\n",
       "      <td>0.467717</td>\n",
       "      <td>0.831349</td>\n",
       "      <td>1.946667e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.476270</td>\n",
       "      <td>0.822150</td>\n",
       "      <td>0.465529</td>\n",
       "      <td>0.831260</td>\n",
       "      <td>2.893333e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.475265</td>\n",
       "      <td>0.822704</td>\n",
       "      <td>0.473305</td>\n",
       "      <td>0.829616</td>\n",
       "      <td>3.840000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.465932</td>\n",
       "      <td>0.830063</td>\n",
       "      <td>0.456562</td>\n",
       "      <td>0.837276</td>\n",
       "      <td>2.688300e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.460545</td>\n",
       "      <td>0.833981</td>\n",
       "      <td>0.450134</td>\n",
       "      <td>0.841456</td>\n",
       "      <td>1.882110e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.456116</td>\n",
       "      <td>0.837108</td>\n",
       "      <td>0.449006</td>\n",
       "      <td>0.842029</td>\n",
       "      <td>1.317777e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.453224</td>\n",
       "      <td>0.839245</td>\n",
       "      <td>0.447184</td>\n",
       "      <td>0.844220</td>\n",
       "      <td>9.227439e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.450481</td>\n",
       "      <td>0.841333</td>\n",
       "      <td>0.446542</td>\n",
       "      <td>0.843957</td>\n",
       "      <td>6.462207e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.448149</td>\n",
       "      <td>0.843212</td>\n",
       "      <td>0.444896</td>\n",
       "      <td>0.844588</td>\n",
       "      <td>4.526545e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.447949</td>\n",
       "      <td>0.842991</td>\n",
       "      <td>0.443797</td>\n",
       "      <td>0.845905</td>\n",
       "      <td>3.171582e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.446308</td>\n",
       "      <td>0.844078</td>\n",
       "      <td>0.444397</td>\n",
       "      <td>0.845652</td>\n",
       "      <td>2.223107e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.445467</td>\n",
       "      <td>0.845141</td>\n",
       "      <td>0.444819</td>\n",
       "      <td>0.845174</td>\n",
       "      <td>1.559175e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.445917</td>\n",
       "      <td>0.844577</td>\n",
       "      <td>0.443221</td>\n",
       "      <td>0.845899</td>\n",
       "      <td>1.094422e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.444727</td>\n",
       "      <td>0.845522</td>\n",
       "      <td>0.444711</td>\n",
       "      <td>0.845832</td>\n",
       "      <td>7.690957e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.444608</td>\n",
       "      <td>0.845629</td>\n",
       "      <td>0.444223</td>\n",
       "      <td>0.846646</td>\n",
       "      <td>5.413670e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.444399</td>\n",
       "      <td>0.845575</td>\n",
       "      <td>0.444906</td>\n",
       "      <td>0.846932</td>\n",
       "      <td>3.819569e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.444387</td>\n",
       "      <td>0.845723</td>\n",
       "      <td>0.444515</td>\n",
       "      <td>0.846202</td>\n",
       "      <td>2.703698e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.444153</td>\n",
       "      <td>0.845754</td>\n",
       "      <td>0.443413</td>\n",
       "      <td>0.846927</td>\n",
       "      <td>1.922589e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.444245</td>\n",
       "      <td>0.845873</td>\n",
       "      <td>0.442987</td>\n",
       "      <td>0.846859</td>\n",
       "      <td>1.375812e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.444516</td>\n",
       "      <td>0.845603</td>\n",
       "      <td>0.443465</td>\n",
       "      <td>0.846813</td>\n",
       "      <td>9.930685e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.444001</td>\n",
       "      <td>0.845977</td>\n",
       "      <td>0.442552</td>\n",
       "      <td>0.847384</td>\n",
       "      <td>7.251480e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.444088</td>\n",
       "      <td>0.845903</td>\n",
       "      <td>0.443662</td>\n",
       "      <td>0.846509</td>\n",
       "      <td>5.376036e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.444049</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>0.442861</td>\n",
       "      <td>0.847270</td>\n",
       "      <td>4.063225e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.444312</td>\n",
       "      <td>0.845628</td>\n",
       "      <td>0.444593</td>\n",
       "      <td>0.846398</td>\n",
       "      <td>3.144258e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.443789</td>\n",
       "      <td>0.846158</td>\n",
       "      <td>0.443566</td>\n",
       "      <td>0.846516</td>\n",
       "      <td>2.500980e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.444495</td>\n",
       "      <td>0.845751</td>\n",
       "      <td>0.444589</td>\n",
       "      <td>0.846280</td>\n",
       "      <td>2.050686e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.444156</td>\n",
       "      <td>0.845733</td>\n",
       "      <td>0.443189</td>\n",
       "      <td>0.847073</td>\n",
       "      <td>1.735480e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.444249</td>\n",
       "      <td>0.845783</td>\n",
       "      <td>0.443414</td>\n",
       "      <td>0.846615</td>\n",
       "      <td>1.514836e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.444158</td>\n",
       "      <td>0.845928</td>\n",
       "      <td>0.444103</td>\n",
       "      <td>0.846414</td>\n",
       "      <td>1.360385e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.444084</td>\n",
       "      <td>0.846214</td>\n",
       "      <td>0.443627</td>\n",
       "      <td>0.846391</td>\n",
       "      <td>1.252270e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.444050</td>\n",
       "      <td>0.845974</td>\n",
       "      <td>0.443039</td>\n",
       "      <td>0.847125</td>\n",
       "      <td>1.176589e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss       auc  val_loss   val_auc            lr\n",
       "0   0.600826  0.700335  0.506785  0.801284  1.000000e-04\n",
       "1   0.499083  0.804990  0.481724  0.820202  1.000000e-04\n",
       "2   0.485878  0.815358  0.474436  0.826956  1.000000e-04\n",
       "3   0.479117  0.820763  0.466680  0.831916  1.000000e-04\n",
       "4   0.478497  0.820899  0.467717  0.831349  1.946667e-04\n",
       "5   0.476270  0.822150  0.465529  0.831260  2.893333e-04\n",
       "6   0.475265  0.822704  0.473305  0.829616  3.840000e-04\n",
       "7   0.465932  0.830063  0.456562  0.837276  2.688300e-04\n",
       "8   0.460545  0.833981  0.450134  0.841456  1.882110e-04\n",
       "9   0.456116  0.837108  0.449006  0.842029  1.317777e-04\n",
       "10  0.453224  0.839245  0.447184  0.844220  9.227439e-05\n",
       "11  0.450481  0.841333  0.446542  0.843957  6.462207e-05\n",
       "12  0.448149  0.843212  0.444896  0.844588  4.526545e-05\n",
       "13  0.447949  0.842991  0.443797  0.845905  3.171582e-05\n",
       "14  0.446308  0.844078  0.444397  0.845652  2.223107e-05\n",
       "15  0.445467  0.845141  0.444819  0.845174  1.559175e-05\n",
       "16  0.445917  0.844577  0.443221  0.845899  1.094422e-05\n",
       "17  0.444727  0.845522  0.444711  0.845832  7.690957e-06\n",
       "18  0.444608  0.845629  0.444223  0.846646  5.413670e-06\n",
       "19  0.444399  0.845575  0.444906  0.846932  3.819569e-06\n",
       "20  0.444387  0.845723  0.444515  0.846202  2.703698e-06\n",
       "21  0.444153  0.845754  0.443413  0.846927  1.922589e-06\n",
       "22  0.444245  0.845873  0.442987  0.846859  1.375812e-06\n",
       "23  0.444516  0.845603  0.443465  0.846813  9.930685e-07\n",
       "24  0.444001  0.845977  0.442552  0.847384  7.251480e-07\n",
       "25  0.444088  0.845903  0.443662  0.846509  5.376036e-07\n",
       "26  0.444049  0.846000  0.442861  0.847270  4.063225e-07\n",
       "27  0.444312  0.845628  0.444593  0.846398  3.144258e-07\n",
       "28  0.443789  0.846158  0.443566  0.846516  2.500980e-07\n",
       "29  0.444495  0.845751  0.444589  0.846280  2.050686e-07\n",
       "30  0.444156  0.845733  0.443189  0.847073  1.735480e-07\n",
       "31  0.444249  0.845783  0.443414  0.846615  1.514836e-07\n",
       "32  0.444158  0.845928  0.444103  0.846414  1.360385e-07\n",
       "33  0.444084  0.846214  0.443627  0.846391  1.252270e-07\n",
       "34  0.444050  0.845974  0.443039  0.847125  1.176589e-07"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historyFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best epoch: 24 | best loss: 0.44400131702423096 | best auc: 0.8473840951919556\n"
     ]
    }
   ],
   "source": [
    "best_epoch = historyFrame.val_auc.argmax()\n",
    "best_loss = historyFrame.iloc[best_epoch].loss\n",
    "best_auc = historyFrame.iloc[best_epoch].val_auc\n",
    "print(\"best epoch:\", best_epoch,\n",
    "      \"| best loss:\", best_loss,\n",
    "      \"| best auc:\", best_auc\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Params.copy()\n",
    "result[\"bavg_epoch\"] = int(best_epoch)\n",
    "result[\"bavg_loss\"] = float(best_loss)\n",
    "result[\"bavg_auc\"] = float(best_auc)\n",
    "with open(f\"{MDL_PATH}/params.json\", \"w\") as file:\n",
    "    json.dump(result, file)\n",
    "\n",
    "if not os.path.exists(f\"{MDLS_PATH}/results.csv\"):\n",
    "    df_save = pd.DataFrame(result, index=[0])\n",
    "    df_save.to_csv(f\"{MDLS_PATH}/results.csv\")\n",
    "else:\n",
    "    df_old = pd.read_csv(f\"{MDLS_PATH}/results.csv\", index_col=0)\n",
    "    df_save = pd.DataFrame(result, index = [df_old.index.max() + 1])\n",
    "    df_save = df_old.append(df_save, ignore_index=True)\n",
    "    df_save.to_csv(f\"{MDLS_PATH}/results.csv\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>version</th>\n",
       "      <th>train_mode</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>bavg_epoch</th>\n",
       "      <th>bavg_loss</th>\n",
       "      <th>bavg_auc</th>\n",
       "      <th>changelog</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>1</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>16</td>\n",
       "      <td>0.451789</td>\n",
       "      <td>0.838065</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>13</td>\n",
       "      <td>0.449180</td>\n",
       "      <td>0.822957</td>\n",
       "      <td>moved all relu layers before the pooling layers</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>3</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>20</td>\n",
       "      <td>0.442866</td>\n",
       "      <td>0.837724</td>\n",
       "      <td>tried on large ds, since hight overfitting</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>5</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>13</td>\n",
       "      <td>0.449431</td>\n",
       "      <td>0.822973</td>\n",
       "      <td>added second layer to first block or reference</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>6</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>0.442229</td>\n",
       "      <td>0.820143</td>\n",
       "      <td>added relu activations to all conv layers</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>9</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>0.480765</td>\n",
       "      <td>0.812309</td>\n",
       "      <td>set all pool sizes to 1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>11</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>0.438630</td>\n",
       "      <td>0.819107</td>\n",
       "      <td>set all pool sizes to 8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>13</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>0.448575</td>\n",
       "      <td>0.820421</td>\n",
       "      <td>added second layer to first block with small k...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>14</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>0.457616</td>\n",
       "      <td>0.820876</td>\n",
       "      <td>added second layer to first block with small k...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>17</td>\n",
       "      <td>test</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>0.475783</td>\n",
       "      <td>0.814108</td>\n",
       "      <td>trial of completely different achitecture</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>33</td>\n",
       "      <td>full</td>\n",
       "      <td>64</td>\n",
       "      <td>60</td>\n",
       "      <td>13</td>\n",
       "      <td>0.457444</td>\n",
       "      <td>0.825847</td>\n",
       "      <td>trial of completely different achitecture</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00500</td>\n",
       "      <td>43</td>\n",
       "      <td>full</td>\n",
       "      <td>64</td>\n",
       "      <td>60</td>\n",
       "      <td>43</td>\n",
       "      <td>0.452197</td>\n",
       "      <td>0.836913</td>\n",
       "      <td>trial of completely different achitecture</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>47</td>\n",
       "      <td>full</td>\n",
       "      <td>64</td>\n",
       "      <td>60</td>\n",
       "      <td>9</td>\n",
       "      <td>0.452419</td>\n",
       "      <td>0.836464</td>\n",
       "      <td>model before was standard model with 0.001 no ...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>55</td>\n",
       "      <td>full</td>\n",
       "      <td>64</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>0.451851</td>\n",
       "      <td>0.833302</td>\n",
       "      <td>removed dropout layer in conv</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>57</td>\n",
       "      <td>full</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>0.434063</td>\n",
       "      <td>0.835922</td>\n",
       "      <td>before: removed 2 fully connected layers. now:...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>61</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.483809</td>\n",
       "      <td>0.824025</td>\n",
       "      <td>removed dropout, flatten, added lstm</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>64</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.465674</td>\n",
       "      <td>0.824023</td>\n",
       "      <td>removed 2 layers in larger block</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>66</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.460225</td>\n",
       "      <td>0.827190</td>\n",
       "      <td>added back one layer</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>67</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.454083</td>\n",
       "      <td>0.827726</td>\n",
       "      <td>third layer</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>68</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.455897</td>\n",
       "      <td>0.827700</td>\n",
       "      <td>doubled the first layer and added a second one...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>70</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.467193</td>\n",
       "      <td>0.832003</td>\n",
       "      <td>base model</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>72</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.483452</td>\n",
       "      <td>0.824098</td>\n",
       "      <td>took out the fully connected layers before the...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>73</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.472798</td>\n",
       "      <td>0.830427</td>\n",
       "      <td>added back one fully connected layer</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>75</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.454001</td>\n",
       "      <td>0.830381</td>\n",
       "      <td>doubled pool size, except for last, doubled nu...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>76</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.469206</td>\n",
       "      <td>0.832270</td>\n",
       "      <td>base model with same padding</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>78</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "      <td>0.442443</td>\n",
       "      <td>0.841306</td>\n",
       "      <td>doubled last layer in all blocks. lr/10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>79</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>25</td>\n",
       "      <td>0.439258</td>\n",
       "      <td>0.842578</td>\n",
       "      <td>doubled last layer in all blocks doubled size ...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>83</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>27</td>\n",
       "      <td>0.439051</td>\n",
       "      <td>0.841536</td>\n",
       "      <td>added dropout layers to dense layers</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>88</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.453092</td>\n",
       "      <td>0.834111</td>\n",
       "      <td>everything had 3 layers now and relu activation</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>92</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>17</td>\n",
       "      <td>0.434836</td>\n",
       "      <td>0.832301</td>\n",
       "      <td>dropout layers</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>110</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>11</td>\n",
       "      <td>0.470093</td>\n",
       "      <td>0.815836</td>\n",
       "      <td>dropout layers</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>113</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "      <td>0.476270</td>\n",
       "      <td>0.813889</td>\n",
       "      <td>dropout layers</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>119</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>36</td>\n",
       "      <td>0.463551</td>\n",
       "      <td>0.812370</td>\n",
       "      <td>new dataset using best model (base model with ...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.00020</td>\n",
       "      <td>124</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>23</td>\n",
       "      <td>0.448196</td>\n",
       "      <td>0.827187</td>\n",
       "      <td>10x lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>134</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>12</td>\n",
       "      <td>0.467185</td>\n",
       "      <td>0.826250</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>135</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>9</td>\n",
       "      <td>0.444164</td>\n",
       "      <td>0.848162</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>136</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>21</td>\n",
       "      <td>0.420500</td>\n",
       "      <td>0.854332</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>137</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>31</td>\n",
       "      <td>0.364450</td>\n",
       "      <td>0.876402</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>139</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.436394</td>\n",
       "      <td>0.845982</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>139</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.436394</td>\n",
       "      <td>0.845982</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>140</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>0.423539</td>\n",
       "      <td>0.844709</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>142</td>\n",
       "      <td>full</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.447673</td>\n",
       "      <td>0.836974</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>164</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>9</td>\n",
       "      <td>0.438891</td>\n",
       "      <td>0.844589</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>200</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>36</td>\n",
       "      <td>0.464103</td>\n",
       "      <td>0.839057</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>210</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>24</td>\n",
       "      <td>0.444001</td>\n",
       "      <td>0.847384</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lr  version train_mode  batch_size  epochs  bavg_epoch  bavg_loss  \\\n",
       "0   0.00100        1       full         256      60          16   0.451789   \n",
       "1   0.00100        2       test         256      60          13   0.449180   \n",
       "2   0.00100        3       full         256      60          20   0.442866   \n",
       "3   0.00100        5       test         256      60          13   0.449431   \n",
       "4   0.00100        6       test         256      30           9   0.442229   \n",
       "5   0.00100        9       test         256      30           4   0.480765   \n",
       "6   0.00100       11       test         256      30           8   0.438630   \n",
       "7   0.00100       13       test         256      30          12   0.448575   \n",
       "8   0.00010       14       test         256      30          12   0.457616   \n",
       "9   0.00010       17       test          64      30           6   0.475783   \n",
       "10  0.00010       33       full          64      60          13   0.457444   \n",
       "11  0.00500       43       full          64      60          43   0.452197   \n",
       "12  0.00010       47       full          64      60           9   0.452419   \n",
       "13  0.00010       55       full          64      60           4   0.451851   \n",
       "14  0.00010       57       full         128      20           9   0.434063   \n",
       "15  0.00010       61       full         256       3           2   0.483809   \n",
       "16  0.00010       64       full         256      10           7   0.465674   \n",
       "17  0.00010       66       full         256      10           6   0.460225   \n",
       "18  0.00010       67       full         256      10           7   0.454083   \n",
       "19  0.00010       68       full         256      10           7   0.455897   \n",
       "20  0.00100       70       full         256      10           9   0.467193   \n",
       "21  0.00100       72       full         256      10           9   0.483452   \n",
       "22  0.00100       73       full         256      10           9   0.472798   \n",
       "23  0.00010       75       full         256      10           9   0.454001   \n",
       "24  0.00100       76       full         256      10           6   0.469206   \n",
       "25  0.00010       78       full         256      40          12   0.442443   \n",
       "26  0.00050       79       full         256      40          25   0.439258   \n",
       "27  0.00050       83       full         256      40          27   0.439051   \n",
       "28  0.00010       88       full         256      40           8   0.453092   \n",
       "29  0.00010       92       full         256     100          17   0.434836   \n",
       "30  0.00010      110       full         256     100          11   0.470093   \n",
       "31  0.00010      113       full         256     100           8   0.476270   \n",
       "32  0.00002      119       full         256     100          36   0.463551   \n",
       "33  0.00020      124       full         256     100          23   0.448196   \n",
       "34  0.00010      134       full         256     100          12   0.467185   \n",
       "35  0.00010      135       full         256     100           9   0.444164   \n",
       "36  0.00010      136       full         256     100          21   0.420500   \n",
       "37  0.00010      137       full         256     100          31   0.364450   \n",
       "38  0.00010      139       full         256     100          10   0.436394   \n",
       "39  0.00010      139       full         256     100          10   0.436394   \n",
       "40  0.00010      140       full         256      15          12   0.423539   \n",
       "41  0.00010      142       full         128       5           4   0.447673   \n",
       "42  0.00010      164       full         256     100           9   0.438891   \n",
       "43  0.00001      200       full         256     100          36   0.464103   \n",
       "44  0.00100      210       full         256     100          24   0.444001   \n",
       "\n",
       "    bavg_auc                                          changelog  seed  \n",
       "0   0.838065                                                NaN   NaN  \n",
       "1   0.822957    moved all relu layers before the pooling layers   NaN  \n",
       "2   0.837724         tried on large ds, since hight overfitting   NaN  \n",
       "3   0.822973     added second layer to first block or reference   NaN  \n",
       "4   0.820143          added relu activations to all conv layers   NaN  \n",
       "5   0.812309                            set all pool sizes to 1   NaN  \n",
       "6   0.819107                            set all pool sizes to 8   NaN  \n",
       "7   0.820421  added second layer to first block with small k...   NaN  \n",
       "8   0.820876  added second layer to first block with small k...   NaN  \n",
       "9   0.814108          trial of completely different achitecture   NaN  \n",
       "10  0.825847          trial of completely different achitecture   NaN  \n",
       "11  0.836913          trial of completely different achitecture   NaN  \n",
       "12  0.836464  model before was standard model with 0.001 no ...  42.0  \n",
       "13  0.833302                      removed dropout layer in conv  42.0  \n",
       "14  0.835922  before: removed 2 fully connected layers. now:...  42.0  \n",
       "15  0.824025               removed dropout, flatten, added lstm  42.0  \n",
       "16  0.824023                   removed 2 layers in larger block  42.0  \n",
       "17  0.827190                               added back one layer  42.0  \n",
       "18  0.827726                                        third layer  42.0  \n",
       "19  0.827700  doubled the first layer and added a second one...  42.0  \n",
       "20  0.832003                                         base model  42.0  \n",
       "21  0.824098  took out the fully connected layers before the...  42.0  \n",
       "22  0.830427               added back one fully connected layer  42.0  \n",
       "23  0.830381  doubled pool size, except for last, doubled nu...  42.0  \n",
       "24  0.832270                       base model with same padding  42.0  \n",
       "25  0.841306            doubled last layer in all blocks. lr/10  42.0  \n",
       "26  0.842578  doubled last layer in all blocks doubled size ...  42.0  \n",
       "27  0.841536               added dropout layers to dense layers  42.0  \n",
       "28  0.834111    everything had 3 layers now and relu activation  42.0  \n",
       "29  0.832301                                     dropout layers  42.0  \n",
       "30  0.815836                                     dropout layers  42.0  \n",
       "31  0.813889                                     dropout layers  42.0  \n",
       "32  0.812370  new dataset using best model (base model with ...  42.0  \n",
       "33  0.827187                                             10x lr  42.0  \n",
       "34  0.826250                                             1/4 lr  42.0  \n",
       "35  0.848162                                             1/4 lr  42.0  \n",
       "36  0.854332                                             1/4 lr  42.0  \n",
       "37  0.876402                                             1/4 lr  42.0  \n",
       "38  0.845982                                             1/4 lr  42.0  \n",
       "39  0.845982                                             1/4 lr  42.0  \n",
       "40  0.844709                                             1/4 lr  42.0  \n",
       "41  0.836974                                             1/4 lr  42.0  \n",
       "42  0.844589                                             1/4 lr  42.0  \n",
       "43  0.839057                                             1/4 lr  42.0  \n",
       "44  0.847384                                             1/4 lr  42.0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(f\"{MDLS_PATH}/results.csv\",index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7690"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_ds, val_ds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['../input/filtered-whitened-tfrec\\\\train_00.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_01.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_02.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_03.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_04.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_05.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_06.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_07.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_08.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_09.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_10.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_11.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_12.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_13.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_14.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_15.tfrec'], dtype='<U47')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8516381439735574]\n"
     ]
    }
   ],
   "source": [
    "prediction_scores = []\n",
    "with strategy.scope():\n",
    "    model = tf.keras.models.load_model(f\"{MDL_PATH}/model_{Params['version']:03}.h5\")\n",
    "train_df = pd.read_csv(\"../input/g2net-gravitational-wave-detection/training_labels.csv\")\n",
    "for ds_ind in range(len(all_train_files)):\n",
    "    train_set = load_dataset(all_train_files[ds_ind], shuffle=False, ordered=True, labeled=True, repeat=False, return_labels=False)\n",
    "    prediction = model.predict(train_set)\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    prediction_scores.append(roc_auc_score(train_df.target, prediction))\n",
    "print(prediction_scores)\n",
    "best_pred = np.array(prediction_scores).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = load_dataset(test_files[best_pred], shuffle=False, ordered=True, labeled=False, repeat=False, return_labels=False)\n",
    "test_prediction = model.predict(test_set)\n",
    "sub = pd.read_csv(\"../input/g2net-gravitational-wave-detection/sample_submission.csv\")\n",
    "sub.target = test_prediction.flatten()\n",
    "sub.to_csv(f\"{MDL_PATH}/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
