{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:41.713595Z",
     "iopub.status.busy": "2021-09-17T17:22:41.713335Z",
     "iopub.status.idle": "2021-09-17T17:22:41.725791Z",
     "shell.execute_reply": "2021-09-17T17:22:41.725140Z",
     "shell.execute_reply.started": "2021-09-17T17:22:41.713521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KAGGLE: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "KAGGLE = True\n",
    "if os.name == \"nt\":\n",
    "    KAGGLE = False\n",
    "print(f\"KAGGLE: {KAGGLE}\")\n",
    "if not KAGGLE:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:41.727703Z",
     "iopub.status.busy": "2021-09-17T17:22:41.726861Z",
     "iopub.status.idle": "2021-09-17T17:22:43.782183Z",
     "shell.execute_reply": "2021-09-17T17:22:43.781042Z",
     "shell.execute_reply.started": "2021-09-17T17:22:41.727659Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apist\\anaconda3\\envs\\g2net-tf2\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:43: UserWarning: You are currently using a nightly version of TensorFlow (2.8.0-dev20210928). \n",
      "TensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not. \n",
      "If you encounter a bug, do not file an issue on GitHub.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "# general\n",
    "import gc\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "from glob import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from IPython.core.display import display, HTML\n",
    "if KAGGLE:\n",
    "    from kaggle_datasets import KaggleDatasets\n",
    "\n",
    "# ML\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# DL\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "if not KAGGLE:\n",
    "    physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:43.784172Z",
     "iopub.status.busy": "2021-09-17T17:22:43.783660Z",
     "iopub.status.idle": "2021-09-17T17:22:43.799591Z",
     "shell.execute_reply": "2021-09-17T17:22:43.798788Z",
     "shell.execute_reply.started": "2021-09-17T17:22:43.784124Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:43.803149Z",
     "iopub.status.busy": "2021-09-17T17:22:43.802737Z",
     "iopub.status.idle": "2021-09-17T17:22:43.811423Z",
     "shell.execute_reply": "2021-09-17T17:22:43.810381Z",
     "shell.execute_reply.started": "2021-09-17T17:22:43.803115Z"
    }
   },
   "outputs": [],
   "source": [
    "def auto_select_accelerator():\n",
    "    TPU_DETECTED = False\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "        tf.keras.mixed_precision.set_global_policy(\"mixed_bfloat16\")\n",
    "        print(f\"Running on TPU: {tpu.master()}\")\n",
    "        TPU_DETECTED = True\n",
    "    except ValueError:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "        tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    num_replicas = strategy.num_replicas_in_sync\n",
    "    print(f\"Running on {num_replicas} replica{'s' if num_replicas > 1 else ''}\")\n",
    "    return strategy, TPU_DETECTED, num_replicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:43.813082Z",
     "iopub.status.busy": "2021-09-17T17:22:43.812779Z",
     "iopub.status.idle": "2021-09-17T17:22:49.385095Z",
     "shell.execute_reply": "2021-09-17T17:22:49.384154Z",
     "shell.execute_reply.started": "2021-09-17T17:22:43.813043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "Running on 1 replica\n"
     ]
    }
   ],
   "source": [
    "strategy, TPU_Detected, REPLICAS = auto_select_accelerator()\n",
    "INPUT_DIR = \"../input/g2net-gravitational-wave-detection\"\n",
    "MDLS_PATH = \".\" if KAGGLE else \"../models\"\n",
    "# TRAIN_FILES_PATH = \"../input/filtered*_tfrec\"\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "tfrec_folders = [\"like-synthetic-whitened-highpassed-tfrec\"]#, \"like-synthetic-whitened-highpassed-\"]#[\"whitened-longer-tfrec\", \"whitened-tfrec\", \"filtered-whitened-tfrec\"]\n",
    "if KAGGLE:\n",
    "    tfrec_folders = [\"filteredwhitenedtfrec\",\"whitened-tfrec\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:10:49.388016Z",
     "iopub.status.busy": "2021-09-17T17:10:49.387794Z",
     "iopub.status.idle": "2021-09-17T17:10:49.406903Z",
     "shell.execute_reply": "2021-09-17T17:10:49.405619Z",
     "shell.execute_reply.started": "2021-09-17T17:10:49.387988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>version</th>\n",
       "      <th>train_mode</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>bavg_epoch</th>\n",
       "      <th>bavg_loss</th>\n",
       "      <th>bavg_auc</th>\n",
       "      <th>changelog</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>136</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>21</td>\n",
       "      <td>0.420500</td>\n",
       "      <td>0.854332</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>137</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>31</td>\n",
       "      <td>0.364450</td>\n",
       "      <td>0.876402</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>139</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.436394</td>\n",
       "      <td>0.845982</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>139</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.436394</td>\n",
       "      <td>0.845982</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>140</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>0.423539</td>\n",
       "      <td>0.844709</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>142</td>\n",
       "      <td>full</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.447673</td>\n",
       "      <td>0.836974</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>164</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>9</td>\n",
       "      <td>0.438891</td>\n",
       "      <td>0.844589</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>200</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>36</td>\n",
       "      <td>0.464103</td>\n",
       "      <td>0.839057</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>210</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>24</td>\n",
       "      <td>0.444001</td>\n",
       "      <td>0.847384</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>221</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "      <td>0.435199</td>\n",
       "      <td>0.850387</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>222</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>26</td>\n",
       "      <td>0.437144</td>\n",
       "      <td>0.849532</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>224</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>30</td>\n",
       "      <td>0.434321</td>\n",
       "      <td>0.851999</td>\n",
       "      <td>Input layers: 1/2 kernel size. more kernels</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>231</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>29</td>\n",
       "      <td>0.439757</td>\n",
       "      <td>0.848020</td>\n",
       "      <td>only whitened ds</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>242</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>0.447073</td>\n",
       "      <td>0.848576</td>\n",
       "      <td>only whitened ds with sgd</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>244</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>0.465401</td>\n",
       "      <td>0.834070</td>\n",
       "      <td>only whitened ds with sgd</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>245</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.842836</td>\n",
       "      <td>all ds</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>247</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>0.444699</td>\n",
       "      <td>0.842759</td>\n",
       "      <td>second run with adam</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>307</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0.490005</td>\n",
       "      <td>0.852317</td>\n",
       "      <td>second run with adam</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>346</td>\n",
       "      <td>full</td>\n",
       "      <td>64</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0.403609</td>\n",
       "      <td>0.839891</td>\n",
       "      <td>second run with adam</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>355</td>\n",
       "      <td>full</td>\n",
       "      <td>64</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>0.431249</td>\n",
       "      <td>0.840779</td>\n",
       "      <td>second run with adam</td>\n",
       "      <td>69420.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lr  version train_mode  batch_size  epochs  bavg_epoch  bavg_loss  \\\n",
       "36  0.00010      136       full         256     100          21   0.420500   \n",
       "37  0.00010      137       full         256     100          31   0.364450   \n",
       "38  0.00010      139       full         256     100          10   0.436394   \n",
       "39  0.00010      139       full         256     100          10   0.436394   \n",
       "40  0.00010      140       full         256      15          12   0.423539   \n",
       "41  0.00010      142       full         128       5           4   0.447673   \n",
       "42  0.00010      164       full         256     100           9   0.438891   \n",
       "43  0.00001      200       full         256     100          36   0.464103   \n",
       "44  0.00100      210       full         256     100          24   0.444001   \n",
       "45  0.00010      221       full         256     100          40   0.435199   \n",
       "46  0.00010      222       full         256     100          26   0.437144   \n",
       "47  0.00010      224       full         256     100          30   0.434321   \n",
       "48  0.00010      231       full         256     100          29   0.439757   \n",
       "49  0.00010      242       full         256      25          23   0.447073   \n",
       "50  0.00010      244       full         256      25          21   0.465401   \n",
       "51  0.00010      245       full         256      25          23   0.446809   \n",
       "52  0.00010      247       full         256      35           9   0.444699   \n",
       "53  0.00010      307       full         256      35           1   0.490005   \n",
       "54  0.00010      346       full          64      35           0   0.403609   \n",
       "55  0.00001      355       full          64      35           6   0.431249   \n",
       "\n",
       "    bavg_auc                                    changelog     seed  \n",
       "36  0.854332                                       1/4 lr     42.0  \n",
       "37  0.876402                                       1/4 lr     42.0  \n",
       "38  0.845982                                       1/4 lr     42.0  \n",
       "39  0.845982                                       1/4 lr     42.0  \n",
       "40  0.844709                                       1/4 lr     42.0  \n",
       "41  0.836974                                       1/4 lr     42.0  \n",
       "42  0.844589                                       1/4 lr     42.0  \n",
       "43  0.839057                                       1/4 lr     42.0  \n",
       "44  0.847384                                       1/4 lr     42.0  \n",
       "45  0.850387                                       1/4 lr     69.0  \n",
       "46  0.849532                                       1/4 lr     69.0  \n",
       "47  0.851999  Input layers: 1/2 kernel size. more kernels     69.0  \n",
       "48  0.848020                             only whitened ds     69.0  \n",
       "49  0.848576                    only whitened ds with sgd     42.0  \n",
       "50  0.834070                    only whitened ds with sgd     42.0  \n",
       "51  0.842836                                       all ds     42.0  \n",
       "52  0.842759                         second run with adam     42.0  \n",
       "53  0.852317                         second run with adam     42.0  \n",
       "54  0.839891                         second run with adam     42.0  \n",
       "55  0.840779                         second run with adam  69420.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = \"no file\"\n",
    "if not KAGGLE:\n",
    "    results_df = pd.read_csv(\"../models/results.csv\", index_col=[0]).tail(20)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:49.386775Z",
     "iopub.status.busy": "2021-09-17T17:22:49.386544Z",
     "iopub.status.idle": "2021-09-17T17:22:49.599221Z",
     "shell.execute_reply": "2021-09-17T17:22:49.598282Z",
     "shell.execute_reply.started": "2021-09-17T17:22:49.386749Z"
    }
   },
   "outputs": [],
   "source": [
    "if KAGGLE:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    user_credential = user_secrets.get_gcloud_credential()\n",
    "    user_secrets.set_tensorflow_credential(user_credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:49.601675Z",
     "iopub.status.busy": "2021-09-17T17:22:49.601417Z",
     "iopub.status.idle": "2021-09-17T17:22:49.608611Z",
     "shell.execute_reply": "2021-09-17T17:22:49.607652Z",
     "shell.execute_reply.started": "2021-09-17T17:22:49.601646Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{MDLS_PATH}/results.csv\"):\n",
    "    VER = 1\n",
    "else:\n",
    "    results = pd.read_csv(f\"{MDLS_PATH}/results.csv\", index_col=[0])\n",
    "    VER = int(results.version.max())\n",
    "Params ={\n",
    "    \"lr\": 1e-4 * REPLICAS,\n",
    "    \"version\": VER,\n",
    "    \"train_mode\": \"full\", #test, full\n",
    "    \"batch_size\": 128 * REPLICAS,\n",
    "    \"epochs\":35,\n",
    "    \"seed\": 69420,\n",
    "    \"changelog\": \"second run with adam\",\n",
    "}\n",
    "seed_everything(Params[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:49.609927Z",
     "iopub.status.busy": "2021-09-17T17:22:49.609716Z",
     "iopub.status.idle": "2021-09-17T17:22:49.626003Z",
     "shell.execute_reply": "2021-09-17T17:22:49.624837Z",
     "shell.execute_reply.started": "2021-09-17T17:22:49.609902Z"
    }
   },
   "outputs": [],
   "source": [
    "VER = Params[\"version\"]\n",
    "MDL_PATH = f\"{MDLS_PATH}/models_v{VER:03}\"\n",
    "while os.path.exists(MDL_PATH):\n",
    "    VER += 1\n",
    "    MDL_PATH = f\"{MDLS_PATH}/models_v{VER:03}\"\n",
    "Params[\"version\"]=VER\n",
    "os.mkdir(MDL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_cut(x,y):\n",
    "    tensor = x\n",
    "    if random.random() > 0.65:\n",
    "        maxVal=128\n",
    "        dt = tf.random.uniform(shape=[],minval=2, maxval=maxVal, dtype=tf.int32)\n",
    "        t0 = tf.random.uniform(shape=[],minval=1, maxval=dt, dtype=tf.int32)\n",
    "        t1 = tf.random.uniform(shape=[],minval=0, maxval=t0, dtype=tf.int32)\n",
    "        paddings =  [\n",
    "            [0,0],\n",
    "            [t0,dt-t0],\n",
    "            [0,0]\n",
    "        ]\n",
    "        tensor = tf.pad(tensor[:,t0:t0+(4096-dt)], paddings=paddings)\n",
    "    tensor = tensor * [-1. if random.random() > 0.5 else 1.,\n",
    "                       -1. if random.random() > 0.5 else 1.,\n",
    "                       -1. if random.random() > 0.5 else 1.]\n",
    "    \n",
    "#     paddings = [\n",
    "#         [0,0],\n",
    "#         [256,256],\n",
    "#         [0,0]\n",
    "#     ]\n",
    "#     tensor = tf.pad(tensor[:,256:-256], paddings=paddings)\n",
    "    tensor = tf.reshape(tensor,[Params[\"batch_size\"], 4096, 3])\n",
    "    \n",
    "    tensor = tf.cast(tensor, tf.float32)\n",
    "    return tensor, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:49.628201Z",
     "iopub.status.busy": "2021-09-17T17:22:49.627767Z",
     "iopub.status.idle": "2021-09-17T17:22:49.642197Z",
     "shell.execute_reply": "2021-09-17T17:22:49.641358Z",
     "shell.execute_reply.started": "2021-09-17T17:22:49.628155Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset(files, shuffle=True, ordered=False, labeled = True, repeat=True, return_labels = False, cut = False, cache=False):\n",
    "    if ordered:\n",
    "        dataset = tf.data.TFRecordDataset(files, num_parallel_reads=None)\n",
    "    else:\n",
    "        dataset = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n",
    "    \n",
    "\n",
    "    def _parse_function(example_proto):\n",
    "        if labeled:\n",
    "            keys_to_feature = {\n",
    "                \"TimeSeries\":tf.io.FixedLenFeature([4096,3],tf.float32),\n",
    "                \"Target\":tf.io.FixedLenFeature([], tf.int64, default_value=0)}\n",
    "            if return_labels:\n",
    "                keys_to_feature[\"id\"]=tf.io.FixedLenFeature([],tf.string, default_value=\"\")\n",
    "        else:\n",
    "            keys_to_feature = {\n",
    "                \"TimeSeries\": tf.io.FixedLenFeature([4096,3],tf.float32)\n",
    "            }\n",
    "        parsed_features = tf.io.parse_single_example(example_proto, keys_to_feature)\n",
    "        if labeled:\n",
    "            if return_labels:\n",
    "                return parsed_features[\"TimeSeries\"], parsed_features[\"Target\"], parsed_features[\"id\"]\n",
    "            else:\n",
    "                return parsed_features[\"TimeSeries\"], parsed_features[\"Target\"]\n",
    "        else:\n",
    "            return parsed_features[\"TimeSeries\"]\n",
    "    if not ordered:\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic=False\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "    # parse the record into tensors.\n",
    "    dataset = dataset.map(_parse_function, num_parallel_calls=AUTO)\n",
    "#     dataset = dataset.cache()\n",
    "\n",
    "    # Repeat the input infinitely\n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "\n",
    "    # shuffle the dataset\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=50000, reshuffle_each_iteration=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # Generate batches\n",
    "    dataset = dataset.batch(Params[\"batch_size\"])\n",
    "    if cut:\n",
    "        dataset = dataset.map(random_cut, num_parallel_calls=AUTO)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:49.644543Z",
     "iopub.status.busy": "2021-09-17T17:22:49.644208Z",
     "iopub.status.idle": "2021-09-17T17:22:53.400603Z",
     "shell.execute_reply": "2021-09-17T17:22:53.399615Z",
     "shell.execute_reply.started": "2021-09-17T17:22:49.644502Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_val_files(folders):\n",
    "    train_files = []\n",
    "    val_files = []\n",
    "    test_files = []\n",
    "    all_train_files = []\n",
    "    for folder in folders:\n",
    "        if KAGGLE:\n",
    "            TRAIN_FILES_PATH = KaggleDatasets().get_gcs_path(folder)\n",
    "            TEST_FILES_PATH = KaggleDatasets().get_gcs_path(f\"{folder}test\")\n",
    "            all_files_train = np.sort(tf.io.gfile.glob(f\"{TRAIN_FILES_PATH}/train_*.tfrec\"))\n",
    "            all_files_test = np.sort(tf.io.gfile.glob(f\"{TEST_FILES_PATH}/test_*.tfrec\"))\n",
    "        else:\n",
    "            all_files_train = np.sort(glob(f\"../input/{folder}/train*.tfrec\"))\n",
    "            all_files_test = np.sort(glob(f\"../input/{folder}/test*.tfrec\"))\n",
    "        \n",
    "        if Params[\"train_mode\"] == \"test\":\n",
    "            train_files.extend(all_files_train[:1])\n",
    "            val_files.extend(all_files_train[-1:])\n",
    "            \n",
    "        else:\n",
    "            train_files.extend(all_files_train[:-1])\n",
    "            val_files.extend(all_files_train[-1:])\n",
    "        test_files.append(all_files_test)\n",
    "        all_train_files.append(all_files_train)\n",
    "    return train_files, val_files, test_files, all_train_files\n",
    "\n",
    "train_files, val_files, test_files, all_train_files = get_train_val_files(tfrec_folders)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import time\n",
    "def benchmark(dataset, num_epochs=2):\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        for sample in dataset:\n",
    "            # Performing a training step\n",
    "            time.sleep(0.01)\n",
    "    print(\"Execution time:\", time.perf_counter() - start_time)\n",
    "benchmark(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_files = [\"../input/synthetic-tfrec/train_250_Mpc.tfrec\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:53.402399Z",
     "iopub.status.busy": "2021-09-17T17:22:53.402073Z",
     "iopub.status.idle": "2021-09-17T17:22:53.738249Z",
     "shell.execute_reply": "2021-09-17T17:22:53.737171Z",
     "shell.execute_reply.started": "2021-09-17T17:22:53.402358Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apist\\anaconda3\\envs\\g2net-tf2\\lib\\site-packages\\ipykernel_launcher.py:49: UserWarning: Seed 69420 from outer graph might be getting used by function Dataset_map_random_cut, if the random op has not been provided any seed. Explicitly set the seed in the function if this is not the intended behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = load_dataset(train_files, cut=True)\n",
    "val_ds = load_dataset(val_files, shuffle=False, cache=False, ordered=True, repeat=False)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for x in train_ds:\n",
    "    tens = x\n",
    "    break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-15T15:02:39.670012Z",
     "iopub.status.busy": "2021-09-15T15:02:39.669804Z",
     "iopub.status.idle": "2021-09-15T15:02:40.728147Z",
     "shell.execute_reply": "2021-09-15T15:02:40.727152Z",
     "shell.execute_reply.started": "2021-09-15T15:02:39.669988Z"
    }
   },
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "with strategy.scope():\n",
    "    model = Sequential([\n",
    "        layers.Conv1D(filters=32, kernel_size=16, padding=\"causal\",input_shape=[4096,3]),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=64, kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=128, kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=256, kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1),\n",
    "        layers.Activation(\"sigmoid\", dtype=\"float32\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        tf.keras.optimizers.Adam(learning_rate = Params[\"lr\"]),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"AUC\"]\n",
    "    )\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 560000 // 16 * len(train_files) // Params[\"batch_size\"]\n",
    "validation_steps = 560000 // 16 * len(val_files) // Params[\"batch_size\"]\n",
    "# steps_per_epoch = (560000 // 16 * 15 + 12500 * 82) // Params[\"batch_size\"]\n",
    "# validation_steps = (560000 // 16 * 1 + 12500 * 1) // Params[\"batch_size\"]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:54.953060Z",
     "iopub.status.busy": "2021-09-17T17:22:54.952812Z",
     "iopub.status.idle": "2021-09-17T17:22:54.957958Z",
     "shell.execute_reply": "2021-09-17T17:22:54.957019Z",
     "shell.execute_reply.started": "2021-09-17T17:22:54.953033Z"
    }
   },
   "source": [
    "#best\n",
    "from tensorflow.keras import Sequential, layers\n",
    "with strategy.scope():\n",
    "    model = Sequential([\n",
    "        layers.Conv1D(filters=256, activation=\"relu\", kernel_size=3, padding=\"causal\",input_shape=[4096,3]),\n",
    "        layers.Conv1D(filters=256, activation=\"relu\", kernel_size=6, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=256, activation=\"relu\", kernel_size=6, padding=\"causal\"),\n",
    "        \n",
    "        layers.MaxPool1D(pool_size=2),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=64, activation=\"relu\", kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, activation=\"relu\", kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, activation=\"relu\", kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=128, activation=\"relu\", kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, activation=\"relu\",kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, activation=\"relu\",kernel_size=8, padding=\"causal\"),\n",
    "        \n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=256, activation=\"relu\", kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=256, activation=\"relu\",kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=256, activation=\"relu\",kernel_size=8, padding=\"causal\"),\n",
    "        \n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        \n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1),\n",
    "        layers.Activation(\"sigmoid\", dtype=\"float32\")\n",
    "    ])\n",
    "    \n",
    "    lr_decayed_fn = tf.keras.experimental.CosineDecay(\n",
    "        1e-3,\n",
    "        steps_per_epoch\n",
    "    )\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = Params[\"lr\"])\n",
    "\n",
    "    #opt = tfa.optimizers.AdamW(lr_decayed_fn, learning_rate = Params[\"lr\"])\n",
    "    model.compile(\n",
    "        opt,\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"AUC\"]\n",
    "    )\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#best\n",
    "from tensorflow.keras import Sequential, layers\n",
    "with strategy.scope():\n",
    "    model = Sequential([\n",
    "        tf.keras.Input(shape=[4096,3]),\n",
    "        layers.Conv1D(filters=256, activation=\"elu\", kernel_size=3, padding=\"same\"),\n",
    "        layers.Conv1D(filters=256, activation=\"elu\", kernel_size=6, padding=\"same\"),\n",
    "        layers.Conv1D(filters=256, activation=\"elu\", kernel_size=6, padding=\"same\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPool1D(pool_size=2),\n",
    "        layers.Activation(\"elu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=64, activation=\"elu\", kernel_size=8, padding=\"same\"),\n",
    "        layers.Conv1D(filters=128, activation=\"elu\", kernel_size=8, padding=\"same\"),\n",
    "        layers.Conv1D(filters=128, activation=\"elu\", kernel_size=8, padding=\"same\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Dropout(0.4),\n",
    "        layers.Activation(\"elu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=128, activation=\"elu\", kernel_size=8, padding=\"same\"),\n",
    "        layers.Conv1D(filters=128, activation=\"elu\",kernel_size=8, padding=\"same\"),\n",
    "        layers.Conv1D(filters=128, activation=\"elu\",kernel_size=8, padding=\"same\"),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"elu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=256, activation=\"elu\", kernel_size=8, padding=\"same\"),\n",
    "        layers.Conv1D(filters=256, activation=\"elu\",kernel_size=8, padding=\"same\"),\n",
    "        layers.Conv1D(filters=256, activation=\"elu\",kernel_size=8, padding=\"same\"),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"elu\"),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"elu\"),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(64, activation=\"elu\"),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(32, activation=\"elu\"),\n",
    "        \n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, kernel_constraint=tf.keras.constraints.MinMaxNorm(min_value=0.001, max_value=0.999),\n",
    "                     kernel_initializer=tf.keras.initializers.RandomUniform(minval=0.3, maxval=0.4),\n",
    "                     dtype=tf.float32\n",
    "                    ),\n",
    "#         layers.Activation(\"sigmoid\", dtype=\"float32\")\n",
    "    ])\n",
    "    \n",
    "    lr_decayed_fn = tf.keras.experimental.CosineDecay(\n",
    "        1e-3,\n",
    "        steps_per_epoch\n",
    "    )\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = Params[\"lr\"])\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    AUC = tf.keras.metrics.AUC(from_logits=True)\n",
    "    \n",
    "    #opt = tfa.optimizers.AdamW(lr_decayed_fn, learning_rate = Params[\"lr\"])\n",
    "    model.compile(\n",
    "        opt,\n",
    "        loss=loss,\n",
    "        metrics=[AUC]\n",
    "    )\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_2:0' shape=(4, 3, 4) dtype=float32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ownInitializer(shape, dtype=None):\n",
    "    return tf.constant([\n",
    "        [[1,0,0,0],[1,0,0,0],[1,0,0,0]],\n",
    "        [[0,1,0,0],[0,1,0,0],[0,1,0,0]],\n",
    "        [[0,0,1,0],[0,0,1,0],[0,0,1,0]],\n",
    "        [[0,0,0,1],[0,0,0,1],[0,0,0,1]]\n",
    "    ],dtype=dtype)\n",
    "\n",
    "ownInitializer(2,tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 4096, 12)          156       \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 4096, 12, 1)       0         \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 4096, 12, 32)     128       \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 4096, 6, 32)      0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 4096, 6, 16)      1552      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 4096, 96)         0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 4096, 256)        231424    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 4096, 256)        395264    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 4096, 256)        395264    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, 4096, 3)          771       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,049,118\n",
      "Trainable params: 1,024,559\n",
      "Non-trainable params: 1,024,559\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pre_filter = tf.keras.models.load_model(\"model3.h5\")\n",
    "pre_filter.trainable=False\n",
    "pre_filter.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\apist\\anaconda3\\envs\\g2net-tf2\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py:532: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 4096, 3)           1024559   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 4096, 192)         12480     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 4096, 192)        768       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 4096, 192)         0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 4096, 64)          393280    \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 512, 64)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 512, 64)          256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 512, 64)           0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 512, 128)          262272    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 512, 128)         512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 512, 128)          0         \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 512, 128)          262272    \n",
      "                                                                 \n",
      " average_pooling1d (AverageP  (None, 85, 128)          0         \n",
      " ooling1D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 85, 128)          512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 85, 128)           0         \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 85, 256)           524544    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 85, 256)          1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 85, 256)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 21760)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                1392704   \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,879,920\n",
      "Trainable params: 2,853,569\n",
      "Non-trainable params: 1,026,351\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "with strategy.scope():\n",
    "    model = Sequential([\n",
    "        tf.keras.Input(shape=([4096,3])),\n",
    "        pre_filter,\n",
    "        layers.Conv1D(filters=64*3, activation=None, kernel_size=64, padding=\"causal\",input_shape=[4096,3], groups=3),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation(\"elu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=64, activation=None, kernel_size=32, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=8),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation(\"elu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=128, activation=None,kernel_size=32, padding=\"causal\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation(\"elu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=128, activation=None,kernel_size=16, padding=\"causal\"),\n",
    "        layers.AvgPool1D(pool_size=6),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation(\"elu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=256, activation=\"relu\",kernel_size=16, padding=\"causal\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation(\"elu\"),\n",
    "        \n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation=None),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Activation(\"elu\"),\n",
    "        \n",
    "        layers.Dense(64, activation=None),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Activation(\"elu\"),\n",
    "        \n",
    "        \n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(1, kernel_constraint=tf.keras.constraints.MinMaxNorm(min_value=0.001, max_value=0.999),\n",
    "                     kernel_initializer=tf.keras.initializers.RandomUniform(minval=0.3, maxval=0.4)\n",
    "                    ),\n",
    "        layers.Activation(\"sigmoid\", dtype=\"float32\")\n",
    "    ])\n",
    "    \n",
    "    lr_decayed_fn = tf.keras.experimental.CosineDecay(\n",
    "        1e-3,\n",
    "        steps_per_epoch\n",
    "    )\n",
    "    opt = tfa.optimizers.Lookahead(\n",
    "        tfa.optimizers.AdamW(lr_decayed_fn, learning_rate = Params[\"lr\"]),\n",
    "        sync_period=6\n",
    "    )\n",
    "\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    AUC = tf.keras.metrics.AUC(from_logits=True)\n",
    "\n",
    "    #opt = tfa.optimizers.AdamW(lr_decayed_fn, learning_rate = Params[\"lr\"])\n",
    "    model.compile(\n",
    "        opt,\n",
    "        loss=loss,\n",
    "        metrics=[AUC]\n",
    "    )\n",
    "    model.summary()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.optimizer = tfa.optimizers.AdamW(lr_decayed_fn, learning_rate = Params[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_callback(batch_size=8):\n",
    "    lr_start = Params[\"lr\"]\n",
    "    lr_max = 0.0000015 * batch_size\n",
    "    lr_min = 1e-6\n",
    "    lr_ramp_ep = 3\n",
    "    lr_sus_ep = 0\n",
    "    lr_decay = 0.7\n",
    "    \n",
    "    def lrfn(epoch):\n",
    "        initial_epochs = 4\n",
    "        \n",
    "        epoch = epoch - initial_epochs\n",
    "        if epoch < -1 * initial_epochs + 2:\n",
    "            return lr_start/10\n",
    "        elif epoch < 0:\n",
    "            lr = lr_start\n",
    "        elif epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "        return lr\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
    "    return lr_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:54.960073Z",
     "iopub.status.busy": "2021-09-17T17:22:54.959840Z",
     "iopub.status.idle": "2021-09-17T17:22:54.971237Z",
     "shell.execute_reply": "2021-09-17T17:22:54.970202Z",
     "shell.execute_reply.started": "2021-09-17T17:22:54.960046Z"
    }
   },
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.2,\n",
    "    patience=6, min_lr = 0.0000001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta = 0.0003,\n",
    "    mode=\"min\",\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    f\"{MDL_PATH}/model_{Params['version']:03}.h5\",\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weight_only=False,\n",
    "    mode=\"auto\",\n",
    "    save_freq=\"epoch\",\n",
    "    \n",
    ")\n",
    "\n",
    "# callbacks=[get_lr_callback(Params[\"batch_size\"]) ,reduce_lr, early_stop, model_checkpoint]\n",
    "callbacks=[reduce_lr, early_stop,model_checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../input/like-synthetic-whitened-highpassed-tfrec\\\\train15.tfrec']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_files"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with strategy.scope():\n",
    "    model = tf.keras.models.load_model(\"../models/models_v300/model_300.h5\")\n",
    "\n",
    "\n",
    "lr_decayed_fn = tf.keras.experimental.CosineDecay(\n",
    "    1e-3,\n",
    "    steps_per_epoch\n",
    ")\n",
    "    \n",
    "#     opt = tf.keras.optimizers.Adam(learning_rate = Params[\"lr\"])\n",
    "    #opt = tf.keras.optimizers.SGD(learning_rate=Params[\"lr\"])\n",
    "#     opt = tfa.optimizers.Lookahead(\n",
    "#         tf.keras.optimizers.Adam(learning_rate = Params[\"lr\"]),\n",
    "#         sync_period = 6\n",
    "#     )\n",
    "#     opt = tfa.optimizers.AdamW(lr_decayed_fn, learning_rate = Params[\"lr\"])\n",
    "    \n",
    "model.optimizer = tfa.optimizers.Lookahead(\n",
    "    tfa.optimizers.AdamW(lr_decayed_fn, learning_rate = Params[\"lr\"]),\n",
    "    sync_period=6\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if Params[\"train_mode\"] == \"test\":\n",
    "    steps_per_epoch = steps_per_epoch // 10\n",
    "    validation_steps = validation_steps // 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:54.972832Z",
     "iopub.status.busy": "2021-09-17T17:22:54.972552Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4101 steps, validate on 273 steps\n",
      "Epoch 1/35\n",
      "4101/4101 [==============================] - ETA: 0s - batch: 2050.0000 - size: 1.0000 - loss: 0.5914 - auc: 0.7271"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apist\\anaconda3\\envs\\g2net-tf2\\lib\\site-packages\\keras\\engine\\training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4101/4101 [==============================] - 1182s 269ms/step - batch: 2050.0000 - size: 1.0000 - loss: 0.5914 - auc: 0.7271 - val_loss: 1.2928 - val_auc: 0.7018 - lr: 1.0000e-04\n",
      "Epoch 2/35\n",
      "4101/4101 [==============================] - 1104s 269ms/step - batch: 2050.0000 - size: 1.0000 - loss: 0.5498 - auc: 0.7659 - val_loss: 1.3636 - val_auc: 0.6782 - lr: 1.0000e-04\n",
      "Epoch 3/35\n",
      "4101/4101 [==============================] - 1068s 260ms/step - batch: 2050.0000 - size: 1.0000 - loss: 0.5394 - auc: 0.7745 - val_loss: 0.9188 - val_auc: 0.7457 - lr: 1.0000e-04\n",
      "Epoch 4/35\n",
      "4101/4101 [==============================] - 1061s 259ms/step - batch: 2050.0000 - size: 1.0000 - loss: 0.5339 - auc: 0.7790 - val_loss: 1.0271 - val_auc: 0.7062 - lr: 1.0000e-04\n",
      "Epoch 5/35\n",
      "4101/4101 [==============================] - 1065s 260ms/step - batch: 2050.0000 - size: 1.0000 - loss: 0.5293 - auc: 0.7826 - val_loss: 0.7677 - val_auc: 0.7528 - lr: 1.0000e-04\n",
      "Epoch 6/35\n",
      "4101/4101 [==============================] - 1102s 269ms/step - batch: 2050.0000 - size: 1.0000 - loss: 0.5265 - auc: 0.7848 - val_loss: 0.8743 - val_auc: 0.6429 - lr: 1.0000e-04\n",
      "Epoch 7/35\n",
      "4101/4101 [==============================] - 1098s 268ms/step - batch: 2050.0000 - size: 1.0000 - loss: 0.5231 - auc: 0.7879 - val_loss: 0.6601 - val_auc: 0.7655 - lr: 1.0000e-04\n",
      "Epoch 8/35\n",
      "4101/4101 [==============================] - 1095s 267ms/step - batch: 2050.0000 - size: 1.0000 - loss: 0.5210 - auc: 0.7893 - val_loss: 2.6793 - val_auc: 0.4902 - lr: 1.0000e-04\n",
      "Epoch 9/35\n",
      "4101/4101 [==============================] - 1083s 264ms/step - batch: 2050.0000 - size: 1.0000 - loss: 0.5184 - auc: 0.7914 - val_loss: 0.7766 - val_auc: 0.6942 - lr: 1.0000e-04\n",
      "Epoch 10/35\n",
      "4101/4101 [==============================] - 1073s 262ms/step - batch: 2050.0000 - size: 1.0000 - loss: 0.5168 - auc: 0.7926 - val_loss: 0.8989 - val_auc: 0.6898 - lr: 1.0000e-04\n",
      "Epoch 11/35\n",
      "4101/4101 [==============================] - 1073s 262ms/step - batch: 2050.0000 - size: 1.0000 - loss: 0.5144 - auc: 0.7948 - val_loss: 0.7801 - val_auc: 0.7249 - lr: 1.0000e-04\n",
      "Epoch 12/35\n",
      "4101/4101 [==============================] - 1106s 270ms/step - batch: 2050.0000 - size: 1.0000 - loss: 0.5125 - auc: 0.7962 - val_loss: 1.1923 - val_auc: 0.7092 - lr: 1.0000e-04\n",
      "Epoch 13/35\n",
      "4101/4101 [==============================] - ETA: 0s - batch: 2050.0000 - size: 1.0000 - loss: 0.5105 - auc: 0.7980\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "4101/4101 [==============================] - 1108s 270ms/step - batch: 2050.0000 - size: 1.0000 - loss: 0.5105 - auc: 0.7980 - val_loss: 0.6854 - val_auc: 0.6921 - lr: 1.0000e-04\n",
      "Epoch 14/35\n",
      "4101/4101 [==============================] - 1101s 268ms/step - batch: 2050.0000 - size: 1.0000 - loss: 0.5025 - auc: 0.8050 - val_loss: 0.6639 - val_auc: 0.7816 - lr: 2.0000e-05\n",
      "Epoch 15/35\n",
      "4101/4101 [==============================] - 1069s 261ms/step - batch: 2050.0000 - size: 1.0000 - loss: 0.5006 - auc: 0.8064 - val_loss: 0.5652 - val_auc: 0.7840 - lr: 2.0000e-05\n",
      "Epoch 16/35\n",
      "4101/4101 [==============================] - 1077s 263ms/step - batch: 2050.0000 - size: 1.0000 - loss: 0.4996 - auc: 0.8073 - val_loss: 0.5676 - val_auc: 0.7884 - lr: 2.0000e-05\n",
      "Epoch 17/35\n",
      "4101/4101 [==============================] - 1103s 269ms/step - batch: 2050.0000 - size: 1.0000 - loss: 0.4981 - auc: 0.8086 - val_loss: 0.6053 - val_auc: 0.7559 - lr: 2.0000e-05\n",
      "Epoch 18/35\n",
      "4101/4101 [==============================] - 1173s 286ms/step - batch: 2050.0000 - size: 1.0000 - loss: 0.4975 - auc: 0.8092 - val_loss: 0.6451 - val_auc: 0.7389 - lr: 2.0000e-05\n",
      "Epoch 19/35\n",
      "4101/4101 [==============================] - 1162s 283ms/step - batch: 2050.0000 - size: 1.0000 - loss: 0.4969 - auc: 0.8095 - val_loss: 0.6583 - val_auc: 0.7755 - lr: 2.0000e-05\n",
      "Epoch 20/35\n",
      "4101/4101 [==============================] - 1159s 282ms/step - batch: 2050.0000 - size: 1.0000 - loss: 0.4963 - auc: 0.8103 - val_loss: 0.5494 - val_auc: 0.7894 - lr: 2.0000e-05\n",
      "Epoch 21/35\n",
      " 199/4101 [>.............................] - ETA: 17:22 - batch: 99.0000 - size: 1.0000 - loss: 0.4955 - auc: 0.8090"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_38568/211468715.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m history = model.fit(train_ds, validation_data = val_ds, epochs = Params[\"epochs\"], shuffle=True,\n\u001b[0;32m      2\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                     verbose=1, callbacks=callbacks)\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\g2net-tf2\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    794\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 796\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    797\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\g2net-tf2\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    655\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    656\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 657\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\g2net-tf2\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[0mactual_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactual_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\g2net-tf2\\lib\\site-packages\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   4185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4186\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 4187\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   4188\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4189\u001b[0m     output_structure = tf.nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\g2net-tf2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1483\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1484\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1485\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1486\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1487\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, validation_data = val_ds, epochs = Params[\"epochs\"], shuffle=True,\n",
    "                    steps_per_epoch = steps_per_epoch, validation_steps=validation_steps,\n",
    "                    verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.2,\n",
    "    patience=6, min_lr = 0.0000001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta = 0.0003,\n",
    "    mode=\"min\",\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    f\"{MDL_PATH}/model_{Params['version']:03}_2.h5\",\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weight_only=False,\n",
    "    mode=\"auto\",\n",
    "    save_freq=\"epoch\",\n",
    "    \n",
    ")\n",
    "\n",
    "# callbacks=[get_lr_callback(Params[\"batch_size\"]) ,reduce_lr, early_stop, model_checkpoint]\n",
    "callbacks=[reduce_lr, early_stop,model_checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 4096, 3)           1024559   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 4096, 192)         12480     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 4096, 192)        768       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 4096, 192)         0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 4096, 64)          393280    \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 512, 64)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 512, 64)          256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 512, 64)           0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 512, 128)          262272    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 512, 128)         512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 512, 128)          0         \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 512, 128)          262272    \n",
      "                                                                 \n",
      " average_pooling1d (AverageP  (None, 85, 128)          0         \n",
      " ooling1D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 85, 128)          512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 85, 128)           0         \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 85, 256)           524544    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 85, 256)          1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 85, 256)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 21760)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                1392704   \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,879,920\n",
      "Trainable params: 1,024,559\n",
      "Non-trainable params: 2,855,361\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    for layer in model.layers:\n",
    "        layer.trainable=False\n",
    "    #model.load_weights(f\"{MDL_PATH}/model_{Params['version']:03}.h5\")\n",
    "    pre_filter.trainable=True\n",
    "    \n",
    "\n",
    "    opt = tfa.optimizers.Lookahead(\n",
    "        tfa.optimizers.AdamW(lr_decayed_fn, learning_rate = 5e-4),\n",
    "        sync_period=6\n",
    "    )\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    AUC = tf.keras.metrics.AUC()\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "        opt,\n",
    "        loss=loss,\n",
    "        metrics=[AUC]\n",
    "    )\n",
    "    model.summary()\n",
    "    model.load_weights(f\"{MDL_PATH}/model_{Params['version']:03}.h5\")\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4101 steps, validate on 273 steps\n",
      "Epoch 1/35\n",
      "4101/4101 [==============================] - 2463s 599ms/step - batch: 2050.0000 - size: 1.0000 - loss: 0.4929 - auc_8: 0.8137 - val_loss: 7.6120 - val_auc_8: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 2/35\n",
      " 283/4101 [=>............................] - ETA: 37:10 - batch: 141.0000 - size: 1.0000 - loss: 0.4757 - auc_8: 0.8249"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_38568/1171825608.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m history2 = model.fit(train_ds, validation_data = val_ds, epochs = Params[\"epochs\"], shuffle=True,\n\u001b[0;32m      2\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                     verbose=1, callbacks=callbacks)\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\g2net-tf2\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    794\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 796\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    797\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\g2net-tf2\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    655\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    656\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 657\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\g2net-tf2\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[0mactual_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactual_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\g2net-tf2\\lib\\site-packages\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   4185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4186\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 4187\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   4188\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4189\u001b[0m     output_structure = tf.nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\g2net-tf2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1483\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1484\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1485\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1486\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1487\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history2 = model.fit(train_ds, validation_data = val_ds, epochs = Params[\"epochs\"], shuffle=True,\n",
    "                    steps_per_epoch = steps_per_epoch, validation_steps=validation_steps,\n",
    "                    verbose=1, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 4096, 3)           1024559   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 4096, 192)         12480     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 4096, 192)        768       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 4096, 192)         0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 4096, 64)          393280    \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 512, 64)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 512, 64)          256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 512, 64)           0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 512, 128)          262272    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 512, 128)         512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 512, 128)          0         \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 512, 128)          262272    \n",
      "                                                                 \n",
      " average_pooling1d (AverageP  (None, 85, 128)          0         \n",
      " ooling1D)                                                       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 85, 128)          512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 85, 128)           0         \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 85, 256)           524544    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 85, 256)          1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 85, 256)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 21760)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                1392704   \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,879,920\n",
      "Trainable params: 2,607\n",
      "Non-trainable params: 3,877,313\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    for layer in model.layers:\n",
    "        layer.trainable=False\n",
    "    #model.load_weights(f\"{MDL_PATH}/model_{Params['version']:03}.h5\")\n",
    "    pre_filter.trainable=True\n",
    "    for layer in pre_filter.layers[6:-1]:\n",
    "        layer.trainable=False\n",
    "    \n",
    "\n",
    "    opt = tfa.optimizers.Lookahead(\n",
    "        tfa.optimizers.AdamW(lr_decayed_fn, learning_rate = 1e-3),\n",
    "        sync_period=6\n",
    "    )\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    AUC = tf.keras.metrics.AUC()\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "        opt,\n",
    "        loss=loss,\n",
    "        metrics=[AUC]\n",
    "    )\n",
    "    model.summary()\n",
    "#     model.load_weights(f\"{MDL_PATH}/model_{Params['version']:03}.h5\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4101 steps, validate on 273 steps\n",
      "Epoch 1/35\n",
      "4101/4101 [==============================] - ETA: 0s - batch: 2050.0000 - size: 1.0000 - loss: 0.5240 - auc_12: 0.7951"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apist\\anaconda3\\envs\\g2net-tf2\\lib\\site-packages\\keras\\engine\\training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4101/4101 [==============================] - 2406s 585ms/step - batch: 2050.0000 - size: 1.0000 - loss: 0.5240 - auc_12: 0.7951 - val_loss: 1.2926 - val_auc_12: 0.7791 - lr: 0.0010\n",
      "Epoch 2/35\n",
      " 723/4101 [====>.........................] - ETA: 32:13 - batch: 361.0000 - size: 1.0000 - loss: 0.5045 - auc_12: 0.8125"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_38568/1171825608.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m history2 = model.fit(train_ds, validation_data = val_ds, epochs = Params[\"epochs\"], shuffle=True,\n\u001b[0;32m      2\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                     verbose=1, callbacks=callbacks)\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\g2net-tf2\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    794\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 796\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    797\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\g2net-tf2\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    655\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    656\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 657\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\g2net-tf2\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[0mactual_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactual_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\g2net-tf2\\lib\\site-packages\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   4185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4186\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 4187\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   4188\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4189\u001b[0m     output_structure = tf.nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\g2net-tf2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1483\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1484\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1485\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1486\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1487\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history2 = model.fit(train_ds, validation_data = val_ds, epochs = Params[\"epochs\"], shuffle=True,\n",
    "                    steps_per_epoch = steps_per_epoch, validation_steps=validation_steps,\n",
    "                    verbose=1, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_38568/3879627881.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{MDL_PATH}/*\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\g2net-tf2\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mtranspose\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\g2net-tf2\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mtranspose\u001b[1;34m(a, axes)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m     \"\"\"\n\u001b[1;32m--> 653\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'transpose'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\g2net-tf2\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "historyFrame = pd.DataFrame(history.history)\n",
    "historyFrame[[\"auc\", \"val_auc\"]].plot()\n",
    "historyFrame[[\"loss\", \"val_loss\"]].plot()\n",
    "historyFrame.to_csv(f\"{MDL_PATH}/history_mdl{Params['version']:03}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyFrame2 = pd.DataFrame(history2.history)\n",
    "historyFrame2[[\"auc_1\", \"val_auc_1\"]].plot()\n",
    "historyFrame2[[\"loss\", \"val_loss\"]].plot()\n",
    "historyFrame2.to_csv(f\"{MDL_PATH}/history_mdl{Params['version']:03}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "historyFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyFrame2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch = historyFrame.val_auc.argmax()\n",
    "best_loss = historyFrame.iloc[best_epoch].loss\n",
    "best_auc = historyFrame.iloc[best_epoch].val_auc\n",
    "print(\"best epoch:\", best_epoch,\n",
    "      \"| best loss:\", best_loss,\n",
    "      \"| best auc:\", best_auc\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch = historyFrame2.val_auc.argmax()\n",
    "best_loss = historyFrame2.iloc[best_epoch].loss\n",
    "best_auc = historyFrame2.iloc[best_epoch].val_auc\n",
    "print(\"best epoch:\", best_epoch,\n",
    "      \"| best loss:\", best_loss,\n",
    "      \"| best auc:\", best_auc\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Params.copy()\n",
    "result[\"bavg_epoch\"] = int(best_epoch)\n",
    "result[\"bavg_loss\"] = float(best_loss)\n",
    "result[\"bavg_auc\"] = float(best_auc)\n",
    "with open(f\"{MDL_PATH}/params.json\", \"w\") as file:\n",
    "    json.dump(result, file)\n",
    "\n",
    "if not os.path.exists(f\"{MDLS_PATH}/results.csv\"):\n",
    "    df_save = pd.DataFrame(result, index=[0])\n",
    "    df_save.to_csv(f\"{MDLS_PATH}/results.csv\")\n",
    "else:\n",
    "    df_old = pd.read_csv(f\"{MDLS_PATH}/results.csv\", index_col=0)\n",
    "    df_save = pd.DataFrame(result, index = [df_old.index.max() + 1])\n",
    "    df_save = df_old.append(df_save, ignore_index=True)\n",
    "    df_save.to_csv(f\"{MDLS_PATH}/results.csv\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(f\"{MDLS_PATH}/results.csv\",index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_ds, val_ds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_scores = []\n",
    "with strategy.scope():\n",
    "    model = tf.keras.models.load_model(f\"{MDL_PATH}/model_{Params['version']:03}_2.h5\")\n",
    "n_vals = len(val_files) // len(tfrec_folders)\n",
    "for i in range(len(tfrec_folders)):\n",
    "    files = val_files[i * n_vals: (i + 1)*n_vals] \n",
    "    val_ds = load_dataset(files, shuffle=False, cache=False, ordered=True, repeat=False)\n",
    "    history = model.evaluate(val_ds, steps = 35000 * n_vals // Params[\"batch_size\"])\n",
    "    prediction_scores.append(history[1])\n",
    "print(prediction_scores)\n",
    "best_pred = np.array(prediction_scores).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = load_dataset(test_files[best_pred], cache=False, shuffle=False, ordered=True, labeled=False, repeat=False, return_labels=False)\n",
    "test_prediction = model.predict(test_set)\n",
    "sub = pd.read_csv(\"../input/g2net-gravitational-wave-detection/sample_submission.csv\")\n",
    "sub.target = test_prediction.flatten()\n",
    "sub.to_csv(f\"{MDL_PATH}/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
