{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:41.713595Z",
     "iopub.status.busy": "2021-09-17T17:22:41.713335Z",
     "iopub.status.idle": "2021-09-17T17:22:41.725791Z",
     "shell.execute_reply": "2021-09-17T17:22:41.725140Z",
     "shell.execute_reply.started": "2021-09-17T17:22:41.713521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KAGGLE: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "KAGGLE = True\n",
    "if os.name == \"nt\":\n",
    "    KAGGLE = False\n",
    "print(f\"KAGGLE: {KAGGLE}\")\n",
    "if not KAGGLE:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:41.727703Z",
     "iopub.status.busy": "2021-09-17T17:22:41.726861Z",
     "iopub.status.idle": "2021-09-17T17:22:43.782183Z",
     "shell.execute_reply": "2021-09-17T17:22:43.781042Z",
     "shell.execute_reply.started": "2021-09-17T17:22:41.727659Z"
    }
   },
   "outputs": [],
   "source": [
    "# general\n",
    "import gc\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "from glob import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from IPython.core.display import display, HTML\n",
    "if KAGGLE:\n",
    "    from kaggle_datasets import KaggleDatasets\n",
    "\n",
    "# ML\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# DL\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "if not KAGGLE:\n",
    "    physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:43.784172Z",
     "iopub.status.busy": "2021-09-17T17:22:43.783660Z",
     "iopub.status.idle": "2021-09-17T17:22:43.799591Z",
     "shell.execute_reply": "2021-09-17T17:22:43.798788Z",
     "shell.execute_reply.started": "2021-09-17T17:22:43.784124Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:43.803149Z",
     "iopub.status.busy": "2021-09-17T17:22:43.802737Z",
     "iopub.status.idle": "2021-09-17T17:22:43.811423Z",
     "shell.execute_reply": "2021-09-17T17:22:43.810381Z",
     "shell.execute_reply.started": "2021-09-17T17:22:43.803115Z"
    }
   },
   "outputs": [],
   "source": [
    "def auto_select_accelerator():\n",
    "    TPU_DETECTED = False\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "        tf.keras.mixed_precision.set_global_policy(\"mixed_bfloat16\")\n",
    "        print(f\"Running on TPU: {tpu.master()}\")\n",
    "        TPU_DETECTED = True\n",
    "    except ValueError:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "        tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    num_replicas = strategy.num_replicas_in_sync\n",
    "    print(f\"Running on {num_replicas} replica{'s' if num_replicas > 1 else ''}\")\n",
    "    return strategy, TPU_DETECTED, num_replicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:43.813082Z",
     "iopub.status.busy": "2021-09-17T17:22:43.812779Z",
     "iopub.status.idle": "2021-09-17T17:22:49.385095Z",
     "shell.execute_reply": "2021-09-17T17:22:49.384154Z",
     "shell.execute_reply.started": "2021-09-17T17:22:43.813043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 2080 Ti, compute capability 7.5\n",
      "Running on 1 replica\n"
     ]
    }
   ],
   "source": [
    "strategy, TPU_Detected, REPLICAS = auto_select_accelerator()\n",
    "INPUT_DIR = \"../input/g2net-gravitational-wave-detection\"\n",
    "MDLS_PATH = \".\" if KAGGLE else \"../models\"\n",
    "# TRAIN_FILES_PATH = \"../input/filtered*_tfrec\"\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "tfrec_folders = [\"like-synthetic-whitened-highpassed-tfrec\"]#, \"like-synthetic-whitened-highpassed-\"]#[\"whitened-longer-tfrec\", \"whitened-tfrec\", \"filtered-whitened-tfrec\"]\n",
    "if KAGGLE:\n",
    "    tfrec_folders = [\"filteredwhitenedtfrec\",\"whitened-tfrec\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:10:49.388016Z",
     "iopub.status.busy": "2021-09-17T17:10:49.387794Z",
     "iopub.status.idle": "2021-09-17T17:10:49.406903Z",
     "shell.execute_reply": "2021-09-17T17:10:49.405619Z",
     "shell.execute_reply.started": "2021-09-17T17:10:49.387988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>version</th>\n",
       "      <th>train_mode</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>bavg_epoch</th>\n",
       "      <th>bavg_loss</th>\n",
       "      <th>bavg_auc</th>\n",
       "      <th>changelog</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>136</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>21</td>\n",
       "      <td>0.420500</td>\n",
       "      <td>0.854332</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>137</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>31</td>\n",
       "      <td>0.364450</td>\n",
       "      <td>0.876402</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>139</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.436394</td>\n",
       "      <td>0.845982</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>139</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.436394</td>\n",
       "      <td>0.845982</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>140</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>0.423539</td>\n",
       "      <td>0.844709</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>142</td>\n",
       "      <td>full</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.447673</td>\n",
       "      <td>0.836974</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>164</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>9</td>\n",
       "      <td>0.438891</td>\n",
       "      <td>0.844589</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>200</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>36</td>\n",
       "      <td>0.464103</td>\n",
       "      <td>0.839057</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>210</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>24</td>\n",
       "      <td>0.444001</td>\n",
       "      <td>0.847384</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>221</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "      <td>0.435199</td>\n",
       "      <td>0.850387</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>222</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>26</td>\n",
       "      <td>0.437144</td>\n",
       "      <td>0.849532</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>224</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>30</td>\n",
       "      <td>0.434321</td>\n",
       "      <td>0.851999</td>\n",
       "      <td>Input layers: 1/2 kernel size. more kernels</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>231</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>29</td>\n",
       "      <td>0.439757</td>\n",
       "      <td>0.848020</td>\n",
       "      <td>only whitened ds</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>242</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>0.447073</td>\n",
       "      <td>0.848576</td>\n",
       "      <td>only whitened ds with sgd</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>244</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>0.465401</td>\n",
       "      <td>0.834070</td>\n",
       "      <td>only whitened ds with sgd</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>245</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.842836</td>\n",
       "      <td>all ds</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>247</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>0.444699</td>\n",
       "      <td>0.842759</td>\n",
       "      <td>second run with adam</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>307</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0.490005</td>\n",
       "      <td>0.852317</td>\n",
       "      <td>second run with adam</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>346</td>\n",
       "      <td>full</td>\n",
       "      <td>64</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0.403609</td>\n",
       "      <td>0.839891</td>\n",
       "      <td>second run with adam</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>355</td>\n",
       "      <td>full</td>\n",
       "      <td>64</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>0.431249</td>\n",
       "      <td>0.840779</td>\n",
       "      <td>second run with adam</td>\n",
       "      <td>69420.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lr  version train_mode  batch_size  epochs  bavg_epoch  bavg_loss  \\\n",
       "36  0.00010      136       full         256     100          21   0.420500   \n",
       "37  0.00010      137       full         256     100          31   0.364450   \n",
       "38  0.00010      139       full         256     100          10   0.436394   \n",
       "39  0.00010      139       full         256     100          10   0.436394   \n",
       "40  0.00010      140       full         256      15          12   0.423539   \n",
       "41  0.00010      142       full         128       5           4   0.447673   \n",
       "42  0.00010      164       full         256     100           9   0.438891   \n",
       "43  0.00001      200       full         256     100          36   0.464103   \n",
       "44  0.00100      210       full         256     100          24   0.444001   \n",
       "45  0.00010      221       full         256     100          40   0.435199   \n",
       "46  0.00010      222       full         256     100          26   0.437144   \n",
       "47  0.00010      224       full         256     100          30   0.434321   \n",
       "48  0.00010      231       full         256     100          29   0.439757   \n",
       "49  0.00010      242       full         256      25          23   0.447073   \n",
       "50  0.00010      244       full         256      25          21   0.465401   \n",
       "51  0.00010      245       full         256      25          23   0.446809   \n",
       "52  0.00010      247       full         256      35           9   0.444699   \n",
       "53  0.00010      307       full         256      35           1   0.490005   \n",
       "54  0.00010      346       full          64      35           0   0.403609   \n",
       "55  0.00001      355       full          64      35           6   0.431249   \n",
       "\n",
       "    bavg_auc                                    changelog     seed  \n",
       "36  0.854332                                       1/4 lr     42.0  \n",
       "37  0.876402                                       1/4 lr     42.0  \n",
       "38  0.845982                                       1/4 lr     42.0  \n",
       "39  0.845982                                       1/4 lr     42.0  \n",
       "40  0.844709                                       1/4 lr     42.0  \n",
       "41  0.836974                                       1/4 lr     42.0  \n",
       "42  0.844589                                       1/4 lr     42.0  \n",
       "43  0.839057                                       1/4 lr     42.0  \n",
       "44  0.847384                                       1/4 lr     42.0  \n",
       "45  0.850387                                       1/4 lr     69.0  \n",
       "46  0.849532                                       1/4 lr     69.0  \n",
       "47  0.851999  Input layers: 1/2 kernel size. more kernels     69.0  \n",
       "48  0.848020                             only whitened ds     69.0  \n",
       "49  0.848576                    only whitened ds with sgd     42.0  \n",
       "50  0.834070                    only whitened ds with sgd     42.0  \n",
       "51  0.842836                                       all ds     42.0  \n",
       "52  0.842759                         second run with adam     42.0  \n",
       "53  0.852317                         second run with adam     42.0  \n",
       "54  0.839891                         second run with adam     42.0  \n",
       "55  0.840779                         second run with adam  69420.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = \"no file\"\n",
    "if not KAGGLE:\n",
    "    results_df = pd.read_csv(\"../models/results.csv\", index_col=[0]).tail(20)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:49.386775Z",
     "iopub.status.busy": "2021-09-17T17:22:49.386544Z",
     "iopub.status.idle": "2021-09-17T17:22:49.599221Z",
     "shell.execute_reply": "2021-09-17T17:22:49.598282Z",
     "shell.execute_reply.started": "2021-09-17T17:22:49.386749Z"
    }
   },
   "outputs": [],
   "source": [
    "if KAGGLE:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    user_credential = user_secrets.get_gcloud_credential()\n",
    "    user_secrets.set_tensorflow_credential(user_credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:49.601675Z",
     "iopub.status.busy": "2021-09-17T17:22:49.601417Z",
     "iopub.status.idle": "2021-09-17T17:22:49.608611Z",
     "shell.execute_reply": "2021-09-17T17:22:49.607652Z",
     "shell.execute_reply.started": "2021-09-17T17:22:49.601646Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{MDLS_PATH}/results.csv\"):\n",
    "    VER = 1\n",
    "else:\n",
    "    results = pd.read_csv(f\"{MDLS_PATH}/results.csv\", index_col=[0])\n",
    "    VER = int(results.version.max())\n",
    "Params ={\n",
    "    \"lr\": 1e-4 * REPLICAS,\n",
    "    \"version\": VER,\n",
    "    \"train_mode\": \"full\", #test, full\n",
    "    \"batch_size\": 128 * REPLICAS,\n",
    "    \"epochs\":35,\n",
    "    \"seed\": 69420,\n",
    "    \"changelog\": \"second run with adam\",\n",
    "}\n",
    "seed_everything(Params[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:49.609927Z",
     "iopub.status.busy": "2021-09-17T17:22:49.609716Z",
     "iopub.status.idle": "2021-09-17T17:22:49.626003Z",
     "shell.execute_reply": "2021-09-17T17:22:49.624837Z",
     "shell.execute_reply.started": "2021-09-17T17:22:49.609902Z"
    }
   },
   "outputs": [],
   "source": [
    "VER = Params[\"version\"]\n",
    "MDL_PATH = f\"{MDLS_PATH}/models_v{VER:03}\"\n",
    "while os.path.exists(MDL_PATH):\n",
    "    VER += 1\n",
    "    MDL_PATH = f\"{MDLS_PATH}/models_v{VER:03}\"\n",
    "Params[\"version\"]=VER\n",
    "os.mkdir(MDL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_cut(x,y):\n",
    "    tensor = x\n",
    "    if random.random() > 0.65:\n",
    "        maxVal=128\n",
    "        dt = tf.random.uniform(shape=[],minval=2, maxval=maxVal, dtype=tf.int32)\n",
    "        t0 = tf.random.uniform(shape=[],minval=1, maxval=dt, dtype=tf.int32)\n",
    "        t1 = tf.random.uniform(shape=[],minval=0, maxval=t0, dtype=tf.int32)\n",
    "        paddings =  [\n",
    "            [0,0],\n",
    "            [t0,dt-t0],\n",
    "            [0,0]\n",
    "        ]\n",
    "        tensor = tf.pad(tensor[:,t0:t0+(4096-dt)], paddings=paddings)\n",
    "    tensor = tensor * [-1. if random.random() > 0.5 else 1.,\n",
    "                       -1. if random.random() > 0.5 else 1.,\n",
    "                       -1. if random.random() > 0.5 else 1.]\n",
    "    \n",
    "#     paddings = [\n",
    "#         [0,0],\n",
    "#         [256,256],\n",
    "#         [0,0]\n",
    "#     ]\n",
    "#     tensor = tf.pad(tensor[:,256:-256], paddings=paddings)\n",
    "    tensor = tf.reshape(tensor,[Params[\"batch_size\"], 4096, 3])\n",
    "    \n",
    "    tensor = tf.cast(tensor, tf.float32)\n",
    "    return tensor, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:49.628201Z",
     "iopub.status.busy": "2021-09-17T17:22:49.627767Z",
     "iopub.status.idle": "2021-09-17T17:22:49.642197Z",
     "shell.execute_reply": "2021-09-17T17:22:49.641358Z",
     "shell.execute_reply.started": "2021-09-17T17:22:49.628155Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset(files, shuffle=True, ordered=False, labeled = True, repeat=True, return_labels = False, cut = False, cache=False):\n",
    "    if ordered:\n",
    "        dataset = tf.data.TFRecordDataset(files, num_parallel_reads=None)\n",
    "    else:\n",
    "        dataset = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n",
    "    \n",
    "\n",
    "    def _parse_function(example_proto):\n",
    "        if labeled:\n",
    "            keys_to_feature = {\n",
    "                \"TimeSeries\":tf.io.FixedLenFeature([4096,3],tf.float32),\n",
    "                \"Target\":tf.io.FixedLenFeature([], tf.int64, default_value=0)}\n",
    "            if return_labels:\n",
    "                keys_to_feature[\"id\"]=tf.io.FixedLenFeature([],tf.string, default_value=\"\")\n",
    "        else:\n",
    "            keys_to_feature = {\n",
    "                \"TimeSeries\": tf.io.FixedLenFeature([4096,3],tf.float32)\n",
    "            }\n",
    "        parsed_features = tf.io.parse_single_example(example_proto, keys_to_feature)\n",
    "        if labeled:\n",
    "            if return_labels:\n",
    "                return parsed_features[\"TimeSeries\"], parsed_features[\"Target\"], parsed_features[\"id\"]\n",
    "            else:\n",
    "                return parsed_features[\"TimeSeries\"], parsed_features[\"Target\"]\n",
    "        else:\n",
    "            return parsed_features[\"TimeSeries\"]\n",
    "    if not ordered:\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic=False\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "    # parse the record into tensors.\n",
    "    dataset = dataset.map(_parse_function, num_parallel_calls=AUTO)\n",
    "#     dataset = dataset.cache()\n",
    "\n",
    "    # Repeat the input infinitely\n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "\n",
    "    # shuffle the dataset\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=50000, reshuffle_each_iteration=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # Generate batches\n",
    "    dataset = dataset.batch(Params[\"batch_size\"])\n",
    "    if cut:\n",
    "        dataset = dataset.map(random_cut, num_parallel_calls=AUTO, deterministic=False)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:49.644543Z",
     "iopub.status.busy": "2021-09-17T17:22:49.644208Z",
     "iopub.status.idle": "2021-09-17T17:22:53.400603Z",
     "shell.execute_reply": "2021-09-17T17:22:53.399615Z",
     "shell.execute_reply.started": "2021-09-17T17:22:49.644502Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_val_files(folders):\n",
    "    train_files = []\n",
    "    val_files = []\n",
    "    test_files = []\n",
    "    all_train_files = []\n",
    "    for folder in folders:\n",
    "        if KAGGLE:\n",
    "            TRAIN_FILES_PATH = KaggleDatasets().get_gcs_path(folder)\n",
    "            TEST_FILES_PATH = KaggleDatasets().get_gcs_path(f\"{folder}test\")\n",
    "            all_files_train = np.sort(tf.io.gfile.glob(f\"{TRAIN_FILES_PATH}/train_*.tfrec\"))\n",
    "            all_files_test = np.sort(tf.io.gfile.glob(f\"{TEST_FILES_PATH}/test_*.tfrec\"))\n",
    "        else:\n",
    "            all_files_train = np.sort(glob(f\"../input/{folder}/train*.tfrec\"))\n",
    "            all_files_test = np.sort(glob(f\"../input/{folder}/test*.tfrec\"))\n",
    "        \n",
    "        if Params[\"train_mode\"] == \"test\":\n",
    "            train_files.extend(all_files_train[:1])\n",
    "            val_files.extend(all_files_train[-1:])\n",
    "            \n",
    "        else:\n",
    "            train_files.extend(all_files_train[:-1])\n",
    "            val_files.extend(all_files_train[-1:])\n",
    "        test_files.append(all_files_test)\n",
    "        all_train_files.append(all_files_train)\n",
    "    return train_files, val_files, test_files, all_train_files\n",
    "\n",
    "train_files, val_files, test_files, all_train_files = get_train_val_files(tfrec_folders)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import time\n",
    "def benchmark(dataset, num_epochs=2):\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        for sample in dataset:\n",
    "            # Performing a training step\n",
    "            time.sleep(0.01)\n",
    "    print(\"Execution time:\", time.perf_counter() - start_time)\n",
    "benchmark(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_files = [\"../input/synthetic-tfrec/train_250_Mpc.tfrec\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:53.402399Z",
     "iopub.status.busy": "2021-09-17T17:22:53.402073Z",
     "iopub.status.idle": "2021-09-17T17:22:53.738249Z",
     "shell.execute_reply": "2021-09-17T17:22:53.737171Z",
     "shell.execute_reply.started": "2021-09-17T17:22:53.402358Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apist\\anaconda3\\envs\\g2net-tf\\lib\\site-packages\\ipykernel_launcher.py:49: UserWarning: Seed 69420 from outer graph might be getting used by function Dataset_map_random_cut, if the random op has not been provided any seed. Explicitly set the seed in the function if this is not the intended behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = load_dataset(train_files, cut=True)\n",
    "val_ds = load_dataset(val_files, shuffle=False, cache=False, ordered=True, repeat=False)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for x in train_ds:\n",
    "    tens = x\n",
    "    break"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-15T15:02:39.670012Z",
     "iopub.status.busy": "2021-09-15T15:02:39.669804Z",
     "iopub.status.idle": "2021-09-15T15:02:40.728147Z",
     "shell.execute_reply": "2021-09-15T15:02:40.727152Z",
     "shell.execute_reply.started": "2021-09-15T15:02:39.669988Z"
    }
   },
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "with strategy.scope():\n",
    "    model = Sequential([\n",
    "        layers.Conv1D(filters=32, kernel_size=16, padding=\"causal\",input_shape=[4096,3]),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=64, kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=128, kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=256, kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1),\n",
    "        layers.Activation(\"sigmoid\", dtype=\"float32\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        tf.keras.optimizers.Adam(learning_rate = Params[\"lr\"]),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"AUC\"]\n",
    "    )\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 560000 // 16 * len(train_files) // Params[\"batch_size\"]\n",
    "validation_steps = 560000 // 16 * len(val_files) // Params[\"batch_size\"]\n",
    "# steps_per_epoch = (560000 // 16 * 15 + 12500 * 82) // Params[\"batch_size\"]\n",
    "# validation_steps = (560000 // 16 * 1 + 12500 * 1) // Params[\"batch_size\"]\n",
    "# steps_per_epoch = 12500 * len(train_files) // Params[\"batch_size\"]\n",
    "# validation_steps = 12500 * len(val_files) // Params[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:54.953060Z",
     "iopub.status.busy": "2021-09-17T17:22:54.952812Z",
     "iopub.status.idle": "2021-09-17T17:22:54.957958Z",
     "shell.execute_reply": "2021-09-17T17:22:54.957019Z",
     "shell.execute_reply.started": "2021-09-17T17:22:54.953033Z"
    }
   },
   "source": [
    "#best\n",
    "from tensorflow.keras import Sequential, layers\n",
    "with strategy.scope():\n",
    "    model = Sequential([\n",
    "        layers.Conv1D(filters=256, activation=\"relu\", kernel_size=3, padding=\"causal\",input_shape=[4096,3]),\n",
    "        layers.Conv1D(filters=256, activation=\"relu\", kernel_size=6, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=256, activation=\"relu\", kernel_size=6, padding=\"causal\"),\n",
    "        \n",
    "        layers.MaxPool1D(pool_size=2),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=64, activation=\"relu\", kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, activation=\"relu\", kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, activation=\"relu\", kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=128, activation=\"relu\", kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, activation=\"relu\",kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, activation=\"relu\",kernel_size=8, padding=\"causal\"),\n",
    "        \n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=256, activation=\"relu\", kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=256, activation=\"relu\",kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=256, activation=\"relu\",kernel_size=8, padding=\"causal\"),\n",
    "        \n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        \n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1),\n",
    "        layers.Activation(\"sigmoid\", dtype=\"float32\")\n",
    "    ])\n",
    "    \n",
    "    lr_decayed_fn = tf.keras.experimental.CosineDecay(\n",
    "        1e-3,\n",
    "        steps_per_epoch\n",
    "    )\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = Params[\"lr\"])\n",
    "\n",
    "    #opt = tfa.optimizers.AdamW(lr_decayed_fn, learning_rate = Params[\"lr\"])\n",
    "    model.compile(\n",
    "        opt,\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"AUC\"]\n",
    "    )\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#best\n",
    "from tensorflow.keras import Sequential, layers\n",
    "with strategy.scope():\n",
    "    model = Sequential([\n",
    "        tf.keras.Input(shape=[4096,3]),\n",
    "        layers.Conv1D(filters=256, activation=\"elu\", kernel_size=3, padding=\"same\"),\n",
    "        layers.Conv1D(filters=256, activation=\"elu\", kernel_size=6, padding=\"same\"),\n",
    "        layers.Conv1D(filters=256, activation=\"elu\", kernel_size=6, padding=\"same\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPool1D(pool_size=2),\n",
    "        layers.Activation(\"elu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=64, activation=\"elu\", kernel_size=8, padding=\"same\"),\n",
    "        layers.Conv1D(filters=128, activation=\"elu\", kernel_size=8, padding=\"same\"),\n",
    "        layers.Conv1D(filters=128, activation=\"elu\", kernel_size=8, padding=\"same\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Dropout(0.4),\n",
    "        layers.Activation(\"elu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=128, activation=\"elu\", kernel_size=8, padding=\"same\"),\n",
    "        layers.Conv1D(filters=128, activation=\"elu\",kernel_size=8, padding=\"same\"),\n",
    "        layers.Conv1D(filters=128, activation=\"elu\",kernel_size=8, padding=\"same\"),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"elu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=256, activation=\"elu\", kernel_size=8, padding=\"same\"),\n",
    "        layers.Conv1D(filters=256, activation=\"elu\",kernel_size=8, padding=\"same\"),\n",
    "        layers.Conv1D(filters=256, activation=\"elu\",kernel_size=8, padding=\"same\"),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"elu\"),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"elu\"),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(64, activation=\"elu\"),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(32, activation=\"elu\"),\n",
    "        \n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, kernel_constraint=tf.keras.constraints.MinMaxNorm(min_value=0.001, max_value=0.999),\n",
    "                     kernel_initializer=tf.keras.initializers.RandomUniform(minval=0.3, maxval=0.4),\n",
    "                     dtype=tf.float32\n",
    "                    ),\n",
    "#         layers.Activation(\"sigmoid\", dtype=\"float32\")\n",
    "    ])\n",
    "    \n",
    "    lr_decayed_fn = tf.keras.experimental.CosineDecay(\n",
    "        1e-3,\n",
    "        steps_per_epoch\n",
    "    )\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = Params[\"lr\"])\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    AUC = tf.keras.metrics.AUC(from_logits=True)\n",
    "    \n",
    "    #opt = tfa.optimizers.AdamW(lr_decayed_fn, learning_rate = Params[\"lr\"])\n",
    "    model.compile(\n",
    "        opt,\n",
    "        loss=loss,\n",
    "        metrics=[AUC]\n",
    "    )\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_2:0' shape=(4, 3, 4) dtype=float32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ownInitializer(shape, dtype=None):\n",
    "    return tf.constant([\n",
    "        [[1,0,0,0],[1,0,0,0],[1,0,0,0]],\n",
    "        [[0,1,0,0],[0,1,0,0],[0,1,0,0]],\n",
    "        [[0,0,1,0],[0,0,1,0],[0,0,1,0]],\n",
    "        [[0,0,0,1],[0,0,0,1],[0,0,0,1]]\n",
    "    ],dtype=dtype)\n",
    "\n",
    "ownInitializer(2,tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 4096, 12)          156       \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4096, 12, 1)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 4096, 12, 32)      128       \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 4096, 6, 32)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 4096, 6, 16)       1552      \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 4096, 96)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 4096, 256)         231424    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 4096, 256)         395264    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 4096, 256)         395264    \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 4096, 3)           771       \n",
      "=================================================================\n",
      "Total params: 2,049,118\n",
      "Trainable params: 1,024,559\n",
      "Non-trainable params: 1,024,559\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    pre_filter = tf.keras.models.load_model(\"model3.h5\")\n",
    "    pre_filter.trainable=False\n",
    "# pre_filter.trainable=False\n",
    "pre_filter.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "with strategy.scope():\n",
    "    model = Sequential([\n",
    "        tf.keras.Input(shape=([4096,3])),\n",
    "        pre_filter,\n",
    "        layers.Conv1D(filters=64*3, activation=None, kernel_size=64, padding=\"causal\",input_shape=[4096,3], groups=3),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation(\"elu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=64, activation=None, kernel_size=32, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=8),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation(\"elu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=128, activation=None,kernel_size=32, padding=\"causal\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation(\"elu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=128, activation=None,kernel_size=16, padding=\"causal\"),\n",
    "        layers.AvgPool1D(pool_size=6),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation(\"elu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=256, activation=\"relu\",kernel_size=16, padding=\"causal\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation(\"elu\"),\n",
    "        \n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation=None),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Activation(\"elu\"),\n",
    "        \n",
    "        layers.Dense(64, activation=None),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Activation(\"elu\"),\n",
    "        \n",
    "        \n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(1, kernel_constraint=tf.keras.constraints.MinMaxNorm(min_value=0.001, max_value=0.999),\n",
    "                     kernel_initializer=tf.keras.initializers.RandomUniform(minval=0.3, maxval=0.4)\n",
    "                    ),\n",
    "        layers.Activation(\"sigmoid\", dtype=\"float32\")\n",
    "    ])\n",
    "    \n",
    "    lr_decayed_fn = tf.keras.experimental.CosineDecay(\n",
    "        1e-3,\n",
    "        steps_per_epoch\n",
    "    )\n",
    "    opt = tfa.optimizers.Lookahead(\n",
    "        tfa.optimizers.AdamW(lr_decayed_fn, learning_rate = Params[\"lr\"]),\n",
    "        sync_period=6\n",
    "    )\n",
    "\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    AUC = tf.keras.metrics.AUC(from_logits=True)\n",
    "\n",
    "    #opt = tfa.optimizers.AdamW(lr_decayed_fn, learning_rate = Params[\"lr\"])\n",
    "    model.compile(\n",
    "        opt,\n",
    "        loss=loss,\n",
    "        metrics=[AUC]\n",
    "    )\n",
    "    model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\apist\\anaconda3\\envs\\g2net-tf\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py:520: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      (None, 4096, 3)           1024559   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 4096, 96)          576       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 4096, 96)          384       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 4096, 96)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 2048, 96)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 2048, 64)          49216     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 2048, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2048, 64)          0         \n",
      "_________________________________________________________________\n",
      "average_pooling1d (AveragePo (None, 341, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 21824)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                1396800   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,474,288\n",
      "Trainable params: 1,449,217\n",
      "Non-trainable params: 1,025,071\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "with strategy.scope():\n",
    "    model = Sequential([\n",
    "        tf.keras.Input(shape=([4096,3])),\n",
    "        pre_filter,\n",
    "        layers.Conv1D(filters=32*3, activation=None, kernel_size=5, padding=\"same\",input_shape=[4096,3], groups=3),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation(\"elu\"),\n",
    "        layers.MaxPool1D(),\n",
    "        \n",
    "        layers.Conv1D(filters=64, activation=\"relu\",kernel_size=8, padding=\"same\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation(\"elu\"),\n",
    "        \n",
    "        layers.AvgPool1D(pool_size=6),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation=None),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Activation(\"elu\"),\n",
    "        \n",
    "        layers.Dense(32, activation=None),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Activation(\"elu\"),\n",
    "        \n",
    "        \n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(1, kernel_constraint=tf.keras.constraints.MinMaxNorm(min_value=0.001, max_value=0.999),\n",
    "                     kernel_initializer=tf.keras.initializers.RandomUniform(minval=0.3, maxval=0.4),\n",
    "                     dtype=\"float32\"\n",
    "                    ),\n",
    "#         layers.Activation(\"sigmoid\", dtype=\"float32\")\n",
    "    ])\n",
    "    \n",
    "    lr_decayed_fn = tf.keras.experimental.CosineDecay(\n",
    "        1e-3,\n",
    "        steps_per_epoch\n",
    "    )\n",
    "    opt = tfa.optimizers.Lookahead(\n",
    "        tfa.optimizers.AdamW(lr_decayed_fn, learning_rate = Params[\"lr\"]),\n",
    "        sync_period=6\n",
    "    )\n",
    "\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    AUC = tf.keras.metrics.AUC(from_logits=True)\n",
    "\n",
    "    #opt = tfa.optimizers.AdamW(lr_decayed_fn, learning_rate = Params[\"lr\"])\n",
    "    model.compile(\n",
    "        opt,\n",
    "        loss=loss,\n",
    "        metrics=[AUC]\n",
    "    )\n",
    "    model.summary()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.optimizer = tfa.optimizers.AdamW(lr_decayed_fn, learning_rate = Params[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_callback(batch_size=8):\n",
    "    lr_start = Params[\"lr\"]\n",
    "    lr_max = 0.0000015 * batch_size\n",
    "    lr_min = 1e-6\n",
    "    lr_ramp_ep = 3\n",
    "    lr_sus_ep = 0\n",
    "    lr_decay = 0.7\n",
    "    \n",
    "    def lrfn(epoch):\n",
    "        initial_epochs = 4\n",
    "        \n",
    "        epoch = epoch - initial_epochs\n",
    "        if epoch < -1 * initial_epochs + 2:\n",
    "            return lr_start/10\n",
    "        elif epoch < 0:\n",
    "            lr = lr_start\n",
    "        elif epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "        return lr\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
    "    return lr_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:54.960073Z",
     "iopub.status.busy": "2021-09-17T17:22:54.959840Z",
     "iopub.status.idle": "2021-09-17T17:22:54.971237Z",
     "shell.execute_reply": "2021-09-17T17:22:54.970202Z",
     "shell.execute_reply.started": "2021-09-17T17:22:54.960046Z"
    }
   },
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.2,\n",
    "    patience=6, min_lr = 0.0000001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta = 0.0003,\n",
    "    mode=\"min\",\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    f\"{MDL_PATH}/model_{Params['version']:03}.h5\",\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weight_only=False,\n",
    "    include_optimizer=True,\n",
    "    mode=\"auto\",\n",
    "    save_freq=\"epoch\",\n",
    "    \n",
    ")\n",
    "\n",
    "# callbacks=[get_lr_callback(Params[\"batch_size\"]) ,reduce_lr, early_stop, model_checkpoint]\n",
    "callbacks=[reduce_lr, early_stop,model_checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../input/like-synthetic-whitened-highpassed-tfrec\\\\train15.tfrec']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_files"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with strategy.scope():\n",
    "    model = tf.keras.models.load_model(\"../models/models_v300/model_300.h5\")\n",
    "\n",
    "\n",
    "lr_decayed_fn = tf.keras.experimental.CosineDecay(\n",
    "    1e-3,\n",
    "    steps_per_epoch\n",
    ")\n",
    "    \n",
    "#     opt = tf.keras.optimizers.Adam(learning_rate = Params[\"lr\"])\n",
    "    #opt = tf.keras.optimizers.SGD(learning_rate=Params[\"lr\"])\n",
    "#     opt = tfa.optimizers.Lookahead(\n",
    "#         tf.keras.optimizers.Adam(learning_rate = Params[\"lr\"]),\n",
    "#         sync_period = 6\n",
    "#     )\n",
    "#     opt = tfa.optimizers.AdamW(lr_decayed_fn, learning_rate = Params[\"lr\"])\n",
    "    \n",
    "model.optimizer = tfa.optimizers.Lookahead(\n",
    "    tfa.optimizers.AdamW(lr_decayed_fn, learning_rate = Params[\"lr\"]),\n",
    "    sync_period=6\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if Params[\"train_mode\"] == \"test\":\n",
    "    steps_per_epoch = steps_per_epoch // 10\n",
    "    validation_steps = validation_steps // 10"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pretrain_files = glob(\"../input/synthetic-tfrec/*\")\n",
    "pretrain_train_files = random.sample(pretrain_files[:-1],8)\n",
    "pretrain_files[0] = \"like\"\n",
    "pretrain_val_files = pretrain_files[-1:]\n",
    "pretrain_train_ds = load_dataset(pretrain_train_files, repeat=False)\n",
    "pretrain_val_ds = load_dataset(pretrain_val_files, repeat=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "history = model.fit(train_ds, validation_data=val_ds, steps_per_epoch=500, validation_steps=500, epochs=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "history = model.fit(pretrain_train_ds,validation_data = pretrain_val_ds,\n",
    "                    steps_per_epoch=len(pretrain_train_files)*12500 // Params[\"batch_size\"],\n",
    "                    validation_steps=len(pretrain_val_files)*12500 // Params[\"batch_size\"],epochs = 2, shuffle=False,\n",
    "                    verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4101 steps, validate on 273 steps\n",
      "Epoch 1/35\n",
      "4101/4101 [==============================] - ETA: 0s - batch: 2050.0000 - size: 1.0000 - loss: 1.4519 - auc: 0.5335"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apist\\anaconda3\\envs\\g2net-tf\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4101/4101 [==============================] - 979s 237ms/step - batch: 2050.0000 - size: 1.0000 - loss: 1.4519 - auc: 0.5335 - val_loss: 1.2752 - val_auc: 0.5091 - lr: 1.0000e-04\n",
      "Epoch 2/35\n",
      "4101/4101 [==============================] - 976s 238ms/step - batch: 2050.0000 - size: 1.0000 - loss: 0.7876 - auc: 0.5710 - val_loss: 0.7394 - val_auc: 0.6555 - lr: 1.0000e-04\n",
      "Epoch 3/35\n",
      "4101/4101 [==============================] - 977s 238ms/step - batch: 2050.0000 - size: 1.0000 - loss: 0.6885 - auc: 0.6331 - val_loss: 0.8021 - val_auc: 0.6958 - lr: 1.0000e-04\n",
      "Epoch 4/35\n",
      "4101/4101 [==============================] - 974s 238ms/step - batch: 2050.0000 - size: 1.0000 - loss: 0.6736 - auc: 0.6481 - val_loss: 0.7474 - val_auc: 0.6934 - lr: 1.0000e-04\n",
      "Epoch 5/35\n",
      "2038/4101 [=============>................] - ETA: 7:44 - batch: 1018.5000 - size: 1.0000 - loss: 0.6648 - auc: 0.6582"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, validation_data = val_ds, epochs = Params[\"epochs\"], shuffle=True,\n",
    "                    steps_per_epoch = steps_per_epoch, validation_steps=validation_steps,\n",
    "                    verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "weights = model.weights"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "encoder = pre_filter.weights[:4]\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    #model.load_weights(f\"{MDL_PATH}/model_{Params['version']:03}.h5\")\n",
    "#     model.trainable=True\n",
    "#     for layer in model.get_layers()\n",
    "    for layer in model.layers:\n",
    "        \n",
    "        layer.trainable=False\n",
    "#     model.layers[0].Trainable=True\n",
    "#     model.layers[-1].Trainable=True\n",
    "    pre_filter.trainable = True\n",
    "    for layer in pre_filter.layers[5:-1]:\n",
    "        layer.trainable=False\n",
    "#     opt = tfa.optimizers.Lookahead(\n",
    "#         tfa.optimizers.AdamW(lr_decayed_fn, learning_rate = 1e-5),\n",
    "#         sync_period=6\n",
    "#     )\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = 1e-5)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    AUC = tf.keras.metrics.AUC()\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "        opt,\n",
    "        loss=loss,\n",
    "        metrics=[AUC]\n",
    "    )\n",
    "    model.summary()\n",
    "#     model.load_weights(f\"{MDL_PATH}/model_{Params['version']:03}.h5\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "history = model.fit(pretrain_train_ds,validation_data = pretrain_val_ds,\n",
    "                    steps_per_epoch=len(pretrain_train_files)*12500 // Params[\"batch_size\"],\n",
    "                    validation_steps=len(pretrain_val_files)*12500 // Params[\"batch_size\"],epochs = 1, shuffle=False,\n",
    "                    verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with strategy.scope():\n",
    "    #model.load_weights(f\"{MDL_PATH}/model_{Params['version']:03}.h5\")\n",
    "#     model.trainable=True\n",
    "# #     for layer in model.get_layers()\n",
    "#     model.layers[0].Trainable=True\n",
    "#     model.layers[-1].Trainable=True\n",
    "    pre_filter.trainable=False\n",
    "\n",
    "    opt = tfa.optimizers.Lookahead(\n",
    "        tfa.optimizers.AdamW(lr_decayed_fn, learning_rate = 1e-5),\n",
    "        sync_period=6\n",
    "    )\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    AUC = tf.keras.metrics.AUC()\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "        opt,\n",
    "        loss=loss,\n",
    "        metrics=[AUC]\n",
    "    )\n",
    "    model.summary()\n",
    "#     model.load_weights(f\"{MDL_PATH}/model_{Params['version']:03}.h5\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(pretrain_train_ds,validation_data = pretrain_val_ds,\n",
    "                    steps_per_epoch=len(pretrain_train_files)*12500 // Params[\"batch_size\"],\n",
    "                    validation_steps=len(pretrain_val_files)*12500 // Params[\"batch_size\"],epochs = 1, shuffle=False,\n",
    "                    verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:54.972832Z",
     "iopub.status.busy": "2021-09-17T17:22:54.972552Z"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, validation_data = val_ds, epochs = Params[\"epochs\"], shuffle=True,\n",
    "                    steps_per_epoch = steps_per_epoch, validation_steps=validation_steps,\n",
    "                    verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    #model.load_weights(f\"{MDL_PATH}/model_{Params['version']:03}.h5\")\n",
    "#     model.trainable=True\n",
    "    for layer in model.layers:\n",
    "        layer.trainable=False\n",
    "# #     for layer in model.get_layers()\n",
    "#     model.layers[0].Trainable=True\n",
    "#     model.layers[-1].Trainable=True\n",
    "    pre_filter.trainable=True\n",
    "\n",
    "#     opt = tfa.optimizers.Lookahead(\n",
    "#         tfa.optimizers.AdamW(lr_decayed_fn, learning_rate = 1e-3),\n",
    "#         sync_period=6\n",
    "#     )\n",
    "    opt = tf.keras.optimizers.Adam()\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    AUC = tf.keras.metrics.AUC()\n",
    "\n",
    "\n",
    "    model.compile(\n",
    "        opt,\n",
    "        loss=loss,\n",
    "        metrics=[AUC]\n",
    "    )\n",
    "    model.summary()\n",
    "#     model.load_weights(f\"{MDL_PATH}/model_{Params['version']:03}.h5\")\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model.fit(train_ds, validation_data = val_ds, epochs = Params[\"epochs\"], shuffle=True,\n",
    "                    steps_per_epoch = steps_per_epoch, validation_steps=validation_steps,\n",
    "                    verbose=1, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "historyFrame = pd.DataFrame(history.history)\n",
    "historyFrame[[\"auc\", \"val_auc\"]].plot()\n",
    "historyFrame[[\"loss\", \"val_loss\"]].plot()\n",
    "historyFrame.to_csv(f\"{MDL_PATH}/history_mdl{Params['version']:03}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyFrame2 = pd.DataFrame(history2.history)\n",
    "historyFrame2[[\"auc_1\", \"val_auc_1\"]].plot()\n",
    "historyFrame2[[\"loss\", \"val_loss\"]].plot()\n",
    "historyFrame2.to_csv(f\"{MDL_PATH}/history_mdl{Params['version']:03}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "historyFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyFrame2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch = historyFrame.val_auc.argmax()\n",
    "best_loss = historyFrame.iloc[best_epoch].loss\n",
    "best_auc = historyFrame.iloc[best_epoch].val_auc\n",
    "print(\"best epoch:\", best_epoch,\n",
    "      \"| best loss:\", best_loss,\n",
    "      \"| best auc:\", best_auc\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch = historyFrame2.val_auc.argmax()\n",
    "best_loss = historyFrame2.iloc[best_epoch].loss\n",
    "best_auc = historyFrame2.iloc[best_epoch].val_auc\n",
    "print(\"best epoch:\", best_epoch,\n",
    "      \"| best loss:\", best_loss,\n",
    "      \"| best auc:\", best_auc\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Params.copy()\n",
    "result[\"bavg_epoch\"] = int(best_epoch)\n",
    "result[\"bavg_loss\"] = float(best_loss)\n",
    "result[\"bavg_auc\"] = float(best_auc)\n",
    "with open(f\"{MDL_PATH}/params.json\", \"w\") as file:\n",
    "    json.dump(result, file)\n",
    "\n",
    "if not os.path.exists(f\"{MDLS_PATH}/results.csv\"):\n",
    "    df_save = pd.DataFrame(result, index=[0])\n",
    "    df_save.to_csv(f\"{MDLS_PATH}/results.csv\")\n",
    "else:\n",
    "    df_old = pd.read_csv(f\"{MDLS_PATH}/results.csv\", index_col=0)\n",
    "    df_save = pd.DataFrame(result, index = [df_old.index.max() + 1])\n",
    "    df_save = df_old.append(df_save, ignore_index=True)\n",
    "    df_save.to_csv(f\"{MDLS_PATH}/results.csv\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(f\"{MDLS_PATH}/results.csv\",index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_ds, val_ds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_scores = []\n",
    "with strategy.scope():\n",
    "    model = tf.keras.models.load_model(f\"{MDL_PATH}/model_{Params['version']:03}.h5\")\n",
    "n_vals = len(val_files) // len(tfrec_folders)\n",
    "for i in range(len(tfrec_folders)):\n",
    "    files = val_files[i * n_vals: (i + 1)*n_vals] \n",
    "    val_ds = load_dataset(files, shuffle=False, cache=False, ordered=True, repeat=False)\n",
    "    history = model.evaluate(val_ds, steps = 35000 * n_vals // Params[\"batch_size\"])\n",
    "    prediction_scores.append(history[1])\n",
    "print(prediction_scores)\n",
    "best_pred = np.array(prediction_scores).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = load_dataset(test_files[best_pred], cache=False, shuffle=False, ordered=True, labeled=False, repeat=False, return_labels=False)\n",
    "test_prediction = model.predict(test_set)\n",
    "sub = pd.read_csv(\"../input/g2net-gravitational-wave-detection/sample_submission.csv\")\n",
    "sub.target = test_prediction.flatten()\n",
    "sub.to_csv(f\"{MDL_PATH}/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(train_ds, steps = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
