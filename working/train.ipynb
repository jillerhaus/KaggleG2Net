{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:41.713595Z",
     "iopub.status.busy": "2021-09-17T17:22:41.713335Z",
     "iopub.status.idle": "2021-09-17T17:22:41.725791Z",
     "shell.execute_reply": "2021-09-17T17:22:41.725140Z",
     "shell.execute_reply.started": "2021-09-17T17:22:41.713521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KAGGLE: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "KAGGLE = True\n",
    "if os.name == \"nt\":\n",
    "    KAGGLE = False\n",
    "print(f\"KAGGLE: {KAGGLE}\")\n",
    "if not KAGGLE:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:41.727703Z",
     "iopub.status.busy": "2021-09-17T17:22:41.726861Z",
     "iopub.status.idle": "2021-09-17T17:22:43.782183Z",
     "shell.execute_reply": "2021-09-17T17:22:43.781042Z",
     "shell.execute_reply.started": "2021-09-17T17:22:41.727659Z"
    }
   },
   "outputs": [],
   "source": [
    "# general\n",
    "import gc\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "from glob import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from IPython.core.display import display, HTML\n",
    "if KAGGLE:\n",
    "    from kaggle_datasets import KaggleDatasets\n",
    "\n",
    "# ML\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# DL\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:43.784172Z",
     "iopub.status.busy": "2021-09-17T17:22:43.783660Z",
     "iopub.status.idle": "2021-09-17T17:22:43.799591Z",
     "shell.execute_reply": "2021-09-17T17:22:43.798788Z",
     "shell.execute_reply.started": "2021-09-17T17:22:43.784124Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tf.compat.v1.disable_eager_execution()\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:43.803149Z",
     "iopub.status.busy": "2021-09-17T17:22:43.802737Z",
     "iopub.status.idle": "2021-09-17T17:22:43.811423Z",
     "shell.execute_reply": "2021-09-17T17:22:43.810381Z",
     "shell.execute_reply.started": "2021-09-17T17:22:43.803115Z"
    }
   },
   "outputs": [],
   "source": [
    "def auto_select_accelerator():\n",
    "    TPU_DETECTED = False\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "        tf.keras.mixed_precision.set_global_policy(\"mixed_bfloat16\")\n",
    "        print(f\"Running on TPU: {tpu.master()}\")\n",
    "        TPU_DETECTED = True\n",
    "    except ValueError:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "        tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    num_replicas = strategy.num_replicas_in_sync\n",
    "    print(f\"Running on {num_replicas} replica{'s' if num_replicas > 1 else ''}\")\n",
    "    return strategy, TPU_DETECTED, num_replicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:43.813082Z",
     "iopub.status.busy": "2021-09-17T17:22:43.812779Z",
     "iopub.status.idle": "2021-09-17T17:22:49.385095Z",
     "shell.execute_reply": "2021-09-17T17:22:49.384154Z",
     "shell.execute_reply.started": "2021-09-17T17:22:43.813043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "Running on 1 replica\n"
     ]
    }
   ],
   "source": [
    "strategy, TPU_Detected, REPLICAS = auto_select_accelerator()\n",
    "INPUT_DIR = \"../input/g2net-gravitational-wave-detection\"\n",
    "MDLS_PATH = \".\" if KAGGLE else \"../models\"\n",
    "# TRAIN_FILES_PATH = \"../input/filtered*_tfrec\"\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "tfrec_folders = [\"filtered-whitened-tfrec\", \"whitened-tfrec\"]#[\"filtered-tfrec\", \"filtered-whitened-tfrec\", \"filtered-whitened-inverted-tfrec\", \"whitened-tfrec\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:10:49.388016Z",
     "iopub.status.busy": "2021-09-17T17:10:49.387794Z",
     "iopub.status.idle": "2021-09-17T17:10:49.406903Z",
     "shell.execute_reply": "2021-09-17T17:10:49.405619Z",
     "shell.execute_reply.started": "2021-09-17T17:10:49.387988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>version</th>\n",
       "      <th>train_mode</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>bavg_epoch</th>\n",
       "      <th>bavg_loss</th>\n",
       "      <th>bavg_auc</th>\n",
       "      <th>changelog</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>78</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "      <td>0.442443</td>\n",
       "      <td>0.841306</td>\n",
       "      <td>doubled last layer in all blocks. lr/10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>79</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>25</td>\n",
       "      <td>0.439258</td>\n",
       "      <td>0.842578</td>\n",
       "      <td>doubled last layer in all blocks doubled size ...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>83</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>27</td>\n",
       "      <td>0.439051</td>\n",
       "      <td>0.841536</td>\n",
       "      <td>added dropout layers to dense layers</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>88</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.453092</td>\n",
       "      <td>0.834111</td>\n",
       "      <td>everything had 3 layers now and relu activation</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>92</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>17</td>\n",
       "      <td>0.434836</td>\n",
       "      <td>0.832301</td>\n",
       "      <td>dropout layers</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>110</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>11</td>\n",
       "      <td>0.470093</td>\n",
       "      <td>0.815836</td>\n",
       "      <td>dropout layers</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>113</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "      <td>0.476270</td>\n",
       "      <td>0.813889</td>\n",
       "      <td>dropout layers</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>119</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>36</td>\n",
       "      <td>0.463551</td>\n",
       "      <td>0.812370</td>\n",
       "      <td>new dataset using best model (base model with ...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.00020</td>\n",
       "      <td>124</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>23</td>\n",
       "      <td>0.448196</td>\n",
       "      <td>0.827187</td>\n",
       "      <td>10x lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>134</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>12</td>\n",
       "      <td>0.467185</td>\n",
       "      <td>0.826250</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>135</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>9</td>\n",
       "      <td>0.444164</td>\n",
       "      <td>0.848162</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>136</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>21</td>\n",
       "      <td>0.420500</td>\n",
       "      <td>0.854332</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>137</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>31</td>\n",
       "      <td>0.364450</td>\n",
       "      <td>0.876402</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>139</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.436394</td>\n",
       "      <td>0.845982</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>139</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.436394</td>\n",
       "      <td>0.845982</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>140</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>0.423539</td>\n",
       "      <td>0.844709</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>142</td>\n",
       "      <td>full</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.447673</td>\n",
       "      <td>0.836974</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>164</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>9</td>\n",
       "      <td>0.438891</td>\n",
       "      <td>0.844589</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>200</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>36</td>\n",
       "      <td>0.464103</td>\n",
       "      <td>0.839057</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>210</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>24</td>\n",
       "      <td>0.444001</td>\n",
       "      <td>0.847384</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lr  version train_mode  batch_size  epochs  bavg_epoch  bavg_loss  \\\n",
       "25  0.00010       78       full         256      40          12   0.442443   \n",
       "26  0.00050       79       full         256      40          25   0.439258   \n",
       "27  0.00050       83       full         256      40          27   0.439051   \n",
       "28  0.00010       88       full         256      40           8   0.453092   \n",
       "29  0.00010       92       full         256     100          17   0.434836   \n",
       "30  0.00010      110       full         256     100          11   0.470093   \n",
       "31  0.00010      113       full         256     100           8   0.476270   \n",
       "32  0.00002      119       full         256     100          36   0.463551   \n",
       "33  0.00020      124       full         256     100          23   0.448196   \n",
       "34  0.00010      134       full         256     100          12   0.467185   \n",
       "35  0.00010      135       full         256     100           9   0.444164   \n",
       "36  0.00010      136       full         256     100          21   0.420500   \n",
       "37  0.00010      137       full         256     100          31   0.364450   \n",
       "38  0.00010      139       full         256     100          10   0.436394   \n",
       "39  0.00010      139       full         256     100          10   0.436394   \n",
       "40  0.00010      140       full         256      15          12   0.423539   \n",
       "41  0.00010      142       full         128       5           4   0.447673   \n",
       "42  0.00010      164       full         256     100           9   0.438891   \n",
       "43  0.00001      200       full         256     100          36   0.464103   \n",
       "44  0.00100      210       full         256     100          24   0.444001   \n",
       "\n",
       "    bavg_auc                                          changelog  seed  \n",
       "25  0.841306            doubled last layer in all blocks. lr/10  42.0  \n",
       "26  0.842578  doubled last layer in all blocks doubled size ...  42.0  \n",
       "27  0.841536               added dropout layers to dense layers  42.0  \n",
       "28  0.834111    everything had 3 layers now and relu activation  42.0  \n",
       "29  0.832301                                     dropout layers  42.0  \n",
       "30  0.815836                                     dropout layers  42.0  \n",
       "31  0.813889                                     dropout layers  42.0  \n",
       "32  0.812370  new dataset using best model (base model with ...  42.0  \n",
       "33  0.827187                                             10x lr  42.0  \n",
       "34  0.826250                                             1/4 lr  42.0  \n",
       "35  0.848162                                             1/4 lr  42.0  \n",
       "36  0.854332                                             1/4 lr  42.0  \n",
       "37  0.876402                                             1/4 lr  42.0  \n",
       "38  0.845982                                             1/4 lr  42.0  \n",
       "39  0.845982                                             1/4 lr  42.0  \n",
       "40  0.844709                                             1/4 lr  42.0  \n",
       "41  0.836974                                             1/4 lr  42.0  \n",
       "42  0.844589                                             1/4 lr  42.0  \n",
       "43  0.839057                                             1/4 lr  42.0  \n",
       "44  0.847384                                             1/4 lr  42.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = \"no file\"\n",
    "if not KAGGLE:\n",
    "    results_df = pd.read_csv(\"../models/results.csv\", index_col=[0]).tail(20)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:49.386775Z",
     "iopub.status.busy": "2021-09-17T17:22:49.386544Z",
     "iopub.status.idle": "2021-09-17T17:22:49.599221Z",
     "shell.execute_reply": "2021-09-17T17:22:49.598282Z",
     "shell.execute_reply.started": "2021-09-17T17:22:49.386749Z"
    }
   },
   "outputs": [],
   "source": [
    "if KAGGLE:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    user_credential = user_secrets.get_gcloud_credential()\n",
    "    user_secrets.set_tensorflow_credential(user_credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:49.601675Z",
     "iopub.status.busy": "2021-09-17T17:22:49.601417Z",
     "iopub.status.idle": "2021-09-17T17:22:49.608611Z",
     "shell.execute_reply": "2021-09-17T17:22:49.607652Z",
     "shell.execute_reply.started": "2021-09-17T17:22:49.601646Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{MDLS_PATH}/results.csv\"):\n",
    "    VER = 1\n",
    "else:\n",
    "    results = pd.read_csv(f\"{MDLS_PATH}/results.csv\", index_col=[0])\n",
    "    VER = int(results.version.max())\n",
    "Params ={\n",
    "    \"lr\": 1e-4 * REPLICAS,\n",
    "    \"version\": VER,\n",
    "    \"train_mode\": \"full\", #test, full\n",
    "    \"batch_size\": 256 * REPLICAS,\n",
    "    \"epochs\":100,\n",
    "    \"seed\": 69,\n",
    "    \"changelog\": \"1/4 lr\",\n",
    "}\n",
    "seed_everything(Params[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:49.609927Z",
     "iopub.status.busy": "2021-09-17T17:22:49.609716Z",
     "iopub.status.idle": "2021-09-17T17:22:49.626003Z",
     "shell.execute_reply": "2021-09-17T17:22:49.624837Z",
     "shell.execute_reply.started": "2021-09-17T17:22:49.609902Z"
    }
   },
   "outputs": [],
   "source": [
    "VER = Params[\"version\"]\n",
    "MDL_PATH = f\"{MDLS_PATH}/models_v{VER:03}\"\n",
    "while os.path.exists(MDL_PATH):\n",
    "    VER += 1\n",
    "    MDL_PATH = f\"{MDLS_PATH}/models_v{VER:03}\"\n",
    "Params[\"version\"]=VER\n",
    "os.mkdir(MDL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_cut(x, y):\n",
    "    a = np.zeros(x.shape, dtype=np.float32)\n",
    "    dt = np.random.randint(2,512)\n",
    "    t0 = np.random.randint(1,dt)\n",
    "    t1 = np.random.randint(0,t0)\n",
    "    a[:,t1:t1+(3584)] = x[:,t0:t0+(3584)]\n",
    "    a[:] *= np.array([-1 if random.random() > 0.5 else 1,-1 if random.random() > 0.5 else 1, -1 if random.random() > 0.5 else 1])\n",
    "    return a,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:49.628201Z",
     "iopub.status.busy": "2021-09-17T17:22:49.627767Z",
     "iopub.status.idle": "2021-09-17T17:22:49.642197Z",
     "shell.execute_reply": "2021-09-17T17:22:49.641358Z",
     "shell.execute_reply.started": "2021-09-17T17:22:49.628155Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "input_context = tf.distribute.InputContext(\n",
    "    input_pipeline_id=1,\n",
    "    num_input_pipelines = 8\n",
    ")\n",
    "read_config= tfds.ReadConfig(\n",
    "    input_context=input_context\n",
    ")\n",
    "\n",
    "def load_dataset(files, shuffle=True, ordered=False, labeled = True, repeat=True, return_labels = False, cut = False):\n",
    "    if ordered:\n",
    "        dataset = tf.data.TFRecordDataset(files, num_parallel_reads=None)\n",
    "    else:\n",
    "        dataset = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n",
    "\n",
    "\n",
    "    def _parse_function(example_proto):\n",
    "        if labeled:\n",
    "            keys_to_feature = {\n",
    "                \"TimeSeries\":tf.io.FixedLenFeature([4096,3],tf.float32),\n",
    "                \"Target\":tf.io.FixedLenFeature([], tf.int64, default_value=0)}\n",
    "            if return_labels:\n",
    "                keys_to_feature[\"id\"]=tf.io.FixedLenFeature([],tf.string, default_value=\"\")\n",
    "        else:\n",
    "            keys_to_feature = {\n",
    "                \"TimeSeries\": tf.io.FixedLenFeature([4096,3],tf.float32)\n",
    "            }\n",
    "        parsed_features = tf.io.parse_single_example(example_proto, keys_to_feature)\n",
    "        if labeled:\n",
    "            if return_labels:\n",
    "                return parsed_features[\"TimeSeries\"], parsed_features[\"Target\"], parsed_features[\"id\"]\n",
    "            else:\n",
    "                return parsed_features[\"TimeSeries\"], parsed_features[\"Target\"]\n",
    "        else:\n",
    "            return parsed_features[\"TimeSeries\"]\n",
    "    \n",
    "    if not ordered:\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic=False\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "    # parse the record into tensors.\n",
    "    dataset = dataset.map(_parse_function, num_parallel_calls=AUTO)\n",
    "#     dataset = dataset.cache()\n",
    "\n",
    "    # Repeat the input infinitely\n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "\n",
    "    # shuffle the dataset\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=10000, reshuffle_each_iteration=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # Generate batches\n",
    "    dataset = dataset.batch(Params[\"batch_size\"])\n",
    "    if cut:\n",
    "        dataset = dataset.map(lambda x,y:tf.numpy_function(random_cut, inp=[x,y], Tout=[tf.float32, tf.int64]), num_parallel_calls=AUTO)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:49.644543Z",
     "iopub.status.busy": "2021-09-17T17:22:49.644208Z",
     "iopub.status.idle": "2021-09-17T17:22:53.400603Z",
     "shell.execute_reply": "2021-09-17T17:22:53.399615Z",
     "shell.execute_reply.started": "2021-09-17T17:22:49.644502Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_val_files(folders):\n",
    "    train_files = []\n",
    "    val_files = []\n",
    "    test_files = []\n",
    "    all_train_files = []\n",
    "    for folder in folders:\n",
    "        if KAGGLE:\n",
    "            TRAIN_FILES_PATH = KaggleDatasets().get_gcs_path(folder)\n",
    "            TEST_FILES_PATH = KaggleDatasets().get_gcs_path(f\"{folder}test\")\n",
    "            all_files_train = np.sort(tf.io.gfile.glob(f\"{TRAIN_FILES_PATH}/train_*.tfrec\"))\n",
    "            all_files_test = np.sort(tf.io.gfile.glob(f\"{TEST_FILES_PATH}/test_*.tfrec\"))\n",
    "        else:\n",
    "            all_files_train = np.sort(glob(f\"../input/{folder}/train_*.tfrec\"))\n",
    "            all_files_test = np.sort(glob(f\"../input/{folder}/test_*.tfrec\"))\n",
    "        train_files.extend(all_files_train[:-2])\n",
    "        val_files.extend(all_files_train[-2:])\n",
    "        test_files.append(all_files_test)\n",
    "        all_train_files.append(all_files_train)\n",
    "    return train_files, val_files, test_files, all_train_files\n",
    "\n",
    "train_files, val_files, test_files, all_train_files = get_train_val_files(tfrec_folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.8745666000000005\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def benchmark(dataset, num_epochs=2):\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        for sample in dataset:\n",
    "            # Performing a training step\n",
    "            time.sleep(0.01)\n",
    "    print(\"Execution time:\", time.perf_counter() - start_time)\n",
    "benchmark(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:53.402399Z",
     "iopub.status.busy": "2021-09-17T17:22:53.402073Z",
     "iopub.status.idle": "2021-09-17T17:22:53.738249Z",
     "shell.execute_reply": "2021-09-17T17:22:53.737171Z",
     "shell.execute_reply.started": "2021-09-17T17:22:53.402358Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ds = load_dataset(train_files, cut = True)\n",
    "val_ds = load_dataset(val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_ds:\n",
    "    tensor = x\n",
    "    input_tensor_shape = x[0].shape[1]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrfn(epoch):\n",
    "    epoch = epoch - 2\n",
    "    lr_start = 1e-4\n",
    "    lr_max = 0.0000015 * Params[\"batch_size\"]\n",
    "    lr_min = 1e-7\n",
    "    lr_ramp_ep = 3\n",
    "    lr_sus_ep = 0\n",
    "    lr_decay = 0.7\n",
    "    if epoch < 0:\n",
    "        lr = lr_start\n",
    "    elif epoch < lr_ramp_ep:\n",
    "        lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "    elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "        lr = lr_max\n",
    "    else:\n",
    "        lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "    return lr\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25fac1b1e48>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrs_list = []\n",
    "for i in range(100):\n",
    "    lrs_list.append(lrfn(i))\n",
    "plt.plot(lrs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tensor[0][0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-15T15:02:39.670012Z",
     "iopub.status.busy": "2021-09-15T15:02:39.669804Z",
     "iopub.status.idle": "2021-09-15T15:02:40.728147Z",
     "shell.execute_reply": "2021-09-15T15:02:40.727152Z",
     "shell.execute_reply.started": "2021-09-15T15:02:39.669988Z"
    }
   },
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "with strategy.scope():\n",
    "    model = Sequential([\n",
    "        layers.Conv1D(filters=32, kernel_size=16, padding=\"causal\",input_shape=[4096,3]),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=64, kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=128, kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=256, kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1),\n",
    "        layers.Activation(\"sigmoid\", dtype=\"float32\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        tf.keras.optimizers.Adam(learning_rate = Params[\"lr\"]),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"AUC\"]\n",
    "    )\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 560000 // 16 * len(train_files) // Params[\"batch_size\"]\n",
    "validation_steps = 560000 // 16 * len(val_files) // Params[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:53.740013Z",
     "iopub.status.busy": "2021-09-17T17:22:53.739592Z",
     "iopub.status.idle": "2021-09-17T17:22:54.947598Z",
     "shell.execute_reply": "2021-09-17T17:22:54.946600Z",
     "shell.execute_reply.started": "2021-09-17T17:22:53.739983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 4096, 32)          1568      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 4096, 32)          16416     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 1024, 32)          0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1024, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1024, 64)          16448     \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1024, 128)         65664     \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 1024, 128)         131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 256, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256, 128)          0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 256, 128)          131200    \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 256, 128)          131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 64, 256)           262400    \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 64, 256)           524544    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 16, 256)           0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               524416    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,813,377\n",
      "Trainable params: 1,813,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "with strategy.scope():\n",
    "    model = Sequential([\n",
    "        layers.Conv1D(filters=32, activation=\"relu\", kernel_size=16, padding=\"causal\",input_shape=[4096,3]),\n",
    "        layers.Conv1D(filters=32, activation=\"relu\", kernel_size=16, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=64, activation=\"relu\", kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, activation=\"relu\", kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, activation=\"relu\", kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=128, activation=\"relu\", kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, activation=\"relu\",kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=256, activation=\"relu\", kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=256, activation=\"relu\",kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1),\n",
    "        layers.Activation(\"sigmoid\", dtype=\"float32\")\n",
    "    ])\n",
    "    \n",
    "    lr_decayed_fn = tf.keras.experimental.CosineDecay(\n",
    "        1e-3,\n",
    "        steps_per_epoch\n",
    "    )\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = Params[\"lr\"])\n",
    "\n",
    "    #opt = tfa.optimizers.AdamW(lr_decayed_fn, learning_rate = Params[\"lr\"])\n",
    "    model.compile(\n",
    "        opt,\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"AUC\"]\n",
    "    )\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:54.953060Z",
     "iopub.status.busy": "2021-09-17T17:22:54.952812Z",
     "iopub.status.idle": "2021-09-17T17:22:54.957958Z",
     "shell.execute_reply": "2021-09-17T17:22:54.957019Z",
     "shell.execute_reply.started": "2021-09-17T17:22:54.953033Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_callback(batch_size=8):\n",
    "    lr_start = Params[\"lr\"]\n",
    "    lr_max = 0.0000015 * batch_size\n",
    "    lr_min = 1e-7\n",
    "    lr_ramp_ep = 3\n",
    "    lr_sus_ep = 0\n",
    "    lr_decay = 0.7\n",
    "    \n",
    "    def lrfn(epoch):\n",
    "        initial_epochs = 4\n",
    "        \n",
    "        epoch = epoch - initial_epochs\n",
    "        if epoch < -1 * initial_epochs + 2:\n",
    "            return lr_start/10\n",
    "        elif epoch < 0:\n",
    "            lr = lr_start\n",
    "        elif epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "        return lr\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
    "    return lr_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:54.960073Z",
     "iopub.status.busy": "2021-09-17T17:22:54.959840Z",
     "iopub.status.idle": "2021-09-17T17:22:54.971237Z",
     "shell.execute_reply": "2021-09-17T17:22:54.970202Z",
     "shell.execute_reply.started": "2021-09-17T17:22:54.960046Z"
    }
   },
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.2,\n",
    "    patience=5, min_lr = 0.000001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    f\"{MDL_PATH}/model_{Params['version']:03}.h5\",\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weight_only=False,\n",
    "    mode=\"auto\",\n",
    "    save_freq=\"epoch\"\n",
    ")\n",
    "\n",
    "callbacks=[get_lr_callback(Params[\"batch_size\"]) ,reduce_lr, early_stop, model_checkpoint]\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = tf.keras.models.load_model(\"../models/models_v040/model_040.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-17T17:22:54.972832Z",
     "iopub.status.busy": "2021-09-17T17:22:54.972552Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 1e-05.\n",
      "3828/3828 [==============================] - 177s 44ms/step - loss: 0.6523 - auc: 0.6187 - val_loss: 0.5603 - val_auc: 0.7547\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 1e-05.\n",
      "3828/3828 [==============================] - 175s 46ms/step - loss: 0.5514 - auc: 0.7626 - val_loss: 0.5388 - val_auc: 0.7753\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.0001.\n",
      "3828/3828 [==============================] - 174s 45ms/step - loss: 0.5038 - auc: 0.8015 - val_loss: 0.4839 - val_auc: 0.8194\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.0001.\n",
      "3828/3828 [==============================] - 175s 46ms/step - loss: 0.4796 - auc: 0.8202 - val_loss: 0.4667 - val_auc: 0.8308\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.0001.\n",
      "3828/3828 [==============================] - 175s 46ms/step - loss: 0.4710 - auc: 0.8266 - val_loss: 0.4620 - val_auc: 0.8342\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.00019466666666666666.\n",
      "3828/3828 [==============================] - 176s 46ms/step - loss: 0.4692 - auc: 0.8277 - val_loss: 0.4579 - val_auc: 0.8374\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.00028933333333333334.\n",
      "3828/3828 [==============================] - 174s 46ms/step - loss: 0.4664 - auc: 0.8294 - val_loss: 0.4548 - val_auc: 0.8389\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 0.000384.\n",
      "3828/3828 [==============================] - 163s 43ms/step - loss: 0.4644 - auc: 0.8309 - val_loss: 0.4524 - val_auc: 0.8399\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 0.00026883.\n",
      "3828/3828 [==============================] - 161s 42ms/step - loss: 0.4562 - auc: 0.8369 - val_loss: 0.4542 - val_auc: 0.8396\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 0.00018821099999999998.\n",
      "3828/3828 [==============================] - 162s 42ms/step - loss: 0.4505 - auc: 0.8411 - val_loss: 0.4446 - val_auc: 0.8440\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 0.00013177769999999996.\n",
      "3828/3828 [==============================] - 162s 42ms/step - loss: 0.4465 - auc: 0.8438 - val_loss: 0.4452 - val_auc: 0.8453\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 9.227438999999999e-05.\n",
      "3828/3828 [==============================] - 163s 43ms/step - loss: 0.4436 - auc: 0.8458 - val_loss: 0.4405 - val_auc: 0.8474\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 6.462207299999999e-05.\n",
      "3828/3828 [==============================] - 162s 42ms/step - loss: 0.4409 - auc: 0.8478 - val_loss: 0.4399 - val_auc: 0.8480\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 4.5265451099999986e-05.\n",
      "3828/3828 [==============================] - 162s 42ms/step - loss: 0.4392 - auc: 0.8490 - val_loss: 0.4385 - val_auc: 0.8485\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 3.171581576999999e-05.\n",
      "3828/3828 [==============================] - 163s 42ms/step - loss: 0.4382 - auc: 0.8499 - val_loss: 0.4385 - val_auc: 0.8489\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 2.2231071038999987e-05.\n",
      "3828/3828 [==============================] - 163s 43ms/step - loss: 0.4372 - auc: 0.8505 - val_loss: 0.4381 - val_auc: 0.8486\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 1.559174972729999e-05.\n",
      "3828/3828 [==============================] - 164s 43ms/step - loss: 0.4368 - auc: 0.8508 - val_loss: 0.4383 - val_auc: 0.8487\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 1.0944224809109993e-05.\n",
      "3828/3828 [==============================] - 164s 43ms/step - loss: 0.4357 - auc: 0.8516 - val_loss: 0.4380 - val_auc: 0.8490\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 7.690957366376995e-06.\n",
      "3828/3828 [==============================] - 163s 43ms/step - loss: 0.4356 - auc: 0.8516 - val_loss: 0.4371 - val_auc: 0.8493\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 5.413670156463896e-06.\n",
      "3828/3828 [==============================] - 164s 43ms/step - loss: 0.4360 - auc: 0.8515 - val_loss: 0.4376 - val_auc: 0.8493\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 3.819569109524727e-06.\n",
      "3828/3828 [==============================] - 163s 43ms/step - loss: 0.4356 - auc: 0.8517 - val_loss: 0.4371 - val_auc: 0.8494\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 2.7036983766673087e-06.\n",
      "3828/3828 [==============================] - 164s 43ms/step - loss: 0.4356 - auc: 0.8515 - val_loss: 0.4372 - val_auc: 0.8494\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 1.922588863667116e-06.\n",
      "3828/3828 [==============================] - 165s 43ms/step - loss: 0.4354 - auc: 0.8518 - val_loss: 0.4369 - val_auc: 0.8498\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 00024: LearningRateScheduler setting learning rate to 1.3758122045669812e-06.\n",
      "3828/3828 [==============================] - 167s 44ms/step - loss: 0.4355 - auc: 0.8516 - val_loss: 0.4373 - val_auc: 0.8495\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 00025: LearningRateScheduler setting learning rate to 9.930685431968867e-07.\n",
      "3828/3828 [==============================] - 167s 44ms/step - loss: 0.4352 - auc: 0.8519 - val_loss: 0.4366 - val_auc: 0.8500\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 00026: LearningRateScheduler setting learning rate to 7.251479802378206e-07.\n",
      "3828/3828 [==============================] - 168s 44ms/step - loss: 0.4356 - auc: 0.8515 - val_loss: 0.4370 - val_auc: 0.8496\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 00027: LearningRateScheduler setting learning rate to 5.376035861664744e-07.\n",
      "3828/3828 [==============================] - 191s 50ms/step - loss: 0.4354 - auc: 0.8519 - val_loss: 0.4364 - val_auc: 0.8500\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 00028: LearningRateScheduler setting learning rate to 4.063225103165321e-07.\n",
      "3828/3828 [==============================] - 176s 46ms/step - loss: 0.4351 - auc: 0.8519 - val_loss: 0.4368 - val_auc: 0.8498\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 00029: LearningRateScheduler setting learning rate to 3.1442575722157244e-07.\n",
      "3828/3828 [==============================] - 186s 49ms/step - loss: 0.4350 - auc: 0.8521 - val_loss: 0.4364 - val_auc: 0.8499\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 00030: LearningRateScheduler setting learning rate to 2.500980300551007e-07.\n",
      "3828/3828 [==============================] - 190s 50ms/step - loss: 0.4351 - auc: 0.8520 - val_loss: 0.4365 - val_auc: 0.8500\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 00031: LearningRateScheduler setting learning rate to 2.050686210385705e-07.\n",
      "3828/3828 [==============================] - 191s 50ms/step - loss: 0.4352 - auc: 0.8519 - val_loss: 0.4365 - val_auc: 0.8500\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 00032: LearningRateScheduler setting learning rate to 1.7354803472699933e-07.\n",
      "3828/3828 [==============================] - 196s 51ms/step - loss: 0.4351 - auc: 0.8520 - val_loss: 0.4361 - val_auc: 0.8501\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 00033: LearningRateScheduler setting learning rate to 1.5148362430889952e-07.\n",
      "3828/3828 [==============================] - 188s 49ms/step - loss: 0.4354 - auc: 0.8520 - val_loss: 0.4364 - val_auc: 0.8501\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 00034: LearningRateScheduler setting learning rate to 1.3603853701622966e-07.\n",
      "3828/3828 [==============================] - 189s 49ms/step - loss: 0.4352 - auc: 0.8518 - val_loss: 0.4365 - val_auc: 0.8499\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 00035: LearningRateScheduler setting learning rate to 1.2522697591136076e-07.\n",
      "3828/3828 [==============================] - 197s 51ms/step - loss: 0.4353 - auc: 0.8518 - val_loss: 0.4369 - val_auc: 0.8497\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 00036: LearningRateScheduler setting learning rate to 1.1765888313795253e-07.\n",
      "3828/3828 [==============================] - 195s 51ms/step - loss: 0.4348 - auc: 0.8523 - val_loss: 0.4368 - val_auc: 0.8498\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 00037: LearningRateScheduler setting learning rate to 1.1236121819656677e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3828/3828 [==============================] - 199s 52ms/step - loss: 0.4354 - auc: 0.8518 - val_loss: 0.4363 - val_auc: 0.8500\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 00038: LearningRateScheduler setting learning rate to 1.0865285273759673e-07.\n",
      "3828/3828 [==============================] - 196s 51ms/step - loss: 0.4352 - auc: 0.8519 - val_loss: 0.4363 - val_auc: 0.8501\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 00039: LearningRateScheduler setting learning rate to 1.0605699691631772e-07.\n",
      "3828/3828 [==============================] - 195s 51ms/step - loss: 0.4354 - auc: 0.8519 - val_loss: 0.4368 - val_auc: 0.8497oss: 0.4355 - auc: - ETA: 8s - loss: 0.435 - ETA: 7s -  - ETA: 5s - loss: 0.4354 - auc: 0.8  - ETA: 2s - loss: 0.435 - ETA: 1s - loss: 0.4354 - auc: 0. - ETA: 1s - loss: 0.4354\n",
      "Epoch 40/100\n",
      "\n",
      "Epoch 00040: LearningRateScheduler setting learning rate to 1.042398978414224e-07.\n",
      "3828/3828 [==============================] - 196s 51ms/step - loss: 0.4355 - auc: 0.8516 - val_loss: 0.4365 - val_auc: 0.8500\n",
      "Epoch 41/100\n",
      "\n",
      "Epoch 00041: LearningRateScheduler setting learning rate to 1.0296792848899568e-07.\n",
      "3828/3828 [==============================] - 203s 53ms/step - loss: 0.4352 - auc: 0.8520 - val_loss: 0.4357 - val_auc: 0.8504s - loss: 0.4 - ETA: 4\n",
      "Epoch 42/100\n",
      "\n",
      "Epoch 00042: LearningRateScheduler setting learning rate to 1.0207754994229697e-07.\n",
      "3828/3828 [==============================] - 198s 52ms/step - loss: 0.4352 - auc: 0.8520 - val_loss: 0.4368 - val_auc: 0.8497\n",
      "Epoch 43/100\n",
      "\n",
      "Epoch 00043: LearningRateScheduler setting learning rate to 1.0145428495960788e-07.\n",
      "3828/3828 [==============================] - 202s 53ms/step - loss: 0.4355 - auc: 0.8517 - val_loss: 0.4362 - val_auc: 0.8501\n",
      "Epoch 44/100\n",
      "\n",
      "Epoch 00044: LearningRateScheduler setting learning rate to 1.0101799947172552e-07.\n",
      "3828/3828 [==============================] - 203s 53ms/step - loss: 0.4351 - auc: 0.8521 - val_loss: 0.4368 - val_auc: 0.8497\n",
      "Epoch 45/100\n",
      "\n",
      "Epoch 00045: LearningRateScheduler setting learning rate to 1.0071259963020786e-07.\n",
      "3828/3828 [==============================] - 204s 53ms/step - loss: 0.4351 - auc: 0.8522 - val_loss: 0.4369 - val_auc: 0.8496\n",
      "Epoch 46/100\n",
      "\n",
      "Epoch 00046: LearningRateScheduler setting learning rate to 1.004988197411455e-07.\n",
      "3828/3828 [==============================] - 205s 54ms/step - loss: 0.4351 - auc: 0.8520 - val_loss: 0.4365 - val_auc: 0.8498\n",
      "Epoch 47/100\n",
      "\n",
      "Epoch 00047: LearningRateScheduler setting learning rate to 1.0034917381880184e-07.\n",
      "3828/3828 [==============================] - 211s 55ms/step - loss: 0.4349 - auc: 0.8521 - val_loss: 0.4365 - val_auc: 0.84990s - loss: 0.4349 - \n",
      "Epoch 48/100\n",
      "\n",
      "Epoch 00048: LearningRateScheduler setting learning rate to 1.0024442167316129e-07.\n",
      "3828/3828 [==============================] - 213s 56ms/step - loss: 0.4356 - auc: 0.8518 - val_loss: 0.4364 - val_auc: 0.8502\n",
      "Epoch 49/100\n",
      "\n",
      "Epoch 00049: LearningRateScheduler setting learning rate to 1.001710951712129e-07.\n",
      "3828/3828 [==============================] - 211s 55ms/step - loss: 0.4353 - auc: 0.8516 - val_loss: 0.4366 - val_auc: 0.8499\n",
      "Epoch 50/100\n",
      "\n",
      "Epoch 00050: LearningRateScheduler setting learning rate to 1.0011976661984903e-07.\n",
      "3828/3828 [==============================] - 216s 56ms/step - loss: 0.4353 - auc: 0.8519 - val_loss: 0.4366 - val_auc: 0.8498\n",
      "Epoch 51/100\n",
      "\n",
      "Epoch 00051: LearningRateScheduler setting learning rate to 1.0008383663389432e-07.\n",
      "3828/3828 [==============================] - 218s 57ms/step - loss: 0.4352 - auc: 0.8519 - val_loss: 0.4368 - val_auc: 0.8496\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, validation_data = val_ds, epochs = Params[\"epochs\"], shuffle=True,\n",
    "                    steps_per_epoch = steps_per_epoch, validation_steps=validation_steps,\n",
    "                    verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "historyFrame = pd.DataFrame(history.history)\n",
    "historyFrame[[\"auc\", \"val_auc\"]].plot()\n",
    "historyFrame[[\"loss\", \"val_loss\"]].plot()\n",
    "historyFrame.to_csv(f\"{MDL_PATH}/history_mdl{Params['version']:03}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>auc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_auc</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.652260</td>\n",
       "      <td>0.618672</td>\n",
       "      <td>0.560270</td>\n",
       "      <td>0.754717</td>\n",
       "      <td>1.000000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.551400</td>\n",
       "      <td>0.762610</td>\n",
       "      <td>0.538756</td>\n",
       "      <td>0.775319</td>\n",
       "      <td>1.000000e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.503759</td>\n",
       "      <td>0.801465</td>\n",
       "      <td>0.483872</td>\n",
       "      <td>0.819401</td>\n",
       "      <td>1.000000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.479576</td>\n",
       "      <td>0.820172</td>\n",
       "      <td>0.466725</td>\n",
       "      <td>0.830822</td>\n",
       "      <td>1.000000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.470989</td>\n",
       "      <td>0.826586</td>\n",
       "      <td>0.461988</td>\n",
       "      <td>0.834212</td>\n",
       "      <td>1.000000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.469210</td>\n",
       "      <td>0.827651</td>\n",
       "      <td>0.457918</td>\n",
       "      <td>0.837381</td>\n",
       "      <td>1.946667e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.466409</td>\n",
       "      <td>0.829439</td>\n",
       "      <td>0.454796</td>\n",
       "      <td>0.838857</td>\n",
       "      <td>2.893333e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.464417</td>\n",
       "      <td>0.830851</td>\n",
       "      <td>0.452366</td>\n",
       "      <td>0.839883</td>\n",
       "      <td>3.840000e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.456163</td>\n",
       "      <td>0.836910</td>\n",
       "      <td>0.454216</td>\n",
       "      <td>0.839611</td>\n",
       "      <td>2.688300e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.450514</td>\n",
       "      <td>0.841071</td>\n",
       "      <td>0.444553</td>\n",
       "      <td>0.843993</td>\n",
       "      <td>1.882110e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.446492</td>\n",
       "      <td>0.843775</td>\n",
       "      <td>0.445169</td>\n",
       "      <td>0.845330</td>\n",
       "      <td>1.317777e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.443621</td>\n",
       "      <td>0.845843</td>\n",
       "      <td>0.440523</td>\n",
       "      <td>0.847417</td>\n",
       "      <td>9.227439e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.440889</td>\n",
       "      <td>0.847798</td>\n",
       "      <td>0.439864</td>\n",
       "      <td>0.847990</td>\n",
       "      <td>6.462207e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.439210</td>\n",
       "      <td>0.848987</td>\n",
       "      <td>0.438509</td>\n",
       "      <td>0.848483</td>\n",
       "      <td>4.526545e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.438177</td>\n",
       "      <td>0.849931</td>\n",
       "      <td>0.438498</td>\n",
       "      <td>0.848870</td>\n",
       "      <td>3.171582e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.437184</td>\n",
       "      <td>0.850462</td>\n",
       "      <td>0.438133</td>\n",
       "      <td>0.848572</td>\n",
       "      <td>2.223107e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.436758</td>\n",
       "      <td>0.850824</td>\n",
       "      <td>0.438346</td>\n",
       "      <td>0.848713</td>\n",
       "      <td>1.559175e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.435671</td>\n",
       "      <td>0.851551</td>\n",
       "      <td>0.437966</td>\n",
       "      <td>0.848992</td>\n",
       "      <td>1.094422e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.435648</td>\n",
       "      <td>0.851561</td>\n",
       "      <td>0.437149</td>\n",
       "      <td>0.849262</td>\n",
       "      <td>7.690957e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.436031</td>\n",
       "      <td>0.851476</td>\n",
       "      <td>0.437555</td>\n",
       "      <td>0.849315</td>\n",
       "      <td>5.413670e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.435643</td>\n",
       "      <td>0.851746</td>\n",
       "      <td>0.437106</td>\n",
       "      <td>0.849444</td>\n",
       "      <td>3.819569e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.435559</td>\n",
       "      <td>0.851526</td>\n",
       "      <td>0.437172</td>\n",
       "      <td>0.849405</td>\n",
       "      <td>2.703698e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.435450</td>\n",
       "      <td>0.851756</td>\n",
       "      <td>0.436869</td>\n",
       "      <td>0.849771</td>\n",
       "      <td>1.922589e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.435502</td>\n",
       "      <td>0.851614</td>\n",
       "      <td>0.437283</td>\n",
       "      <td>0.849548</td>\n",
       "      <td>1.375812e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.435203</td>\n",
       "      <td>0.851902</td>\n",
       "      <td>0.436628</td>\n",
       "      <td>0.849968</td>\n",
       "      <td>9.930685e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.435619</td>\n",
       "      <td>0.851492</td>\n",
       "      <td>0.436951</td>\n",
       "      <td>0.849581</td>\n",
       "      <td>7.251480e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.435407</td>\n",
       "      <td>0.851937</td>\n",
       "      <td>0.436381</td>\n",
       "      <td>0.849998</td>\n",
       "      <td>5.376036e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.435119</td>\n",
       "      <td>0.851912</td>\n",
       "      <td>0.436798</td>\n",
       "      <td>0.849782</td>\n",
       "      <td>4.063225e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.434981</td>\n",
       "      <td>0.852070</td>\n",
       "      <td>0.436400</td>\n",
       "      <td>0.849901</td>\n",
       "      <td>3.144258e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.435131</td>\n",
       "      <td>0.852004</td>\n",
       "      <td>0.436485</td>\n",
       "      <td>0.850024</td>\n",
       "      <td>2.500980e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.435195</td>\n",
       "      <td>0.851863</td>\n",
       "      <td>0.436463</td>\n",
       "      <td>0.849974</td>\n",
       "      <td>2.050686e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.435063</td>\n",
       "      <td>0.852033</td>\n",
       "      <td>0.436108</td>\n",
       "      <td>0.850104</td>\n",
       "      <td>1.735480e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.435353</td>\n",
       "      <td>0.852021</td>\n",
       "      <td>0.436386</td>\n",
       "      <td>0.850099</td>\n",
       "      <td>1.514836e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.435241</td>\n",
       "      <td>0.851817</td>\n",
       "      <td>0.436538</td>\n",
       "      <td>0.849862</td>\n",
       "      <td>1.360385e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.435307</td>\n",
       "      <td>0.851827</td>\n",
       "      <td>0.436869</td>\n",
       "      <td>0.849679</td>\n",
       "      <td>1.252270e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.434788</td>\n",
       "      <td>0.852262</td>\n",
       "      <td>0.436800</td>\n",
       "      <td>0.849794</td>\n",
       "      <td>1.176589e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.435442</td>\n",
       "      <td>0.851812</td>\n",
       "      <td>0.436310</td>\n",
       "      <td>0.850028</td>\n",
       "      <td>1.123612e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.435163</td>\n",
       "      <td>0.851902</td>\n",
       "      <td>0.436286</td>\n",
       "      <td>0.850110</td>\n",
       "      <td>1.086529e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.435392</td>\n",
       "      <td>0.851881</td>\n",
       "      <td>0.436840</td>\n",
       "      <td>0.849677</td>\n",
       "      <td>1.060570e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.435465</td>\n",
       "      <td>0.851602</td>\n",
       "      <td>0.436461</td>\n",
       "      <td>0.849984</td>\n",
       "      <td>1.042399e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.435199</td>\n",
       "      <td>0.851991</td>\n",
       "      <td>0.435742</td>\n",
       "      <td>0.850387</td>\n",
       "      <td>1.029679e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.435183</td>\n",
       "      <td>0.852003</td>\n",
       "      <td>0.436788</td>\n",
       "      <td>0.849734</td>\n",
       "      <td>1.020775e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.435488</td>\n",
       "      <td>0.851693</td>\n",
       "      <td>0.436243</td>\n",
       "      <td>0.850105</td>\n",
       "      <td>1.014543e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.435144</td>\n",
       "      <td>0.852068</td>\n",
       "      <td>0.436751</td>\n",
       "      <td>0.849733</td>\n",
       "      <td>1.010180e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.435056</td>\n",
       "      <td>0.852168</td>\n",
       "      <td>0.436906</td>\n",
       "      <td>0.849616</td>\n",
       "      <td>1.007126e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.435138</td>\n",
       "      <td>0.851996</td>\n",
       "      <td>0.436543</td>\n",
       "      <td>0.849788</td>\n",
       "      <td>1.004988e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.434918</td>\n",
       "      <td>0.852081</td>\n",
       "      <td>0.436490</td>\n",
       "      <td>0.849885</td>\n",
       "      <td>1.003492e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.435606</td>\n",
       "      <td>0.851753</td>\n",
       "      <td>0.436373</td>\n",
       "      <td>0.850227</td>\n",
       "      <td>1.002444e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.435318</td>\n",
       "      <td>0.851646</td>\n",
       "      <td>0.436612</td>\n",
       "      <td>0.849918</td>\n",
       "      <td>1.001711e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.435314</td>\n",
       "      <td>0.851886</td>\n",
       "      <td>0.436596</td>\n",
       "      <td>0.849802</td>\n",
       "      <td>1.001198e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.435244</td>\n",
       "      <td>0.851868</td>\n",
       "      <td>0.436800</td>\n",
       "      <td>0.849612</td>\n",
       "      <td>1.000838e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss       auc  val_loss   val_auc            lr\n",
       "0   0.652260  0.618672  0.560270  0.754717  1.000000e-05\n",
       "1   0.551400  0.762610  0.538756  0.775319  1.000000e-05\n",
       "2   0.503759  0.801465  0.483872  0.819401  1.000000e-04\n",
       "3   0.479576  0.820172  0.466725  0.830822  1.000000e-04\n",
       "4   0.470989  0.826586  0.461988  0.834212  1.000000e-04\n",
       "5   0.469210  0.827651  0.457918  0.837381  1.946667e-04\n",
       "6   0.466409  0.829439  0.454796  0.838857  2.893333e-04\n",
       "7   0.464417  0.830851  0.452366  0.839883  3.840000e-04\n",
       "8   0.456163  0.836910  0.454216  0.839611  2.688300e-04\n",
       "9   0.450514  0.841071  0.444553  0.843993  1.882110e-04\n",
       "10  0.446492  0.843775  0.445169  0.845330  1.317777e-04\n",
       "11  0.443621  0.845843  0.440523  0.847417  9.227439e-05\n",
       "12  0.440889  0.847798  0.439864  0.847990  6.462207e-05\n",
       "13  0.439210  0.848987  0.438509  0.848483  4.526545e-05\n",
       "14  0.438177  0.849931  0.438498  0.848870  3.171582e-05\n",
       "15  0.437184  0.850462  0.438133  0.848572  2.223107e-05\n",
       "16  0.436758  0.850824  0.438346  0.848713  1.559175e-05\n",
       "17  0.435671  0.851551  0.437966  0.848992  1.094422e-05\n",
       "18  0.435648  0.851561  0.437149  0.849262  7.690957e-06\n",
       "19  0.436031  0.851476  0.437555  0.849315  5.413670e-06\n",
       "20  0.435643  0.851746  0.437106  0.849444  3.819569e-06\n",
       "21  0.435559  0.851526  0.437172  0.849405  2.703698e-06\n",
       "22  0.435450  0.851756  0.436869  0.849771  1.922589e-06\n",
       "23  0.435502  0.851614  0.437283  0.849548  1.375812e-06\n",
       "24  0.435203  0.851902  0.436628  0.849968  9.930685e-07\n",
       "25  0.435619  0.851492  0.436951  0.849581  7.251480e-07\n",
       "26  0.435407  0.851937  0.436381  0.849998  5.376036e-07\n",
       "27  0.435119  0.851912  0.436798  0.849782  4.063225e-07\n",
       "28  0.434981  0.852070  0.436400  0.849901  3.144258e-07\n",
       "29  0.435131  0.852004  0.436485  0.850024  2.500980e-07\n",
       "30  0.435195  0.851863  0.436463  0.849974  2.050686e-07\n",
       "31  0.435063  0.852033  0.436108  0.850104  1.735480e-07\n",
       "32  0.435353  0.852021  0.436386  0.850099  1.514836e-07\n",
       "33  0.435241  0.851817  0.436538  0.849862  1.360385e-07\n",
       "34  0.435307  0.851827  0.436869  0.849679  1.252270e-07\n",
       "35  0.434788  0.852262  0.436800  0.849794  1.176589e-07\n",
       "36  0.435442  0.851812  0.436310  0.850028  1.123612e-07\n",
       "37  0.435163  0.851902  0.436286  0.850110  1.086529e-07\n",
       "38  0.435392  0.851881  0.436840  0.849677  1.060570e-07\n",
       "39  0.435465  0.851602  0.436461  0.849984  1.042399e-07\n",
       "40  0.435199  0.851991  0.435742  0.850387  1.029679e-07\n",
       "41  0.435183  0.852003  0.436788  0.849734  1.020775e-07\n",
       "42  0.435488  0.851693  0.436243  0.850105  1.014543e-07\n",
       "43  0.435144  0.852068  0.436751  0.849733  1.010180e-07\n",
       "44  0.435056  0.852168  0.436906  0.849616  1.007126e-07\n",
       "45  0.435138  0.851996  0.436543  0.849788  1.004988e-07\n",
       "46  0.434918  0.852081  0.436490  0.849885  1.003492e-07\n",
       "47  0.435606  0.851753  0.436373  0.850227  1.002444e-07\n",
       "48  0.435318  0.851646  0.436612  0.849918  1.001711e-07\n",
       "49  0.435314  0.851886  0.436596  0.849802  1.001198e-07\n",
       "50  0.435244  0.851868  0.436800  0.849612  1.000838e-07"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historyFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best epoch: 40 | best loss: 0.43519923090934753 | best auc: 0.850387454032898\n"
     ]
    }
   ],
   "source": [
    "best_epoch = historyFrame.val_auc.argmax()\n",
    "best_loss = historyFrame.iloc[best_epoch].loss\n",
    "best_auc = historyFrame.iloc[best_epoch].val_auc\n",
    "print(\"best epoch:\", best_epoch,\n",
    "      \"| best loss:\", best_loss,\n",
    "      \"| best auc:\", best_auc\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Params.copy()\n",
    "result[\"bavg_epoch\"] = int(best_epoch)\n",
    "result[\"bavg_loss\"] = float(best_loss)\n",
    "result[\"bavg_auc\"] = float(best_auc)\n",
    "with open(f\"{MDL_PATH}/params.json\", \"w\") as file:\n",
    "    json.dump(result, file)\n",
    "\n",
    "if not os.path.exists(f\"{MDLS_PATH}/results.csv\"):\n",
    "    df_save = pd.DataFrame(result, index=[0])\n",
    "    df_save.to_csv(f\"{MDLS_PATH}/results.csv\")\n",
    "else:\n",
    "    df_old = pd.read_csv(f\"{MDLS_PATH}/results.csv\", index_col=0)\n",
    "    df_save = pd.DataFrame(result, index = [df_old.index.max() + 1])\n",
    "    df_save = df_old.append(df_save, ignore_index=True)\n",
    "    df_save.to_csv(f\"{MDLS_PATH}/results.csv\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>version</th>\n",
       "      <th>train_mode</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>bavg_epoch</th>\n",
       "      <th>bavg_loss</th>\n",
       "      <th>bavg_auc</th>\n",
       "      <th>changelog</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>1</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>16</td>\n",
       "      <td>0.451789</td>\n",
       "      <td>0.838065</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>13</td>\n",
       "      <td>0.449180</td>\n",
       "      <td>0.822957</td>\n",
       "      <td>moved all relu layers before the pooling layers</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>3</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>20</td>\n",
       "      <td>0.442866</td>\n",
       "      <td>0.837724</td>\n",
       "      <td>tried on large ds, since hight overfitting</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>5</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>13</td>\n",
       "      <td>0.449431</td>\n",
       "      <td>0.822973</td>\n",
       "      <td>added second layer to first block or reference</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>6</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>0.442229</td>\n",
       "      <td>0.820143</td>\n",
       "      <td>added relu activations to all conv layers</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>9</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>0.480765</td>\n",
       "      <td>0.812309</td>\n",
       "      <td>set all pool sizes to 1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>11</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>0.438630</td>\n",
       "      <td>0.819107</td>\n",
       "      <td>set all pool sizes to 8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>13</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>0.448575</td>\n",
       "      <td>0.820421</td>\n",
       "      <td>added second layer to first block with small k...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>14</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>0.457616</td>\n",
       "      <td>0.820876</td>\n",
       "      <td>added second layer to first block with small k...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>17</td>\n",
       "      <td>test</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>0.475783</td>\n",
       "      <td>0.814108</td>\n",
       "      <td>trial of completely different achitecture</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>33</td>\n",
       "      <td>full</td>\n",
       "      <td>64</td>\n",
       "      <td>60</td>\n",
       "      <td>13</td>\n",
       "      <td>0.457444</td>\n",
       "      <td>0.825847</td>\n",
       "      <td>trial of completely different achitecture</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00500</td>\n",
       "      <td>43</td>\n",
       "      <td>full</td>\n",
       "      <td>64</td>\n",
       "      <td>60</td>\n",
       "      <td>43</td>\n",
       "      <td>0.452197</td>\n",
       "      <td>0.836913</td>\n",
       "      <td>trial of completely different achitecture</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>47</td>\n",
       "      <td>full</td>\n",
       "      <td>64</td>\n",
       "      <td>60</td>\n",
       "      <td>9</td>\n",
       "      <td>0.452419</td>\n",
       "      <td>0.836464</td>\n",
       "      <td>model before was standard model with 0.001 no ...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>55</td>\n",
       "      <td>full</td>\n",
       "      <td>64</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>0.451851</td>\n",
       "      <td>0.833302</td>\n",
       "      <td>removed dropout layer in conv</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>57</td>\n",
       "      <td>full</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>0.434063</td>\n",
       "      <td>0.835922</td>\n",
       "      <td>before: removed 2 fully connected layers. now:...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>61</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.483809</td>\n",
       "      <td>0.824025</td>\n",
       "      <td>removed dropout, flatten, added lstm</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>64</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.465674</td>\n",
       "      <td>0.824023</td>\n",
       "      <td>removed 2 layers in larger block</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>66</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.460225</td>\n",
       "      <td>0.827190</td>\n",
       "      <td>added back one layer</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>67</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.454083</td>\n",
       "      <td>0.827726</td>\n",
       "      <td>third layer</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>68</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.455897</td>\n",
       "      <td>0.827700</td>\n",
       "      <td>doubled the first layer and added a second one...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>70</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.467193</td>\n",
       "      <td>0.832003</td>\n",
       "      <td>base model</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>72</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.483452</td>\n",
       "      <td>0.824098</td>\n",
       "      <td>took out the fully connected layers before the...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>73</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.472798</td>\n",
       "      <td>0.830427</td>\n",
       "      <td>added back one fully connected layer</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>75</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.454001</td>\n",
       "      <td>0.830381</td>\n",
       "      <td>doubled pool size, except for last, doubled nu...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>76</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.469206</td>\n",
       "      <td>0.832270</td>\n",
       "      <td>base model with same padding</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>78</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "      <td>0.442443</td>\n",
       "      <td>0.841306</td>\n",
       "      <td>doubled last layer in all blocks. lr/10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>79</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>25</td>\n",
       "      <td>0.439258</td>\n",
       "      <td>0.842578</td>\n",
       "      <td>doubled last layer in all blocks doubled size ...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>83</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>27</td>\n",
       "      <td>0.439051</td>\n",
       "      <td>0.841536</td>\n",
       "      <td>added dropout layers to dense layers</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>88</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.453092</td>\n",
       "      <td>0.834111</td>\n",
       "      <td>everything had 3 layers now and relu activation</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>92</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>17</td>\n",
       "      <td>0.434836</td>\n",
       "      <td>0.832301</td>\n",
       "      <td>dropout layers</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>110</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>11</td>\n",
       "      <td>0.470093</td>\n",
       "      <td>0.815836</td>\n",
       "      <td>dropout layers</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>113</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "      <td>0.476270</td>\n",
       "      <td>0.813889</td>\n",
       "      <td>dropout layers</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>119</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>36</td>\n",
       "      <td>0.463551</td>\n",
       "      <td>0.812370</td>\n",
       "      <td>new dataset using best model (base model with ...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.00020</td>\n",
       "      <td>124</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>23</td>\n",
       "      <td>0.448196</td>\n",
       "      <td>0.827187</td>\n",
       "      <td>10x lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>134</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>12</td>\n",
       "      <td>0.467185</td>\n",
       "      <td>0.826250</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>135</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>9</td>\n",
       "      <td>0.444164</td>\n",
       "      <td>0.848162</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>136</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>21</td>\n",
       "      <td>0.420500</td>\n",
       "      <td>0.854332</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>137</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>31</td>\n",
       "      <td>0.364450</td>\n",
       "      <td>0.876402</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>139</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.436394</td>\n",
       "      <td>0.845982</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>139</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>0.436394</td>\n",
       "      <td>0.845982</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>140</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>0.423539</td>\n",
       "      <td>0.844709</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>142</td>\n",
       "      <td>full</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.447673</td>\n",
       "      <td>0.836974</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>164</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>9</td>\n",
       "      <td>0.438891</td>\n",
       "      <td>0.844589</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>200</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>36</td>\n",
       "      <td>0.464103</td>\n",
       "      <td>0.839057</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>210</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>24</td>\n",
       "      <td>0.444001</td>\n",
       "      <td>0.847384</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>221</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "      <td>0.435199</td>\n",
       "      <td>0.850387</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lr  version train_mode  batch_size  epochs  bavg_epoch  bavg_loss  \\\n",
       "0   0.00100        1       full         256      60          16   0.451789   \n",
       "1   0.00100        2       test         256      60          13   0.449180   \n",
       "2   0.00100        3       full         256      60          20   0.442866   \n",
       "3   0.00100        5       test         256      60          13   0.449431   \n",
       "4   0.00100        6       test         256      30           9   0.442229   \n",
       "5   0.00100        9       test         256      30           4   0.480765   \n",
       "6   0.00100       11       test         256      30           8   0.438630   \n",
       "7   0.00100       13       test         256      30          12   0.448575   \n",
       "8   0.00010       14       test         256      30          12   0.457616   \n",
       "9   0.00010       17       test          64      30           6   0.475783   \n",
       "10  0.00010       33       full          64      60          13   0.457444   \n",
       "11  0.00500       43       full          64      60          43   0.452197   \n",
       "12  0.00010       47       full          64      60           9   0.452419   \n",
       "13  0.00010       55       full          64      60           4   0.451851   \n",
       "14  0.00010       57       full         128      20           9   0.434063   \n",
       "15  0.00010       61       full         256       3           2   0.483809   \n",
       "16  0.00010       64       full         256      10           7   0.465674   \n",
       "17  0.00010       66       full         256      10           6   0.460225   \n",
       "18  0.00010       67       full         256      10           7   0.454083   \n",
       "19  0.00010       68       full         256      10           7   0.455897   \n",
       "20  0.00100       70       full         256      10           9   0.467193   \n",
       "21  0.00100       72       full         256      10           9   0.483452   \n",
       "22  0.00100       73       full         256      10           9   0.472798   \n",
       "23  0.00010       75       full         256      10           9   0.454001   \n",
       "24  0.00100       76       full         256      10           6   0.469206   \n",
       "25  0.00010       78       full         256      40          12   0.442443   \n",
       "26  0.00050       79       full         256      40          25   0.439258   \n",
       "27  0.00050       83       full         256      40          27   0.439051   \n",
       "28  0.00010       88       full         256      40           8   0.453092   \n",
       "29  0.00010       92       full         256     100          17   0.434836   \n",
       "30  0.00010      110       full         256     100          11   0.470093   \n",
       "31  0.00010      113       full         256     100           8   0.476270   \n",
       "32  0.00002      119       full         256     100          36   0.463551   \n",
       "33  0.00020      124       full         256     100          23   0.448196   \n",
       "34  0.00010      134       full         256     100          12   0.467185   \n",
       "35  0.00010      135       full         256     100           9   0.444164   \n",
       "36  0.00010      136       full         256     100          21   0.420500   \n",
       "37  0.00010      137       full         256     100          31   0.364450   \n",
       "38  0.00010      139       full         256     100          10   0.436394   \n",
       "39  0.00010      139       full         256     100          10   0.436394   \n",
       "40  0.00010      140       full         256      15          12   0.423539   \n",
       "41  0.00010      142       full         128       5           4   0.447673   \n",
       "42  0.00010      164       full         256     100           9   0.438891   \n",
       "43  0.00001      200       full         256     100          36   0.464103   \n",
       "44  0.00100      210       full         256     100          24   0.444001   \n",
       "45  0.00010      221       full         256     100          40   0.435199   \n",
       "\n",
       "    bavg_auc                                          changelog  seed  \n",
       "0   0.838065                                                NaN   NaN  \n",
       "1   0.822957    moved all relu layers before the pooling layers   NaN  \n",
       "2   0.837724         tried on large ds, since hight overfitting   NaN  \n",
       "3   0.822973     added second layer to first block or reference   NaN  \n",
       "4   0.820143          added relu activations to all conv layers   NaN  \n",
       "5   0.812309                            set all pool sizes to 1   NaN  \n",
       "6   0.819107                            set all pool sizes to 8   NaN  \n",
       "7   0.820421  added second layer to first block with small k...   NaN  \n",
       "8   0.820876  added second layer to first block with small k...   NaN  \n",
       "9   0.814108          trial of completely different achitecture   NaN  \n",
       "10  0.825847          trial of completely different achitecture   NaN  \n",
       "11  0.836913          trial of completely different achitecture   NaN  \n",
       "12  0.836464  model before was standard model with 0.001 no ...  42.0  \n",
       "13  0.833302                      removed dropout layer in conv  42.0  \n",
       "14  0.835922  before: removed 2 fully connected layers. now:...  42.0  \n",
       "15  0.824025               removed dropout, flatten, added lstm  42.0  \n",
       "16  0.824023                   removed 2 layers in larger block  42.0  \n",
       "17  0.827190                               added back one layer  42.0  \n",
       "18  0.827726                                        third layer  42.0  \n",
       "19  0.827700  doubled the first layer and added a second one...  42.0  \n",
       "20  0.832003                                         base model  42.0  \n",
       "21  0.824098  took out the fully connected layers before the...  42.0  \n",
       "22  0.830427               added back one fully connected layer  42.0  \n",
       "23  0.830381  doubled pool size, except for last, doubled nu...  42.0  \n",
       "24  0.832270                       base model with same padding  42.0  \n",
       "25  0.841306            doubled last layer in all blocks. lr/10  42.0  \n",
       "26  0.842578  doubled last layer in all blocks doubled size ...  42.0  \n",
       "27  0.841536               added dropout layers to dense layers  42.0  \n",
       "28  0.834111    everything had 3 layers now and relu activation  42.0  \n",
       "29  0.832301                                     dropout layers  42.0  \n",
       "30  0.815836                                     dropout layers  42.0  \n",
       "31  0.813889                                     dropout layers  42.0  \n",
       "32  0.812370  new dataset using best model (base model with ...  42.0  \n",
       "33  0.827187                                             10x lr  42.0  \n",
       "34  0.826250                                             1/4 lr  42.0  \n",
       "35  0.848162                                             1/4 lr  42.0  \n",
       "36  0.854332                                             1/4 lr  42.0  \n",
       "37  0.876402                                             1/4 lr  42.0  \n",
       "38  0.845982                                             1/4 lr  42.0  \n",
       "39  0.845982                                             1/4 lr  42.0  \n",
       "40  0.844709                                             1/4 lr  42.0  \n",
       "41  0.836974                                             1/4 lr  42.0  \n",
       "42  0.844589                                             1/4 lr  42.0  \n",
       "43  0.839057                                             1/4 lr  42.0  \n",
       "44  0.847384                                             1/4 lr  42.0  \n",
       "45  0.850387                                             1/4 lr  69.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(f\"{MDLS_PATH}/results.csv\",index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7463"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_ds, val_ds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['../input/filtered-whitened-tfrec\\\\train_00.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_01.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_02.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_03.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_04.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_05.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_06.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_07.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_08.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_09.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_10.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_11.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_12.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_13.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_14.tfrec',\n",
       "        '../input/filtered-whitened-tfrec\\\\train_15.tfrec'], dtype='<U47'),\n",
       " array(['../input/whitened-tfrec\\\\train_whitened_00.tfrec',\n",
       "        '../input/whitened-tfrec\\\\train_whitened_01.tfrec',\n",
       "        '../input/whitened-tfrec\\\\train_whitened_02.tfrec',\n",
       "        '../input/whitened-tfrec\\\\train_whitened_03.tfrec',\n",
       "        '../input/whitened-tfrec\\\\train_whitened_04.tfrec',\n",
       "        '../input/whitened-tfrec\\\\train_whitened_05.tfrec',\n",
       "        '../input/whitened-tfrec\\\\train_whitened_06.tfrec',\n",
       "        '../input/whitened-tfrec\\\\train_whitened_07.tfrec',\n",
       "        '../input/whitened-tfrec\\\\train_whitened_08.tfrec',\n",
       "        '../input/whitened-tfrec\\\\train_whitened_09.tfrec',\n",
       "        '../input/whitened-tfrec\\\\train_whitened_10.tfrec',\n",
       "        '../input/whitened-tfrec\\\\train_whitened_11.tfrec',\n",
       "        '../input/whitened-tfrec\\\\train_whitened_12.tfrec',\n",
       "        '../input/whitened-tfrec\\\\train_whitened_13.tfrec',\n",
       "        '../input/whitened-tfrec\\\\train_whitened_14.tfrec',\n",
       "        '../input/whitened-tfrec\\\\train_whitened_15.tfrec'], dtype='<U47')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8560393182154675, 0.8589913199752228]\n"
     ]
    }
   ],
   "source": [
    "prediction_scores = []\n",
    "with strategy.scope():\n",
    "    model = tf.keras.models.load_model(f\"{MDL_PATH}/model_{Params['version']:03}.h5\")\n",
    "train_df = pd.read_csv(\"../input/g2net-gravitational-wave-detection/training_labels.csv\")\n",
    "for ds_ind in range(len(all_train_files)):\n",
    "    train_set = load_dataset(all_train_files[ds_ind], shuffle=False, ordered=True, labeled=True, repeat=False, return_labels=False)\n",
    "    prediction = model.predict(train_set)\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    prediction_scores.append(roc_auc_score(train_df.target, prediction))\n",
    "print(prediction_scores)\n",
    "best_pred = np.array(prediction_scores).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = load_dataset(test_files[best_pred], shuffle=False, ordered=True, labeled=False, repeat=False, return_labels=False)\n",
    "test_prediction = model.predict(test_set)\n",
    "sub = pd.read_csv(\"../input/g2net-gravitational-wave-detection/sample_submission.csv\")\n",
    "sub.target = test_prediction.flatten()\n",
    "sub.to_csv(f\"{MDL_PATH}/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
