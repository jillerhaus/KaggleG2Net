{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e80813",
   "metadata": {},
   "source": [
    "# Train Filter\n",
    "\n",
    "This notebook is designed to train the filter for the model. The filter is currently designed to take sample made up of a time series with three channels (one from each of the LIGO/Virgo detectors). Each of these samples contains 2s of either just noise or a gravitational wave overlayed with noise. The data is sampled at 2048 Hz.The filter will then try to reconstruct the gravitational wave. Or in the case of just noise, the ground truth is just a constant line at 0.\n",
    "\n",
    "The notebook is recommended for training on a GPU, only. This is because the LSTM cells incur significant CPU overhead, when not using the explicit CUDA implementation and a lot of overhead even then. And more significantly, the loss function needs to be calculated between two tensors of shape [4096,3], when training the model for a specific purpose (reproduce the gravitational-wave or reduce noise) other than detection, also producing very significant CPU overhead. This leads to the model taking about 30 hrs/epoch, when training on a TPU.\n",
    "\n",
    "\n",
    "### Reason for the filter\n",
    "\n",
    "The goal of the filter currently is to extract the gravitational wave burried in detector noise and reconstruct it.\n",
    "\n",
    "The long-term goal for the filter is to produce a time series, where the same possible gravitational wave signal is still present, but with reduced noise. This is an extrapolation from [this paper](https://arxiv.org/pdf/1908.03151.pdf). This paper shows, that the model, that was the basis for the actitecture of my detection model is extemely sensitive to signal to noise ratio(SNR). A generalization of this fact to all CNNs can be defended by the fact that the results in the competition appear to have a very hard cap of an AUC of about 0.88. To go above this threshold requires significant additional computational performance by using many different models in an ensemble. This only slightly appears to improve the score and is true for 2D CNNs as well as 1D CNNs and in ensembles of 1D and 2D CNNs. This small increase leads me to 2 conclusions:\n",
    "\n",
    "1. The different models with vastly different model architectures have the same samples, where they cannot make adequate predictions.\n",
    "2. The small increase of accuracy gained from ensembling large quantities of models means that 1. is only not true for a small number of samples, likely on the fringes of their effective SNRs.\n",
    "\n",
    "This leads me to my working theory, that since models appear to universally react very strongly to changes in the SNR, boosting the SNR is critical to improving model performance. Small improvements would likely lead to a huge gain in performance.\n",
    "\n",
    "For this reason the filter is designed to act analogously to active noise cancellation in headphones. Not eliminating all noise, but reducing the noise level to increase SNR. This is currently not what the filter is doing. \n",
    "\n",
    "But this is the first version and making the filter do what it should necessitates making a SNR-based loss function. Which will be done in the future. This model should then increase the efficacy of all detection models without bottelnecking the efficacy to the worst model in the stack, as is currently the case.\n",
    "\n",
    "All of this is, of course speculation and needs to be verified via tests.\n",
    "\n",
    "Regardless, reducing the noise should be more beneficial to the detection network than reproducing a noise-free version of the wave. In the latter case, the combined model (filter as input to the detection model) will be bottlenecked by the worse network, since both the filter and the body of the detecion network have a very similar purpose. Fine-tuning can somewhat negate this issue and allow the combined model to perform better than the two individual models, but will lead to a very unwieldy model, which can easily overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4acf71e",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f81b01",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80992384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 2080 Ti, compute capability 7.5\n"
     ]
    }
   ],
   "source": [
    "# General\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "from glob import glob\n",
    "import random\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "#ML\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision, layers\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1d7745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a180090a",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7107045",
   "metadata": {},
   "outputs": [],
   "source": [
    "Params = {\n",
    "    \"batch_size\": 128\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7bfa50",
   "metadata": {},
   "source": [
    "## Convenience Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30563088",
   "metadata": {},
   "source": [
    "### load_dataset()\n",
    "\n",
    "This function will load the dataset from different `tfrec` files. The datasets include a \"ground truth\", which is a time series containing either the un-whitened gravitational wave or a vector of 0. The ground truth tensors were not whitened, because the loss function used was the root mean squared error (RMSE), which lead to ill-posed models in cases where the signal was whitened, because in those cases there was only one time point with a high amplitude, compared to all others, that were very close to 0. RMSE was chosen, because in this case it was not necessary to reproduce the wavelet as exactly as possible, but to make a clear distinction between samples only containing noise and those with a signal. In the end the reproduced wavelets were very close to the ground truths regardless, at leas on the synthetically created datasets. These however turned out to be too far removed from the Kaggle dataset. The reasons for this are discussed in `train.ipynb`.\n",
    "\n",
    "#### Dataset Creation\n",
    "In order to create the datasets, I have altered the data creation code discussed in [this paper](https://arxiv.org/pdf/1904.08693.pdf) to allow for the use of Virgo data in the creation of synthetic datasets, as well as added some little convenience changes. The altered version should be available on my github shortly.\n",
    "\n",
    "The data creation process goes as follows:\n",
    "\n",
    "1. Real detector noise is downloaded. The choice to use real detector noise was made, because in numerous papers discussing machine learning applied to gravitational-wave signal detection, the result has always been, that deep learning networks perform much better on the colored Gaussian noise resulting from synthetic noise creation than on actual detector noise, even when transient \"glitches\" are injected into the noise.\n",
    "\n",
    "2. Half of the samples are then injected with gravitational wave signals from black hole mergers chosen at random from all possible parameter sets (weight of the black holes, spin, etc.). The injections were limited to only black hole mergers as their duration will sensibly fit into the 2s window of data present in the Kaggle dataset. The assumption that only black hole mergers are used in the Kaggle dataset may be wrong. The simulated signals are injected and their signal to noise ratios(SNRs) are calculated using matched filtering. They are then scaled to produce SNRs in the chosen window, which was 3-20 in the synthetic datasets. This, however turned out not to work as expected, because in the actual testing datasets where the simulated masses were further away from the detectors were significantly easier for models trained on the Kaggle dataset to make predictions on than those with a closer distance, even though the reported SNR was the same in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bc35bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_cut(x,y):\n",
    "    tensor = x\n",
    "    if random.random() > 0.65:\n",
    "        maxVal=128\n",
    "        dt = tf.random.uniform(shape=[],minval=2, maxval=maxVal, dtype=tf.int32)\n",
    "        t0 = tf.random.uniform(shape=[],minval=1, maxval=dt, dtype=tf.int32)\n",
    "        t1 = tf.random.uniform(shape=[],minval=0, maxval=t0, dtype=tf.int32)\n",
    "        paddings =  [\n",
    "            [0,0],\n",
    "            [t0,dt-t0], #[t1, dt-t1] if you want to move the resulting tensor randomly in the output tensor\n",
    "            [0,0]\n",
    "        ]\n",
    "        tensor = tf.pad(tensor[:,t0:t0+(4096-dt)], paddings=paddings)\n",
    "#     tensor = tensor * [-1. if random.random() > 0.5 else 1.,\n",
    "#                        -1. if random.random() > 0.5 else 1.,\n",
    "#                        -1. if random.random() > 0.5 else 1.]\n",
    "    # Necessary for TPU runtime\n",
    "    tensor = tf.reshape(tensor,[Params[\"batch_size\"], 4096, 3])\n",
    "    # Necessary for GPU runtime\n",
    "    tensor = tf.cast(tensor, tf.float32)\n",
    "    return tensor, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21882e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(files, cut=False):\n",
    "    dataset = tf.data.TFRecordDataset(files, num_parallel_reads = AUTO)\n",
    "    \n",
    "    def _parse_function(example_proto):\n",
    "        keys_to_feature = {}\n",
    "        keys_to_feature[\"TimeSeries\"] = tf.io.FixedLenFeature([4096,3], tf.float32)\n",
    "        keys_to_feature[\"GroundTruths\"] = tf.io.FixedLenFeature([4096,3], tf.float32)\n",
    "        \n",
    "        parsed_features = tf.io.parse_single_example(example_proto, keys_to_feature)\n",
    "        return parsed_features[\"TimeSeries\"], parsed_features[\"GroundTruths\"]\n",
    "    \n",
    "    dataset = dataset.map(_parse_function, num_parallel_calls=AUTO)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(buffer_size=10000, reshuffle_each_iteration=True)\n",
    "    dataset = dataset.batch(Params[\"batch_size\"])\n",
    "    if cut:\n",
    "        dataset = dataset.map(random_cut, num_parallel_calls=AUTO, deterministic=False)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167bac1a",
   "metadata": {},
   "source": [
    "### get_dataset_files()\n",
    "\n",
    "This function divides the files into a training and validation dataset. The function can be given distances to choose from, because in testing it was shown, that even though the signal to noise ratio should be the same at all distances, the datasets with a higher distance between the detector and the event were much harder(distance in the filename is in Mpc). In the end the filter is meant to be trained using curriculum learning, where the dataset is made increasingly more difficult as training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e38d9a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_files(distances):\n",
    "    train_files = []\n",
    "    val_files = []\n",
    "    for distance in distances:\n",
    "        data_files = glob(f\"../input/synthetic-tfrec/train_{distance}*.tfrec\")\n",
    "        train_files.extend(data_files[:-1])\n",
    "        val_files.extend(data_files[-1:])\n",
    "    return train_files, val_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a434860",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../input/synthetic-tfrec\\\\train_350_1_no_whiten_filter_12500_00.tfrec',\n",
       " '../input/synthetic-tfrec\\\\train_350_1_no_whiten_filter_12500_01.tfrec',\n",
       " '../input/synthetic-tfrec\\\\train_350_1_no_whiten_filter_12500_02.tfrec',\n",
       " '../input/synthetic-tfrec\\\\train_350_1_no_whiten_filter_12500_03.tfrec',\n",
       " '../input/synthetic-tfrec\\\\train_350_2_no_whiten_filter_12500_00.tfrec',\n",
       " '../input/synthetic-tfrec\\\\train_350_2_no_whiten_filter_12500_01.tfrec',\n",
       " '../input/synthetic-tfrec\\\\train_350_2_no_whiten_filter_12500_02.tfrec',\n",
       " '../input/synthetic-tfrec\\\\train_350_2_no_whiten_filter_12500_03.tfrec',\n",
       " '../input/synthetic-tfrec\\\\train_350_2_no_whiten_filter_12500_04.tfrec',\n",
       " '../input/synthetic-tfrec\\\\train_350_2_no_whiten_filter_12500_05.tfrec',\n",
       " '../input/synthetic-tfrec\\\\train_350_2_no_whiten_filter_12500_06.tfrec']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files,val_files = get_dataset_files([100,150,200,250,300,350])\n",
    "train_ds = load_dataset(train_files)\n",
    "val_ds = load_dataset(val_files)\n",
    "train_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559dd08a",
   "metadata": {},
   "source": [
    "## Train\n",
    "### Model\n",
    "\n",
    "The first and current architecture used for the filter is a 3-channel version of the architecture proposed in [this paper](https://arxiv.org/abs/2105.03073). It is a encoder, decoder model, where the encoding is done using a simple 1D CNN and the decoding is done using a series of bidirectional LSTM layers. Unfortunately, so far I have not yet had the chance to experiment much with the architecture, because my initial datasets have proven insufficient for training (too easy, pre-processing is different to that of Kaggle dataset). I am currently making better datasets to be able to experiment more with training the filter.\n",
    "\n",
    "The model is trained using root mean squared error between the predicted wave and the ground truth. This produces a model, that can reliably reproduce an approximation of the wave. The goal of the filter is, however to reduce the noise, increasing the SNR, so a different loss function will have to be created for that use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f3503",
   "metadata": {},
   "source": [
    "This custom initializer is meant as an initializer for a 1D convolutional layer to create a windowing effect, i.e. [x1,x2,x3,x4,x5,x5] => [[x1,x2,x3,x4],[x2,x3,x4,x5], ...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cf72642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1, 4), dtype=float32, numpy=\n",
       "array([[[1., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ownInitializer(shape, dtype=None):\n",
    "    return tf.constant([\n",
    "        [[1,0,0,0]],\n",
    "        [[0,1,0,0]],\n",
    "        [[0,0,1,0]],\n",
    "        [[0,0,0,1]]\n",
    "    ],dtype=dtype)\n",
    "\n",
    "ownInitializer(2,tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6eb57e",
   "metadata": {},
   "source": [
    "Same principle but with 3 channels,which are added together. due to the loss of information this would produce, I decided to use a trainable and randomly initialized first layer. This was also done in large part due to time-constraints and will be re-visited."
   ]
  },
  {
   "cell_type": "raw",
   "id": "fcf8ddd4",
   "metadata": {},
   "source": [
    "def ownInitializer(shape, dtype=None):\n",
    "    return tf.constant([\n",
    "        [[1,0,0,0],[1,0,0,0],[1,0,0,0]],\n",
    "        [[0,1,0,0],[0,1,0,0],[0,1,0,0]],\n",
    "        [[0,0,1,0],[0,0,1,0],[0,0,1,0]],\n",
    "        [[0,0,0,1],[0,0,0,1],[0,0,0,1]]\n",
    "    ],dtype=dtype)\n",
    "\n",
    "ownInitializer(2,tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55a3c709",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\apist\\AppData\\Local\\Temp/ipykernel_29068/867815235.py:16: The name tf.keras.layers.CuDNNLSTM is deprecated. Please use tf.compat.v1.keras.layers.CuDNNLSTM instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 4096, 12)          156       \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4096, 12, 1)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 4096, 12, 32)      128       \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 4096, 6, 32)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 4096, 6, 16)       1552      \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 4096, 96)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 4096, 256)         231424    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 4096, 256)         395264    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 4096, 256)         395264    \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 4096, 3)           771       \n",
      "=================================================================\n",
      "Total params: 1,024,559\n",
      "Trainable params: 1,024,559\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape = [4096,3]),\n",
    "#         layers.Conv1D(kernel_size=12, filters=4,\n",
    "#                        kernel_initializer=ownInitializer, input_shape=[4096,3], trainable=True, padding=\"same\"),\n",
    "    layers.Conv1D(kernel_size=4, filters=12, padding=\"same\"),\n",
    "#     layers.ZeroPadding1D(padding=[0,3]),\n",
    "    layers.Reshape([4096,12,1]),\n",
    "    layers.TimeDistributed(layers.Conv1D(kernel_size=3, filters=32, activation=\"tanh\", padding=\"same\")),\n",
    "#     layers.TimeDistributed(layers.Conv1D(kernel_size=5, filters=32, activation=\"tanh\", padding=\"same\")),\n",
    "    \n",
    "    layers.TimeDistributed(layers.MaxPool1D()),\n",
    "    layers.TimeDistributed(layers.Conv1D(kernel_size=3, filters=16, activation=\"tanh\", padding=\"same\")),\n",
    "    layers.TimeDistributed(layers.Flatten()),\n",
    "    \n",
    "#     layers.Reshape([4093,32]),\n",
    "    layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(128, return_sequences=True)),\n",
    "    layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(128, return_sequences=True)),\n",
    "    layers.Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(128, return_sequences=True)),\n",
    "    \n",
    "# These layers need to be used when training on a TPU.\n",
    "#     layers.Bidirectional(layers.LSTM(128, return_sequences=True)),\n",
    "#     layers.Bidirectional(layers.LSTM(128, return_sequences=True)),\n",
    "#     layers.Bidirectional(layers.LSTM(128, return_sequences=True)),\n",
    "    \n",
    "    \n",
    "#     layers.Bidirectional(layers.LSTM(400)),\n",
    "    \n",
    "    \n",
    "    layers.TimeDistributed(layers.Dense(3, dtype=tf.float32)),\n",
    "])\n",
    "opt = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    metrics=[\"mean_squared_error\", \"cosine_similarity\"],\n",
    "    loss=\"mean_squared_error\"\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cab5dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = len(train_files) * 12500 // Params[\"batch_size\"]\n",
    "val_steps = len(val_files) * 12500 // Params[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8068783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import *\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.2,\n",
    "    patience=3, min_lr = 0.000001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     f\"{MDL_PATH}/model_{Params['version']:03}.h5\",\n",
    "    \"./model4.h5\",\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    "    save_weight_only=False,\n",
    "    mode=\"auto\",\n",
    "    save_freq=\"epoch\"\n",
    ")\n",
    "\n",
    "\n",
    "callbacks=[reduce_lr, early_stop, model_checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d62454",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85730b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "1074/1074 [==============================] - 1201s 1s/step - loss: 0.0114 - mean_squared_error: 0.0114 - cosine_similarity: 0.0146 - val_loss: 0.0112 - val_mean_squared_error: 0.0112 - val_cosine_similarity: 0.0181\n",
      "Epoch 2/60\n",
      "1074/1074 [==============================] - 1162s 1s/step - loss: 0.0113 - mean_squared_error: 0.0113 - cosine_similarity: 0.0130 - val_loss: 0.0112 - val_mean_squared_error: 0.0112 - val_cosine_similarity: 0.0051\n",
      "Epoch 3/60\n",
      "1074/1074 [==============================] - 1166s 1s/step - loss: 0.0115 - mean_squared_error: 0.0115 - cosine_similarity: 0.0055 - val_loss: 0.0112 - val_mean_squared_error: 0.0112 - val_cosine_similarity: 0.0046\n",
      "Epoch 4/60\n",
      "1074/1074 [==============================] - 1177s 1s/step - loss: 0.0114 - mean_squared_error: 0.0114 - cosine_similarity: 0.0095 - val_loss: 0.0111 - val_mean_squared_error: 0.0111 - val_cosine_similarity: 0.0107\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 5/60\n",
      "1074/1074 [==============================] - 1175s 1s/step - loss: 0.0107 - mean_squared_error: 0.0107 - cosine_similarity: 0.0194 - val_loss: 0.0106 - val_mean_squared_error: 0.0106 - val_cosine_similarity: 0.0193\n",
      "Epoch 6/60\n",
      "1074/1074 [==============================] - 1177s 1s/step - loss: 0.0103 - mean_squared_error: 0.0103 - cosine_similarity: 0.0246 - val_loss: 0.0104 - val_mean_squared_error: 0.0104 - val_cosine_similarity: 0.0224\n",
      "Epoch 7/60\n",
      "  94/1074 [=>............................] - ETA: 18:58 - loss: 0.0101 - mean_squared_error: 0.0101 - cosine_similarity: 0.0256"
     ]
    }
   ],
   "source": [
    "model.fit(train_ds,validation_data=val_ds, validation_steps=val_steps, steps_per_epoch=train_steps,\n",
    "          epochs=60, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2c1a7a",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff7716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = model.predict(train_ds, steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901ae492",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538ff0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in range(10,50):\n",
    "    \n",
    "    plt.plot(predictions[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633ddb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_ds:\n",
    "    plt.plot(i[1][7])\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a926852",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model5.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00baaa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
