{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-15T15:02:24.741032Z",
     "iopub.status.busy": "2021-09-15T15:02:24.740348Z",
     "iopub.status.idle": "2021-09-15T15:02:24.76936Z",
     "shell.execute_reply": "2021-09-15T15:02:24.768408Z",
     "shell.execute_reply.started": "2021-09-15T15:02:24.740923Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KAGGLE: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "KAGGLE = True\n",
    "if os.name == \"nt\":\n",
    "    KAGGLE = False\n",
    "print(f\"KAGGLE: {KAGGLE}\")\n",
    "if not KAGGLE:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-15T15:02:24.771135Z",
     "iopub.status.busy": "2021-09-15T15:02:24.770889Z",
     "iopub.status.idle": "2021-09-15T15:02:31.370857Z",
     "shell.execute_reply": "2021-09-15T15:02:31.369919Z",
     "shell.execute_reply.started": "2021-09-15T15:02:24.771106Z"
    }
   },
   "outputs": [],
   "source": [
    "# general\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "from glob import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from IPython.core.display import display, HTML\n",
    "if KAGGLE:\n",
    "    from kaggle_datasets import KaggleDatasets\n",
    "\n",
    "# ML\n",
    "\n",
    "# DL\n",
    "import tensorflow as tf\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-15T15:02:31.372548Z",
     "iopub.status.busy": "2021-09-15T15:02:31.3722Z",
     "iopub.status.idle": "2021-09-15T15:02:31.385729Z",
     "shell.execute_reply": "2021-09-15T15:02:31.384962Z",
     "shell.execute_reply.started": "2021-09-15T15:02:31.372518Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tf.compat.v1.disable_eager_execution()\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-15T15:02:31.388574Z",
     "iopub.status.busy": "2021-09-15T15:02:31.387748Z",
     "iopub.status.idle": "2021-09-15T15:02:31.395949Z",
     "shell.execute_reply": "2021-09-15T15:02:31.39525Z",
     "shell.execute_reply.started": "2021-09-15T15:02:31.388533Z"
    }
   },
   "outputs": [],
   "source": [
    "def auto_select_accelerator():\n",
    "    TPU_DETECTED = False\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "        tf.keras.mixed_precision.set_global_policy(\"mixed_bfloat16\")\n",
    "        print(f\"Running on TPU: {tpu.master()}\")\n",
    "        TPU_DETECTED = True\n",
    "    except ValueError:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "        tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    num_replicas = strategy.num_replicas_in_sync\n",
    "    print(f\"Running on {num_replicas} replica{'s' if num_replicas > 1 else ''}\")\n",
    "    return strategy, TPU_DETECTED, num_replicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-15T15:02:31.397819Z",
     "iopub.status.busy": "2021-09-15T15:02:31.397375Z",
     "iopub.status.idle": "2021-09-15T15:02:31.4092Z",
     "shell.execute_reply": "2021-09-15T15:02:31.408603Z",
     "shell.execute_reply.started": "2021-09-15T15:02:31.397773Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>version</th>\n",
       "      <th>train_mode</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>bavg_epoch</th>\n",
       "      <th>bavg_loss</th>\n",
       "      <th>bavg_auc</th>\n",
       "      <th>changelog</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>1</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>16</td>\n",
       "      <td>0.451789</td>\n",
       "      <td>0.838065</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>13</td>\n",
       "      <td>0.449180</td>\n",
       "      <td>0.822957</td>\n",
       "      <td>moved all relu layers before the pooling layers</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>3</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>20</td>\n",
       "      <td>0.442866</td>\n",
       "      <td>0.837724</td>\n",
       "      <td>tried on large ds, since hight overfitting</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>5</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>13</td>\n",
       "      <td>0.449431</td>\n",
       "      <td>0.822973</td>\n",
       "      <td>added second layer to first block or reference</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>6</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>0.442229</td>\n",
       "      <td>0.820143</td>\n",
       "      <td>added relu activations to all conv layers</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>9</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>0.480765</td>\n",
       "      <td>0.812309</td>\n",
       "      <td>set all pool sizes to 1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>11</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>0.438630</td>\n",
       "      <td>0.819107</td>\n",
       "      <td>set all pool sizes to 8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>13</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>0.448575</td>\n",
       "      <td>0.820421</td>\n",
       "      <td>added second layer to first block with small k...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>14</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>0.457616</td>\n",
       "      <td>0.820876</td>\n",
       "      <td>added second layer to first block with small k...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>17</td>\n",
       "      <td>test</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>0.475783</td>\n",
       "      <td>0.814108</td>\n",
       "      <td>trial of completely different achitecture</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>33</td>\n",
       "      <td>full</td>\n",
       "      <td>64</td>\n",
       "      <td>60</td>\n",
       "      <td>13</td>\n",
       "      <td>0.457444</td>\n",
       "      <td>0.825847</td>\n",
       "      <td>trial of completely different achitecture</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00500</td>\n",
       "      <td>43</td>\n",
       "      <td>full</td>\n",
       "      <td>64</td>\n",
       "      <td>60</td>\n",
       "      <td>43</td>\n",
       "      <td>0.452197</td>\n",
       "      <td>0.836913</td>\n",
       "      <td>trial of completely different achitecture</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>47</td>\n",
       "      <td>full</td>\n",
       "      <td>64</td>\n",
       "      <td>60</td>\n",
       "      <td>9</td>\n",
       "      <td>0.452419</td>\n",
       "      <td>0.836464</td>\n",
       "      <td>model before was standard model with 0.001 no ...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>55</td>\n",
       "      <td>full</td>\n",
       "      <td>64</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>0.451851</td>\n",
       "      <td>0.833302</td>\n",
       "      <td>removed dropout layer in conv</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>57</td>\n",
       "      <td>full</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>0.434063</td>\n",
       "      <td>0.835922</td>\n",
       "      <td>before: removed 2 fully connected layers. now:...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>61</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.483809</td>\n",
       "      <td>0.824025</td>\n",
       "      <td>removed dropout, flatten, added lstm</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>64</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.465674</td>\n",
       "      <td>0.824023</td>\n",
       "      <td>removed 2 layers in larger block</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>66</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.460225</td>\n",
       "      <td>0.827190</td>\n",
       "      <td>added back one layer</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>67</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.454083</td>\n",
       "      <td>0.827726</td>\n",
       "      <td>third layer</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>68</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.455897</td>\n",
       "      <td>0.827700</td>\n",
       "      <td>doubled the first layer and added a second one...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>70</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.467193</td>\n",
       "      <td>0.832003</td>\n",
       "      <td>base model</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>72</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.483452</td>\n",
       "      <td>0.824098</td>\n",
       "      <td>took out the fully connected layers before the...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>73</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.472798</td>\n",
       "      <td>0.830427</td>\n",
       "      <td>added back one fully connected layer</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>75</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.454001</td>\n",
       "      <td>0.830381</td>\n",
       "      <td>doubled pool size, except for last, doubled nu...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>76</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.469206</td>\n",
       "      <td>0.832270</td>\n",
       "      <td>base model with same padding</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>78</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "      <td>0.442443</td>\n",
       "      <td>0.841306</td>\n",
       "      <td>doubled last layer in all blocks. lr/10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>79</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>25</td>\n",
       "      <td>0.439258</td>\n",
       "      <td>0.842578</td>\n",
       "      <td>doubled last layer in all blocks doubled size ...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>83</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>27</td>\n",
       "      <td>0.439051</td>\n",
       "      <td>0.841536</td>\n",
       "      <td>added dropout layers to dense layers</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>88</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.453092</td>\n",
       "      <td>0.834111</td>\n",
       "      <td>everything had 3 layers now and relu activation</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>92</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>17</td>\n",
       "      <td>0.434836</td>\n",
       "      <td>0.832301</td>\n",
       "      <td>dropout layers</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>110</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>11</td>\n",
       "      <td>0.470093</td>\n",
       "      <td>0.815836</td>\n",
       "      <td>dropout layers</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>113</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "      <td>0.476270</td>\n",
       "      <td>0.813889</td>\n",
       "      <td>dropout layers</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>119</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>36</td>\n",
       "      <td>0.463551</td>\n",
       "      <td>0.812370</td>\n",
       "      <td>new dataset using best model (base model with ...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.00020</td>\n",
       "      <td>124</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>23</td>\n",
       "      <td>0.448196</td>\n",
       "      <td>0.827187</td>\n",
       "      <td>10x lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>134</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>12</td>\n",
       "      <td>0.467185</td>\n",
       "      <td>0.826250</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>135</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>9</td>\n",
       "      <td>0.444164</td>\n",
       "      <td>0.848162</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>136</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>21</td>\n",
       "      <td>0.420500</td>\n",
       "      <td>0.854332</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lr  version train_mode  batch_size  epochs  bavg_epoch  bavg_loss  \\\n",
       "0   0.00100        1       full         256      60          16   0.451789   \n",
       "1   0.00100        2       test         256      60          13   0.449180   \n",
       "2   0.00100        3       full         256      60          20   0.442866   \n",
       "3   0.00100        5       test         256      60          13   0.449431   \n",
       "4   0.00100        6       test         256      30           9   0.442229   \n",
       "5   0.00100        9       test         256      30           4   0.480765   \n",
       "6   0.00100       11       test         256      30           8   0.438630   \n",
       "7   0.00100       13       test         256      30          12   0.448575   \n",
       "8   0.00010       14       test         256      30          12   0.457616   \n",
       "9   0.00010       17       test          64      30           6   0.475783   \n",
       "10  0.00010       33       full          64      60          13   0.457444   \n",
       "11  0.00500       43       full          64      60          43   0.452197   \n",
       "12  0.00010       47       full          64      60           9   0.452419   \n",
       "13  0.00010       55       full          64      60           4   0.451851   \n",
       "14  0.00010       57       full         128      20           9   0.434063   \n",
       "15  0.00010       61       full         256       3           2   0.483809   \n",
       "16  0.00010       64       full         256      10           7   0.465674   \n",
       "17  0.00010       66       full         256      10           6   0.460225   \n",
       "18  0.00010       67       full         256      10           7   0.454083   \n",
       "19  0.00010       68       full         256      10           7   0.455897   \n",
       "20  0.00100       70       full         256      10           9   0.467193   \n",
       "21  0.00100       72       full         256      10           9   0.483452   \n",
       "22  0.00100       73       full         256      10           9   0.472798   \n",
       "23  0.00010       75       full         256      10           9   0.454001   \n",
       "24  0.00100       76       full         256      10           6   0.469206   \n",
       "25  0.00010       78       full         256      40          12   0.442443   \n",
       "26  0.00050       79       full         256      40          25   0.439258   \n",
       "27  0.00050       83       full         256      40          27   0.439051   \n",
       "28  0.00010       88       full         256      40           8   0.453092   \n",
       "29  0.00010       92       full         256     100          17   0.434836   \n",
       "30  0.00010      110       full         256     100          11   0.470093   \n",
       "31  0.00010      113       full         256     100           8   0.476270   \n",
       "32  0.00002      119       full         256     100          36   0.463551   \n",
       "33  0.00020      124       full         256     100          23   0.448196   \n",
       "34  0.00010      134       full         256     100          12   0.467185   \n",
       "35  0.00010      135       full         256     100           9   0.444164   \n",
       "36  0.00010      136       full         256     100          21   0.420500   \n",
       "\n",
       "    bavg_auc                                          changelog  seed  \n",
       "0   0.838065                                                NaN   NaN  \n",
       "1   0.822957    moved all relu layers before the pooling layers   NaN  \n",
       "2   0.837724         tried on large ds, since hight overfitting   NaN  \n",
       "3   0.822973     added second layer to first block or reference   NaN  \n",
       "4   0.820143          added relu activations to all conv layers   NaN  \n",
       "5   0.812309                            set all pool sizes to 1   NaN  \n",
       "6   0.819107                            set all pool sizes to 8   NaN  \n",
       "7   0.820421  added second layer to first block with small k...   NaN  \n",
       "8   0.820876  added second layer to first block with small k...   NaN  \n",
       "9   0.814108          trial of completely different achitecture   NaN  \n",
       "10  0.825847          trial of completely different achitecture   NaN  \n",
       "11  0.836913          trial of completely different achitecture   NaN  \n",
       "12  0.836464  model before was standard model with 0.001 no ...  42.0  \n",
       "13  0.833302                      removed dropout layer in conv  42.0  \n",
       "14  0.835922  before: removed 2 fully connected layers. now:...  42.0  \n",
       "15  0.824025               removed dropout, flatten, added lstm  42.0  \n",
       "16  0.824023                   removed 2 layers in larger block  42.0  \n",
       "17  0.827190                               added back one layer  42.0  \n",
       "18  0.827726                                        third layer  42.0  \n",
       "19  0.827700  doubled the first layer and added a second one...  42.0  \n",
       "20  0.832003                                         base model  42.0  \n",
       "21  0.824098  took out the fully connected layers before the...  42.0  \n",
       "22  0.830427               added back one fully connected layer  42.0  \n",
       "23  0.830381  doubled pool size, except for last, doubled nu...  42.0  \n",
       "24  0.832270                       base model with same padding  42.0  \n",
       "25  0.841306            doubled last layer in all blocks. lr/10  42.0  \n",
       "26  0.842578  doubled last layer in all blocks doubled size ...  42.0  \n",
       "27  0.841536               added dropout layers to dense layers  42.0  \n",
       "28  0.834111    everything had 3 layers now and relu activation  42.0  \n",
       "29  0.832301                                     dropout layers  42.0  \n",
       "30  0.815836                                     dropout layers  42.0  \n",
       "31  0.813889                                     dropout layers  42.0  \n",
       "32  0.812370  new dataset using best model (base model with ...  42.0  \n",
       "33  0.827187                                             10x lr  42.0  \n",
       "34  0.826250                                             1/4 lr  42.0  \n",
       "35  0.848162                                             1/4 lr  42.0  \n",
       "36  0.854332                                             1/4 lr  42.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if not KAGGLE:\n",
    "pd.read_csv(\"../models/results.csv\", index_col=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-15T15:02:31.410867Z",
     "iopub.status.busy": "2021-09-15T15:02:31.410426Z",
     "iopub.status.idle": "2021-09-15T15:02:37.49379Z",
     "shell.execute_reply": "2021-09-15T15:02:37.492936Z",
     "shell.execute_reply.started": "2021-09-15T15:02:31.410829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3090, compute capability 8.6\n",
      "Running on 1 replica\n"
     ]
    }
   ],
   "source": [
    "strategy, TPU_Detected, REPLICAS = auto_select_accelerator()\n",
    "INPUT_DIR = \"../input/g2net-gravitational-wave-detection\"\n",
    "MDLS_PATH = \".\" if KAGGLE else \"../models\"\n",
    "TRAIN_FILES_PATH = \"../input/filtered*_tfrec\"\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-15T15:02:37.495148Z",
     "iopub.status.busy": "2021-09-15T15:02:37.494768Z",
     "iopub.status.idle": "2021-09-15T15:02:37.719147Z",
     "shell.execute_reply": "2021-09-15T15:02:37.718274Z",
     "shell.execute_reply.started": "2021-09-15T15:02:37.495118Z"
    }
   },
   "outputs": [],
   "source": [
    "if KAGGLE:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    user_credential = user_secrets.get_gcloud_credential()\n",
    "    user_secrets.set_tensorflow_credential(user_credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-15T15:02:37.721115Z",
     "iopub.status.busy": "2021-09-15T15:02:37.720819Z",
     "iopub.status.idle": "2021-09-15T15:02:37.728148Z",
     "shell.execute_reply": "2021-09-15T15:02:37.727068Z",
     "shell.execute_reply.started": "2021-09-15T15:02:37.721075Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"{MDLS_PATH}/results.csv\"):\n",
    "    VER = 1\n",
    "else:\n",
    "    results = pd.read_csv(f\"{MDLS_PATH}/results.csv\", index_col=[0])\n",
    "    VER = int(results.version.max())\n",
    "Params ={\n",
    "    \"lr\": 0.0001 * REPLICAS,\n",
    "    \"version\": VER,\n",
    "    \"train_mode\": \"full\", #test, full\n",
    "    \"batch_size\": 256 * REPLICAS,\n",
    "    \"epochs\":100,\n",
    "    \"seed\": 42,\n",
    "    \"changelog\": \"1/4 lr\",\n",
    "}\n",
    "seed_everything(Params[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-15T15:02:37.730162Z",
     "iopub.status.busy": "2021-09-15T15:02:37.729642Z",
     "iopub.status.idle": "2021-09-15T15:02:37.739448Z",
     "shell.execute_reply": "2021-09-15T15:02:37.73844Z",
     "shell.execute_reply.started": "2021-09-15T15:02:37.730121Z"
    }
   },
   "outputs": [],
   "source": [
    "VER = Params[\"version\"]\n",
    "MDL_PATH = f\"{MDLS_PATH}/models_v{VER:03}\"\n",
    "while os.path.exists(MDL_PATH):\n",
    "    VER += 1\n",
    "    MDL_PATH = f\"{MDLS_PATH}/models_v{VER:03}\"\n",
    "Params[\"version\"]=VER\n",
    "os.mkdir(MDL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-15T15:02:37.743841Z",
     "iopub.status.busy": "2021-09-15T15:02:37.743614Z",
     "iopub.status.idle": "2021-09-15T15:02:37.75515Z",
     "shell.execute_reply": "2021-09-15T15:02:37.754062Z",
     "shell.execute_reply.started": "2021-09-15T15:02:37.743817Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset(files, shuffle=True, ordered=False, labeled = True, repeat=True):\n",
    "    dataset = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n",
    "\n",
    "    def _parse_function(example_proto):\n",
    "        if labeled:\n",
    "            keys_to_feature = {\n",
    "                \"TimeSeries\":tf.io.FixedLenFeature([4096,3],tf.float32),\n",
    "                \"Target\":tf.io.FixedLenFeature([], tf.int64, default_value=0)}\n",
    "        else:\n",
    "            keys_to_feature = {\n",
    "                \"TimeSeries\": tf.io.FixedLenFeature([4096,3],tf.float32)\n",
    "            }\n",
    "        parsed_features = tf.io.parse_single_example(example_proto, keys_to_feature)\n",
    "        return parsed_features[\"TimeSeries\"], parsed_features[\"Target\"] if labeled else parsed_features[\"TimeSeries\"]\n",
    "    \n",
    "    if not ordered:\n",
    "        ignore_order = tf.data.Options()\n",
    "        ignore_order.experimental_deterministic=False\n",
    "        dataset = dataset.with_options(ignore_order)\n",
    "    # parse the record into tensors.\n",
    "    dataset = dataset.map(_parse_function, num_parallel_calls=AUTO)\n",
    "    dataset = dataset.cache()\n",
    "\n",
    "    # shuffle the dataset\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=10000)\n",
    "\n",
    "    # Repeat the input infinitely\n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "\n",
    "\n",
    "    # Generate batches\n",
    "    dataset = dataset.batch(Params[\"batch_size\"])\n",
    "    dataset = dataset.prefetch(-1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-15T15:02:37.757333Z",
     "iopub.status.busy": "2021-09-15T15:02:37.756912Z",
     "iopub.status.idle": "2021-09-15T15:02:37.770476Z",
     "shell.execute_reply": "2021-09-15T15:02:37.769665Z",
     "shell.execute_reply.started": "2021-09-15T15:02:37.75725Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../input\\\\filtered_tfrec\\\\test.tfrec',\n",
       " '../input\\\\filtered_tfrec\\\\test_0.tfrec',\n",
       " '../input\\\\filtered_tfrec\\\\test_1.tfrec',\n",
       " '../input\\\\filtered_tfrec\\\\test_2.tfrec',\n",
       " '../input\\\\filtered_tfrec\\\\test_3.tfrec',\n",
       " '../input\\\\filtered_tfrec\\\\test_4.tfrec',\n",
       " '../input\\\\filtered_tfrec\\\\test_5.tfrec',\n",
       " '../input\\\\filtered_tfrec\\\\test_6.tfrec',\n",
       " '../input\\\\filtered_tfrec\\\\test_7.tfrec',\n",
       " '../input\\\\filtered_tfrec\\\\train_0.tfrec',\n",
       " '../input\\\\filtered_tfrec\\\\train_1.tfrec',\n",
       " '../input\\\\filtered_tfrec\\\\train_10.tfrec',\n",
       " '../input\\\\filtered_tfrec\\\\train_11.tfrec',\n",
       " '../input\\\\filtered_tfrec\\\\train_12.tfrec',\n",
       " '../input\\\\filtered_tfrec\\\\train_13.tfrec',\n",
       " '../input\\\\filtered_tfrec\\\\train_14.tfrec',\n",
       " '../input\\\\filtered_tfrec\\\\train_15.tfrec',\n",
       " '../input\\\\filtered_tfrec\\\\train_2.tfrec',\n",
       " '../input\\\\filtered_tfrec\\\\train_3.tfrec',\n",
       " '../input\\\\filtered_tfrec\\\\train_4.tfrec',\n",
       " '../input\\\\filtered_tfrec\\\\train_5.tfrec',\n",
       " '../input\\\\filtered_tfrec\\\\train_6.tfrec',\n",
       " '../input\\\\filtered_tfrec\\\\train_7.tfrec',\n",
       " '../input\\\\filtered_tfrec\\\\train_8.tfrec',\n",
       " '../input\\\\filtered_tfrec\\\\train_9.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\test.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\test_0.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\test_1.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\test_2.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\test_3.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\test_4.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\test_5.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\test_6.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\test_7.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_0.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_01.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_02.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_03.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_04.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_05.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_06.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_07.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_08.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_09.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_0_inv.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_10.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_10_inv.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_11.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_11_inv.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_12.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_12_inv.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_13.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_13_inv.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_14.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_14_inv.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_15.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_15_inv.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_1_inv.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_2_inv.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_3_inv.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_4_inv.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_5_inv.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_6_inv.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_7_inv.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_8_inv.tfrec',\n",
       " '../input\\\\filtered_whitened_tfrec\\\\train_9_inv.tfrec']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob(f\"{TRAIN_FILES_PATH}/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-15T15:02:37.771951Z",
     "iopub.status.busy": "2021-09-15T15:02:37.771726Z",
     "iopub.status.idle": "2021-09-15T15:02:39.668641Z",
     "shell.execute_reply": "2021-09-15T15:02:39.667667Z",
     "shell.execute_reply.started": "2021-09-15T15:02:37.771928Z"
    }
   },
   "outputs": [],
   "source": [
    "if KAGGLE:\n",
    "    TRAIN_FILES_PATH = KaggleDatasets().get_gcs_path(\"filtered-tfrec\")\n",
    "    tr_files = tf.io.gfile.glob(f\"{TRAIN_FILES_PATH}/train_*.tfrec\")\n",
    "else:\n",
    "    tr_files = glob(f\"{TRAIN_FILES_PATH}/*train_*.tfrec\")\n",
    "if Params[\"train_mode\"] == \"test\":\n",
    "    train_files = tr_files[:3]\n",
    "    val_files = [tr_files[3]]\n",
    "\n",
    "elif Params[\"train_mode\"] == \"full\":\n",
    "    train_files = tr_files[:-2]\n",
    "    val_files = tr_files[-2:]\n",
    "    \n",
    "else:\n",
    "    raise AttributeError(\"Unknown Params['train_mode']\")\n",
    "\n",
    "train_ds = load_dataset(train_files)\n",
    "val_ds = load_dataset(val_files, shuffle=False, ordered=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-15T15:02:39.670012Z",
     "iopub.status.busy": "2021-09-15T15:02:39.669804Z",
     "iopub.status.idle": "2021-09-15T15:02:40.728147Z",
     "shell.execute_reply": "2021-09-15T15:02:40.727152Z",
     "shell.execute_reply.started": "2021-09-15T15:02:39.669988Z"
    }
   },
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "with strategy.scope():\n",
    "    model = Sequential([\n",
    "        layers.Conv1D(filters=32, kernel_size=16, padding=\"causal\",input_shape=[4096,3]),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=64, kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=128, kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=256, kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1),\n",
    "        layers.Activation(\"sigmoid\", dtype=\"float32\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        tf.keras.optimizers.Adam(learning_rate = Params[\"lr\"]),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"AUC\"]\n",
    "    )\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-15T14:03:21.017763Z",
     "iopub.status.busy": "2021-09-15T14:03:21.017538Z",
     "iopub.status.idle": "2021-09-15T14:03:22.376581Z",
     "shell.execute_reply": "2021-09-15T14:03:22.375662Z",
     "shell.execute_reply.started": "2021-09-15T14:03:21.017737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 4096, 32)          1568      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 1024, 32)          0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1024, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1024, 64)          16448     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1024, 128)         65664     \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1024, 128)         131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 256, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256, 128)          0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 256, 128)          131200    \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 256, 128)          131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 64, 256)           262400    \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 64, 256)           524544    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 16, 256)           0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               524416    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,796,961\n",
      "Trainable params: 1,796,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "with strategy.scope():\n",
    "    model = Sequential([\n",
    "        layers.Conv1D(filters=32, kernel_size=16, padding=\"causal\",input_shape=[4096,3]),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=64, kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=128, kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=128, kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Conv1D(filters=256, kernel_size=8, padding=\"causal\"),\n",
    "        layers.Conv1D(filters=256, kernel_size=8, padding=\"causal\"),\n",
    "        layers.MaxPool1D(pool_size=4),\n",
    "        layers.Activation(\"relu\"),\n",
    "        \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1),\n",
    "        layers.Activation(\"sigmoid\", dtype=\"float32\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        tf.keras.optimizers.Adam(learning_rate = Params[\"lr\"]),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"AUC\"]\n",
    "    )\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-15T15:02:40.72962Z",
     "iopub.status.busy": "2021-09-15T15:02:40.729411Z",
     "iopub.status.idle": "2021-09-15T15:02:40.734732Z",
     "shell.execute_reply": "2021-09-15T15:02:40.733878Z",
     "shell.execute_reply.started": "2021-09-15T15:02:40.729595Z"
    }
   },
   "outputs": [],
   "source": [
    "steps_per_epoch = 560000 // 16 * len(train_files) // Params[\"batch_size\"]\n",
    "validation_steps = 560000 // 16 * len(val_files) // Params[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-15T15:02:40.736234Z",
     "iopub.status.busy": "2021-09-15T15:02:40.736024Z",
     "iopub.status.idle": "2021-09-15T15:02:40.747399Z",
     "shell.execute_reply": "2021-09-15T15:02:40.746577Z",
     "shell.execute_reply.started": "2021-09-15T15:02:40.73621Z"
    }
   },
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.2,\n",
    "    patience=3, min_lr = 0.000001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    f\"{MDL_PATH}/model_{Params['version']:03}.h5\"\n",
    ")\n",
    "\n",
    "callbacks=[reduce_lr, early_stop, model_checkpoint]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = tf.keras.models.load_model(\"../models/models_v040/model_040.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-15T15:02:40.750226Z",
     "iopub.status.busy": "2021-09-15T15:02:40.749704Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6289/6289 [==============================] - 486s 76ms/step - loss: 0.5105 - auc: 0.7977 - val_loss: 0.4577 - val_auc: 0.8394\n",
      "Epoch 2/100\n",
      "6289/6289 [==============================] - 198s 31ms/step - loss: 0.4743 - auc: 0.8244 - val_loss: 0.4451 - val_auc: 0.8471ss - ETA: 8s - loss: 0.4748 - auc: 0.82 - ETA: 8s - loss: 0.4748 - auc: 0.8 - ETA - ETA: 6s - loss: 0.4747 - auc: 0.824 - - ETA: 1s - \n",
      "Epoch 3/100\n",
      "6289/6289 [==============================] - 198s 31ms/step - loss: 0.4663 - auc: 0.8303 - val_loss: 0.4401 - val_auc: 0.8507- l - ETA: 2:3 - ETA: 4s\n",
      "Epoch 4/100\n",
      "6289/6289 [==============================] - 199s 32ms/step - loss: 0.4614 - auc: 0.8337 - val_loss: 0.4409 - val_auc: 0.8522\n",
      "Epoch 5/100\n",
      "6289/6289 [==============================] - 199s 32ms/step - loss: 0.4575 - auc: 0.8366 - val_loss: 0.4339 - val_auc: 0.8536\n",
      "Epoch 6/100\n",
      "6289/6289 [==============================] - 452s 72ms/step - loss: 0.4538 - auc: 0.8393 - val_loss: 0.4332 - val_auc: 0.8548TA: 2:36 - loss: 0.4584 - - ETA: 8s - loss:  - ETA: 5s - loss: 0.4540 - au - ETA: \n",
      "Epoch 7/100\n",
      "6289/6289 [==============================] - 195s 31ms/step - loss: 0.4513 - auc: 0.8412 - val_loss: 0.4302 - val_auc: 0.8560ETA: 3:10 - loss -   - ETA: 15s - loss: 0.45 - ETA: - ETA: 9s - ETA: 2s - loss: 0.4515 - auc: 0.8 - ETA: 0s - loss: 0.4513 - auc: 0.84\n",
      "Epoch 8/100\n",
      "6289/6289 [==============================] - 192s 30ms/step - loss: 0.4485 - auc: 0.8438 - val_loss: 0.4289 - val_auc: 0.8576- auc: 0.843 - ETA: 1s - loss: 0.4486 - auc: 0.843 - ETA: 1s - loss: 0.4486 - auc: - ETA: 1s - loss: 0.\n",
      "Epoch 9/100\n",
      "6289/6289 [==============================] - 193s 31ms/step - loss: 0.4460 - auc: 0.8456 - val_loss: 0.4296 - val_auc: 0.8572\n",
      "Epoch 10/100\n",
      "6289/6289 [==============================] - 191s 30ms/step - loss: 0.4432 - auc: 0.8479 - val_loss: 0.4269 - val_auc: 0.8584ETA: 0s - loss: 0.4433 - auc\n",
      "Epoch 11/100\n",
      "6289/6289 [==============================] - 193s 31ms/step - loss: 0.4401 - auc: 0.8505 - val_loss: 0.4271 - val_auc: 0.8587.4406 - auc: 0. - ETA: 10s - loss: - - ETA: 2s - loss: 0.4403 - auc: 0.85 - ETA: 2s - loss: 0.4403 - au - ETA: 1s - loss:  - ETA: 0s - loss: 0.4401 - \n",
      "Epoch 12/100\n",
      "6289/6289 [==============================] - 201s 32ms/step - loss: 0.4374 - auc: 0.8529 - val_loss: 0.4250 - val_auc: 0.8590: 4s - loss: 0. - ETA: 2s - loss: 0.4 - ETA: 1s - loss: 0.4375 - auc: 0 - ETA: 1s - loss: 0.4376 - auc: 0.852 - ETA: 1s - los\n",
      "Epoch 13/100\n",
      "6289/6289 [==============================] - 203s 32ms/step - loss: 0.4343 - auc: 0.8557 - val_loss: 0.4251 - val_auc: 0.8592\n",
      "Epoch 14/100\n",
      "6289/6289 [==============================] - 203s 32ms/step - loss: 0.4309 - auc: 0.8585 - val_loss: 0.4224 - val_auc: 0.8610\n",
      "Epoch 15/100\n",
      "6289/6289 [==============================] - 201s 32ms/step - loss: 0.4275 - auc: 0.8615 - val_loss: 0.4245 - val_auc: 0.8614 ETA: - ETA: 5s - loss: 0.4278 - a - ETA: 4s  - ETA: 2s - loss: 0.42 - ETA: \n",
      "Epoch 16/100\n",
      "6289/6289 [==============================] - 205s 33ms/step - loss: 0.4239 - auc: 0.8646 - val_loss: 0.4259 - val_auc: 0.8610A: 0s - loss: 0.4239\n",
      "Epoch 17/100\n",
      "6289/6289 [==============================] - 206s 33ms/step - loss: 0.4202 - auc: 0.8678 - val_loss: 0.4218 - val_auc: 0.8629\n",
      "Epoch 18/100\n",
      "6289/6289 [==============================] - 207s 33ms/step - loss: 0.4159 - auc: 0.8712 - val_loss: 0.4209 - val_auc: 0.8649- ETA: 0s - loss: 0.4160 - a\n",
      "Epoch 19/100\n",
      "6289/6289 [==============================] - 207s 33ms/step - loss: 0.4121 - auc: 0.8743 - val_loss: 0.4200 - val_auc: 0.8652\n",
      "Epoch 20/100\n",
      "6289/6289 [==============================] - 206s 33ms/step - loss: 0.4082 - auc: 0.8775 - val_loss: 0.4233 - val_auc: 0.86620.\n",
      "Epoch 21/100\n",
      "6289/6289 [==============================] - 206s 33ms/step - loss: 0.4039 - auc: 0.8807 - val_loss: 0.4224 - val_auc: 0.8665\n",
      "Epoch 22/100\n",
      "6289/6289 [==============================] - 206s 33ms/step - loss: 0.3995 - auc: 0.8842 - val_loss: 0.4237 - val_auc: 0.8685\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "Epoch 23/100\n",
      "6289/6289 [==============================] - 207s 33ms/step - loss: 0.3833 - auc: 0.8951 - val_loss: 0.4157 - val_auc: 0.87200s - loss: 0.384 - ETA: 4s - loss: 0.3838 - auc:  - ETA: 4s - ETA: 0s - loss: 0.3833 - auc: 0.89\n",
      "Epoch 24/100\n",
      "6289/6289 [==============================] - 208s 33ms/step - loss: 0.3782 - auc: 0.8983 - val_loss: 0.4156 - val_auc: 0.8738\n",
      "Epoch 25/100\n",
      "6289/6289 [==============================] - 208s 33ms/step - loss: 0.3746 - auc: 0.9006 - val_loss: 0.4137 - val_auc: 0.8741\n",
      "Epoch 26/100\n",
      "6289/6289 [==============================] - 207s 33ms/step - loss: 0.3723 - auc: 0.9021 - val_loss: 0.4178 - val_auc: 0.8741\n",
      "Epoch 27/100\n",
      "6289/6289 [==============================] - 206s 33ms/step - loss: 0.3696 - auc: 0.9037 - val_loss: 0.4149 - val_auc: 0.8746\n",
      "Epoch 28/100\n",
      "6289/6289 [==============================] - 206s 33ms/step - loss: 0.3674 - auc: 0.9052 - val_loss: 0.4157 - val_auc: 0.8748\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "Epoch 29/100\n",
      "6289/6289 [==============================] - 208s 33ms/step - loss: 0.3677 - auc: 0.9050 - val_loss: 0.4128 - val_auc: 0.8760\n",
      "Epoch 30/100\n",
      "6289/6289 [==============================] - 200s 32ms/step - loss: 0.3656 - auc: 0.9062 - val_loss: 0.4132 - val_auc: 0.8760c\n",
      "Epoch 31/100\n",
      "6289/6289 [==============================] - 202s 32ms/step - loss: 0.3648 - auc: 0.9067 - val_loss: 0.4143 - val_auc: 0.876149 - a\n",
      "Epoch 32/100\n",
      "6289/6289 [==============================] - 208s 33ms/step - loss: 0.3644 - auc: 0.9069 - val_loss: 0.4133 - val_auc: 0.8764s - loss: 0.3645 - auc: \n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "Epoch 33/100\n",
      "6289/6289 [==============================] - 210s 33ms/step - loss: 0.3655 - auc: 0.9063 - val_loss: 0.4138 - val_auc: 0.8761TA: 4s - loss: 0.3659 - auc - ETA: 3s - loss: 0.3659 - auc: 0.90 -  - ETA: 1s - loss: \n",
      "Epoch 34/100\n",
      "6289/6289 [==============================] - 204s 32ms/step - loss: 0.3646 - auc: 0.9068 - val_loss: 0.4137 - val_auc: 0.87626 - ETA: 1s - loss - ETA: 0s - loss: 0.3647 - auc: \n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, validation_data = val_ds, epochs = Params[\"epochs\"], shuffle=True,\n",
    "                    steps_per_epoch = steps_per_epoch, validation_steps=validation_steps,\n",
    "                    verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "historyFrame = pd.DataFrame(history.history)\n",
    "historyFrame[[\"auc\", \"val_auc\"]].plot()\n",
    "historyFrame[[\"loss\", \"val_loss\"]].plot()\n",
    "historyFrame.to_csv(f\"{MDL_PATH}/history_mdl{Params['version']:03}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>auc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_auc</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.510482</td>\n",
       "      <td>0.797696</td>\n",
       "      <td>0.457714</td>\n",
       "      <td>0.839423</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.474286</td>\n",
       "      <td>0.824413</td>\n",
       "      <td>0.445144</td>\n",
       "      <td>0.847146</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.466298</td>\n",
       "      <td>0.830277</td>\n",
       "      <td>0.440120</td>\n",
       "      <td>0.850733</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.461407</td>\n",
       "      <td>0.833681</td>\n",
       "      <td>0.440878</td>\n",
       "      <td>0.852206</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.457529</td>\n",
       "      <td>0.836610</td>\n",
       "      <td>0.433870</td>\n",
       "      <td>0.853647</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.453846</td>\n",
       "      <td>0.839307</td>\n",
       "      <td>0.433154</td>\n",
       "      <td>0.854775</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.451325</td>\n",
       "      <td>0.841172</td>\n",
       "      <td>0.430236</td>\n",
       "      <td>0.856034</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.448495</td>\n",
       "      <td>0.843819</td>\n",
       "      <td>0.428862</td>\n",
       "      <td>0.857617</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.445970</td>\n",
       "      <td>0.845556</td>\n",
       "      <td>0.429647</td>\n",
       "      <td>0.857242</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.443222</td>\n",
       "      <td>0.847931</td>\n",
       "      <td>0.426920</td>\n",
       "      <td>0.858368</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.440111</td>\n",
       "      <td>0.850482</td>\n",
       "      <td>0.427081</td>\n",
       "      <td>0.858660</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.437395</td>\n",
       "      <td>0.852851</td>\n",
       "      <td>0.425013</td>\n",
       "      <td>0.859044</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.434272</td>\n",
       "      <td>0.855681</td>\n",
       "      <td>0.425121</td>\n",
       "      <td>0.859213</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.430873</td>\n",
       "      <td>0.858515</td>\n",
       "      <td>0.422421</td>\n",
       "      <td>0.861028</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.427484</td>\n",
       "      <td>0.861491</td>\n",
       "      <td>0.424464</td>\n",
       "      <td>0.861405</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.423868</td>\n",
       "      <td>0.864632</td>\n",
       "      <td>0.425932</td>\n",
       "      <td>0.860958</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.420178</td>\n",
       "      <td>0.867770</td>\n",
       "      <td>0.421770</td>\n",
       "      <td>0.862905</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.415915</td>\n",
       "      <td>0.871188</td>\n",
       "      <td>0.420888</td>\n",
       "      <td>0.864906</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.412136</td>\n",
       "      <td>0.874306</td>\n",
       "      <td>0.419984</td>\n",
       "      <td>0.865230</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.408192</td>\n",
       "      <td>0.877462</td>\n",
       "      <td>0.423283</td>\n",
       "      <td>0.866204</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.403894</td>\n",
       "      <td>0.880726</td>\n",
       "      <td>0.422388</td>\n",
       "      <td>0.866488</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.399517</td>\n",
       "      <td>0.884182</td>\n",
       "      <td>0.423747</td>\n",
       "      <td>0.868484</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.383252</td>\n",
       "      <td>0.895072</td>\n",
       "      <td>0.415742</td>\n",
       "      <td>0.871971</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.378178</td>\n",
       "      <td>0.898290</td>\n",
       "      <td>0.415593</td>\n",
       "      <td>0.873838</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.374597</td>\n",
       "      <td>0.900552</td>\n",
       "      <td>0.413716</td>\n",
       "      <td>0.874100</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.372283</td>\n",
       "      <td>0.902089</td>\n",
       "      <td>0.417833</td>\n",
       "      <td>0.874053</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.369636</td>\n",
       "      <td>0.903704</td>\n",
       "      <td>0.414950</td>\n",
       "      <td>0.874602</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.367428</td>\n",
       "      <td>0.905166</td>\n",
       "      <td>0.415698</td>\n",
       "      <td>0.874843</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.367681</td>\n",
       "      <td>0.904953</td>\n",
       "      <td>0.412834</td>\n",
       "      <td>0.876004</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.365636</td>\n",
       "      <td>0.906226</td>\n",
       "      <td>0.413153</td>\n",
       "      <td>0.875996</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.364805</td>\n",
       "      <td>0.906731</td>\n",
       "      <td>0.414262</td>\n",
       "      <td>0.876064</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.364450</td>\n",
       "      <td>0.906896</td>\n",
       "      <td>0.413322</td>\n",
       "      <td>0.876402</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.365530</td>\n",
       "      <td>0.906322</td>\n",
       "      <td>0.413775</td>\n",
       "      <td>0.876085</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.364646</td>\n",
       "      <td>0.906770</td>\n",
       "      <td>0.413736</td>\n",
       "      <td>0.876164</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss       auc  val_loss   val_auc        lr\n",
       "0   0.510482  0.797696  0.457714  0.839423  0.000100\n",
       "1   0.474286  0.824413  0.445144  0.847146  0.000100\n",
       "2   0.466298  0.830277  0.440120  0.850733  0.000100\n",
       "3   0.461407  0.833681  0.440878  0.852206  0.000100\n",
       "4   0.457529  0.836610  0.433870  0.853647  0.000100\n",
       "5   0.453846  0.839307  0.433154  0.854775  0.000100\n",
       "6   0.451325  0.841172  0.430236  0.856034  0.000100\n",
       "7   0.448495  0.843819  0.428862  0.857617  0.000100\n",
       "8   0.445970  0.845556  0.429647  0.857242  0.000100\n",
       "9   0.443222  0.847931  0.426920  0.858368  0.000100\n",
       "10  0.440111  0.850482  0.427081  0.858660  0.000100\n",
       "11  0.437395  0.852851  0.425013  0.859044  0.000100\n",
       "12  0.434272  0.855681  0.425121  0.859213  0.000100\n",
       "13  0.430873  0.858515  0.422421  0.861028  0.000100\n",
       "14  0.427484  0.861491  0.424464  0.861405  0.000100\n",
       "15  0.423868  0.864632  0.425932  0.860958  0.000100\n",
       "16  0.420178  0.867770  0.421770  0.862905  0.000100\n",
       "17  0.415915  0.871188  0.420888  0.864906  0.000100\n",
       "18  0.412136  0.874306  0.419984  0.865230  0.000100\n",
       "19  0.408192  0.877462  0.423283  0.866204  0.000100\n",
       "20  0.403894  0.880726  0.422388  0.866488  0.000100\n",
       "21  0.399517  0.884182  0.423747  0.868484  0.000100\n",
       "22  0.383252  0.895072  0.415742  0.871971  0.000020\n",
       "23  0.378178  0.898290  0.415593  0.873838  0.000020\n",
       "24  0.374597  0.900552  0.413716  0.874100  0.000020\n",
       "25  0.372283  0.902089  0.417833  0.874053  0.000020\n",
       "26  0.369636  0.903704  0.414950  0.874602  0.000020\n",
       "27  0.367428  0.905166  0.415698  0.874843  0.000020\n",
       "28  0.367681  0.904953  0.412834  0.876004  0.000004\n",
       "29  0.365636  0.906226  0.413153  0.875996  0.000004\n",
       "30  0.364805  0.906731  0.414262  0.876064  0.000004\n",
       "31  0.364450  0.906896  0.413322  0.876402  0.000004\n",
       "32  0.365530  0.906322  0.413775  0.876085  0.000001\n",
       "33  0.364646  0.906770  0.413736  0.876164  0.000001"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historyFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best epoch: 31 | best loss: 0.3644496500492096 | best auc: 0.8764021396636963\n"
     ]
    }
   ],
   "source": [
    "best_epoch = historyFrame.val_auc.argmax()\n",
    "best_loss = historyFrame.iloc[best_epoch].loss\n",
    "best_auc = historyFrame.iloc[best_epoch].val_auc\n",
    "print(\"best epoch:\", best_epoch,\n",
    "      \"| best loss:\", best_loss,\n",
    "      \"| best auc:\", best_auc\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Params.copy()\n",
    "result[\"bavg_epoch\"] = int(best_epoch)\n",
    "result[\"bavg_loss\"] = float(best_loss)\n",
    "result[\"bavg_auc\"] = float(best_auc)\n",
    "with open(f\"{MDL_PATH}/params.json\", \"w\") as file:\n",
    "    json.dump(result, file)\n",
    "\n",
    "if not os.path.exists(f\"{MDLS_PATH}/results.csv\"):\n",
    "    df_save = pd.DataFrame(result, index=[0])\n",
    "    df_save.to_csv(f\"{MDLS_PATH}/results.csv\")\n",
    "else:\n",
    "    df_old = pd.read_csv(f\"{MDLS_PATH}/results.csv\", index_col=0)\n",
    "    df_save = pd.DataFrame(result, index = [df_old.index.max() + 1])\n",
    "    df_save = df_old.append(df_save, ignore_index=True)\n",
    "    df_save.to_csv(f\"{MDLS_PATH}/results.csv\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>version</th>\n",
       "      <th>train_mode</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>bavg_epoch</th>\n",
       "      <th>bavg_loss</th>\n",
       "      <th>bavg_auc</th>\n",
       "      <th>changelog</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>1</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>16</td>\n",
       "      <td>0.451789</td>\n",
       "      <td>0.838065</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>13</td>\n",
       "      <td>0.449180</td>\n",
       "      <td>0.822957</td>\n",
       "      <td>moved all relu layers before the pooling layers</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>3</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>20</td>\n",
       "      <td>0.442866</td>\n",
       "      <td>0.837724</td>\n",
       "      <td>tried on large ds, since hight overfitting</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>5</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>13</td>\n",
       "      <td>0.449431</td>\n",
       "      <td>0.822973</td>\n",
       "      <td>added second layer to first block or reference</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>6</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>0.442229</td>\n",
       "      <td>0.820143</td>\n",
       "      <td>added relu activations to all conv layers</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>9</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>0.480765</td>\n",
       "      <td>0.812309</td>\n",
       "      <td>set all pool sizes to 1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>11</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>0.438630</td>\n",
       "      <td>0.819107</td>\n",
       "      <td>set all pool sizes to 8</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>13</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>0.448575</td>\n",
       "      <td>0.820421</td>\n",
       "      <td>added second layer to first block with small k...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>14</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>0.457616</td>\n",
       "      <td>0.820876</td>\n",
       "      <td>added second layer to first block with small k...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>17</td>\n",
       "      <td>test</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>0.475783</td>\n",
       "      <td>0.814108</td>\n",
       "      <td>trial of completely different achitecture</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>33</td>\n",
       "      <td>full</td>\n",
       "      <td>64</td>\n",
       "      <td>60</td>\n",
       "      <td>13</td>\n",
       "      <td>0.457444</td>\n",
       "      <td>0.825847</td>\n",
       "      <td>trial of completely different achitecture</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00500</td>\n",
       "      <td>43</td>\n",
       "      <td>full</td>\n",
       "      <td>64</td>\n",
       "      <td>60</td>\n",
       "      <td>43</td>\n",
       "      <td>0.452197</td>\n",
       "      <td>0.836913</td>\n",
       "      <td>trial of completely different achitecture</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>47</td>\n",
       "      <td>full</td>\n",
       "      <td>64</td>\n",
       "      <td>60</td>\n",
       "      <td>9</td>\n",
       "      <td>0.452419</td>\n",
       "      <td>0.836464</td>\n",
       "      <td>model before was standard model with 0.001 no ...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>55</td>\n",
       "      <td>full</td>\n",
       "      <td>64</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>0.451851</td>\n",
       "      <td>0.833302</td>\n",
       "      <td>removed dropout layer in conv</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>57</td>\n",
       "      <td>full</td>\n",
       "      <td>128</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>0.434063</td>\n",
       "      <td>0.835922</td>\n",
       "      <td>before: removed 2 fully connected layers. now:...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>61</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.483809</td>\n",
       "      <td>0.824025</td>\n",
       "      <td>removed dropout, flatten, added lstm</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>64</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.465674</td>\n",
       "      <td>0.824023</td>\n",
       "      <td>removed 2 layers in larger block</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>66</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.460225</td>\n",
       "      <td>0.827190</td>\n",
       "      <td>added back one layer</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>67</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.454083</td>\n",
       "      <td>0.827726</td>\n",
       "      <td>third layer</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>68</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.455897</td>\n",
       "      <td>0.827700</td>\n",
       "      <td>doubled the first layer and added a second one...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>70</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.467193</td>\n",
       "      <td>0.832003</td>\n",
       "      <td>base model</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>72</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.483452</td>\n",
       "      <td>0.824098</td>\n",
       "      <td>took out the fully connected layers before the...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>73</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.472798</td>\n",
       "      <td>0.830427</td>\n",
       "      <td>added back one fully connected layer</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>75</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0.454001</td>\n",
       "      <td>0.830381</td>\n",
       "      <td>doubled pool size, except for last, doubled nu...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>76</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>0.469206</td>\n",
       "      <td>0.832270</td>\n",
       "      <td>base model with same padding</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>78</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "      <td>0.442443</td>\n",
       "      <td>0.841306</td>\n",
       "      <td>doubled last layer in all blocks. lr/10</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>79</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>25</td>\n",
       "      <td>0.439258</td>\n",
       "      <td>0.842578</td>\n",
       "      <td>doubled last layer in all blocks doubled size ...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.00050</td>\n",
       "      <td>83</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>27</td>\n",
       "      <td>0.439051</td>\n",
       "      <td>0.841536</td>\n",
       "      <td>added dropout layers to dense layers</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>88</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>0.453092</td>\n",
       "      <td>0.834111</td>\n",
       "      <td>everything had 3 layers now and relu activation</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>92</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>17</td>\n",
       "      <td>0.434836</td>\n",
       "      <td>0.832301</td>\n",
       "      <td>dropout layers</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>110</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>11</td>\n",
       "      <td>0.470093</td>\n",
       "      <td>0.815836</td>\n",
       "      <td>dropout layers</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>113</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "      <td>0.476270</td>\n",
       "      <td>0.813889</td>\n",
       "      <td>dropout layers</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>119</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>36</td>\n",
       "      <td>0.463551</td>\n",
       "      <td>0.812370</td>\n",
       "      <td>new dataset using best model (base model with ...</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.00020</td>\n",
       "      <td>124</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>23</td>\n",
       "      <td>0.448196</td>\n",
       "      <td>0.827187</td>\n",
       "      <td>10x lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>134</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>12</td>\n",
       "      <td>0.467185</td>\n",
       "      <td>0.826250</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>135</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>9</td>\n",
       "      <td>0.444164</td>\n",
       "      <td>0.848162</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>136</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>21</td>\n",
       "      <td>0.420500</td>\n",
       "      <td>0.854332</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>137</td>\n",
       "      <td>full</td>\n",
       "      <td>256</td>\n",
       "      <td>100</td>\n",
       "      <td>31</td>\n",
       "      <td>0.364450</td>\n",
       "      <td>0.876402</td>\n",
       "      <td>1/4 lr</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lr  version train_mode  batch_size  epochs  bavg_epoch  bavg_loss  \\\n",
       "0   0.00100        1       full         256      60          16   0.451789   \n",
       "1   0.00100        2       test         256      60          13   0.449180   \n",
       "2   0.00100        3       full         256      60          20   0.442866   \n",
       "3   0.00100        5       test         256      60          13   0.449431   \n",
       "4   0.00100        6       test         256      30           9   0.442229   \n",
       "5   0.00100        9       test         256      30           4   0.480765   \n",
       "6   0.00100       11       test         256      30           8   0.438630   \n",
       "7   0.00100       13       test         256      30          12   0.448575   \n",
       "8   0.00010       14       test         256      30          12   0.457616   \n",
       "9   0.00010       17       test          64      30           6   0.475783   \n",
       "10  0.00010       33       full          64      60          13   0.457444   \n",
       "11  0.00500       43       full          64      60          43   0.452197   \n",
       "12  0.00010       47       full          64      60           9   0.452419   \n",
       "13  0.00010       55       full          64      60           4   0.451851   \n",
       "14  0.00010       57       full         128      20           9   0.434063   \n",
       "15  0.00010       61       full         256       3           2   0.483809   \n",
       "16  0.00010       64       full         256      10           7   0.465674   \n",
       "17  0.00010       66       full         256      10           6   0.460225   \n",
       "18  0.00010       67       full         256      10           7   0.454083   \n",
       "19  0.00010       68       full         256      10           7   0.455897   \n",
       "20  0.00100       70       full         256      10           9   0.467193   \n",
       "21  0.00100       72       full         256      10           9   0.483452   \n",
       "22  0.00100       73       full         256      10           9   0.472798   \n",
       "23  0.00010       75       full         256      10           9   0.454001   \n",
       "24  0.00100       76       full         256      10           6   0.469206   \n",
       "25  0.00010       78       full         256      40          12   0.442443   \n",
       "26  0.00050       79       full         256      40          25   0.439258   \n",
       "27  0.00050       83       full         256      40          27   0.439051   \n",
       "28  0.00010       88       full         256      40           8   0.453092   \n",
       "29  0.00010       92       full         256     100          17   0.434836   \n",
       "30  0.00010      110       full         256     100          11   0.470093   \n",
       "31  0.00010      113       full         256     100           8   0.476270   \n",
       "32  0.00002      119       full         256     100          36   0.463551   \n",
       "33  0.00020      124       full         256     100          23   0.448196   \n",
       "34  0.00010      134       full         256     100          12   0.467185   \n",
       "35  0.00010      135       full         256     100           9   0.444164   \n",
       "36  0.00010      136       full         256     100          21   0.420500   \n",
       "37  0.00010      137       full         256     100          31   0.364450   \n",
       "\n",
       "    bavg_auc                                          changelog  seed  \n",
       "0   0.838065                                                NaN   NaN  \n",
       "1   0.822957    moved all relu layers before the pooling layers   NaN  \n",
       "2   0.837724         tried on large ds, since hight overfitting   NaN  \n",
       "3   0.822973     added second layer to first block or reference   NaN  \n",
       "4   0.820143          added relu activations to all conv layers   NaN  \n",
       "5   0.812309                            set all pool sizes to 1   NaN  \n",
       "6   0.819107                            set all pool sizes to 8   NaN  \n",
       "7   0.820421  added second layer to first block with small k...   NaN  \n",
       "8   0.820876  added second layer to first block with small k...   NaN  \n",
       "9   0.814108          trial of completely different achitecture   NaN  \n",
       "10  0.825847          trial of completely different achitecture   NaN  \n",
       "11  0.836913          trial of completely different achitecture   NaN  \n",
       "12  0.836464  model before was standard model with 0.001 no ...  42.0  \n",
       "13  0.833302                      removed dropout layer in conv  42.0  \n",
       "14  0.835922  before: removed 2 fully connected layers. now:...  42.0  \n",
       "15  0.824025               removed dropout, flatten, added lstm  42.0  \n",
       "16  0.824023                   removed 2 layers in larger block  42.0  \n",
       "17  0.827190                               added back one layer  42.0  \n",
       "18  0.827726                                        third layer  42.0  \n",
       "19  0.827700  doubled the first layer and added a second one...  42.0  \n",
       "20  0.832003                                         base model  42.0  \n",
       "21  0.824098  took out the fully connected layers before the...  42.0  \n",
       "22  0.830427               added back one fully connected layer  42.0  \n",
       "23  0.830381  doubled pool size, except for last, doubled nu...  42.0  \n",
       "24  0.832270                       base model with same padding  42.0  \n",
       "25  0.841306            doubled last layer in all blocks. lr/10  42.0  \n",
       "26  0.842578  doubled last layer in all blocks doubled size ...  42.0  \n",
       "27  0.841536               added dropout layers to dense layers  42.0  \n",
       "28  0.834111    everything had 3 layers now and relu activation  42.0  \n",
       "29  0.832301                                     dropout layers  42.0  \n",
       "30  0.815836                                     dropout layers  42.0  \n",
       "31  0.813889                                     dropout layers  42.0  \n",
       "32  0.812370  new dataset using best model (base model with ...  42.0  \n",
       "33  0.827187                                             10x lr  42.0  \n",
       "34  0.826250                                             1/4 lr  42.0  \n",
       "35  0.848162                                             1/4 lr  42.0  \n",
       "36  0.854332                                             1/4 lr  42.0  \n",
       "37  0.876402                                             1/4 lr  42.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(f\"{MDLS_PATH}/results.csv\",index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 4096, 3), (None, 4096, 3)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if KAGGLE:\n",
    "    te_files = tf.io.gfile.glob(f\"{TRAIN_FILES_PATH}/test_*.tfrec\")\n",
    "else:\n",
    "    te_files = glob(f\"{TRAIN_FILES_PATH}/test_*.tfrec\")\n",
    "    te_files = glob(f\"../input/filtered_tfrec/test_*.tfrec\")\n",
    "test_set = load_dataset(te_files, shuffle=False, ordered=True, labeled=False, repeat=False)\n",
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "with strategy.scope():\n",
    "    model = tf.keras.models.load_model(f\"{MDL_PATH}/model_{Params['version']:03}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        ],\n",
       "       [0.5228113 ],\n",
       "       [0.16105618],\n",
       "       ...,\n",
       "       [0.31426096],\n",
       "       [1.        ],\n",
       "       [0.10687688]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00005bced6</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000806717</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000ef4fe1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00020de251</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00024887b5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  target\n",
       "0  00005bced6     0.5\n",
       "1  0000806717     0.5\n",
       "2  0000ef4fe1     0.5\n",
       "3  00020de251     0.5\n",
       "4  00024887b5     0.5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.read_csv(\"../input/g2net-gravitational-wave-detection/sample_submission.csv\")\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "sub.target = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(f\"{MDL_PATH}/submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16256/1520512053.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtr_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{TRAIN_FILES_PATH}/train_*\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_files\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mordered\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabeled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpred_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\g2net-tf\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1749\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1750\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1751\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1752\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1753\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\g2net-tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\g2net-tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    922\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\envs\\g2net-tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3040\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3042\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\g2net-tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1964\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\g2net-tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\g2net-tf\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "tr_files = glob(f\"{TRAIN_FILES_PATH}/train_*\")\n",
    "train_set = load_dataset(tr_files, shuffle=False, ordered=True, labeled=False, repeat=False)\n",
    "pred_tr = model.predict(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"{INPUT_DIR}/training_labels.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(train_df.target, pred_tr)\n",
    "test_y = np.load(f\"{INPUT_DIR}/train_x.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(test_y, batch_size = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
